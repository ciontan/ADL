{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f66c2b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from pandas) (2.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: optuna in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (4.2.1)\n",
      "Requirement already satisfied: alembic>=1.5.0 in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from optuna) (1.15.2)\n",
      "Requirement already satisfied: colorlog in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from optuna) (6.9.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from optuna) (2.2.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from optuna) (24.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from optuna) (2.0.40)\n",
      "Requirement already satisfied: tqdm in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from optuna) (6.0.2)\n",
      "Requirement already satisfied: Mako in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from alembic>=1.5.0->optuna) (1.3.9)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from colorlog->optuna) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
      "Requirement already satisfied: optuna-dashboard in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (0.18.0)\n",
      "Requirement already satisfied: bottle>=0.13.0 in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from optuna-dashboard) (0.13.2)\n",
      "Requirement already satisfied: optuna>=3.1.0 in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from optuna-dashboard) (4.2.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from optuna-dashboard) (24.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from optuna-dashboard) (1.6.1)\n",
      "Requirement already satisfied: alembic>=1.5.0 in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from optuna>=3.1.0->optuna-dashboard) (1.15.2)\n",
      "Requirement already satisfied: colorlog in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from optuna>=3.1.0->optuna-dashboard) (6.9.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from optuna>=3.1.0->optuna-dashboard) (2.2.4)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from optuna>=3.1.0->optuna-dashboard) (2.0.40)\n",
      "Requirement already satisfied: tqdm in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from optuna>=3.1.0->optuna-dashboard) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from optuna>=3.1.0->optuna-dashboard) (6.0.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from scikit-learn->optuna-dashboard) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from scikit-learn->optuna-dashboard) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from scikit-learn->optuna-dashboard) (3.6.0)\n",
      "Requirement already satisfied: Mako in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from alembic>=1.5.0->optuna>=3.1.0->optuna-dashboard) (1.3.9)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from alembic>=1.5.0->optuna>=3.1.0->optuna-dashboard) (4.12.2)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from sqlalchemy>=1.4.2->optuna>=3.1.0->optuna-dashboard) (3.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from colorlog->optuna>=3.1.0->optuna-dashboard) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna>=3.1.0->optuna-dashboard) (3.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement scikit (from versions: none)\n",
      "ERROR: No matching distribution found for scikit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imbalanced-learn in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (0.13.0)\n",
      "Requirement already satisfied: numpy<3,>=1.24.3 in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from imbalanced-learn) (2.2.4)\n",
      "Requirement already satisfied: scipy<2,>=1.10.1 in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from imbalanced-learn) (1.15.2)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.3.2 in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from imbalanced-learn) (1.6.1)\n",
      "Requirement already satisfied: sklearn-compat<1,>=0.1 in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from imbalanced-learn) (0.1.3)\n",
      "Requirement already satisfied: joblib<2,>=1.1.1 in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from imbalanced-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from imbalanced-learn) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install optuna\n",
    "!pip install optuna-dashboard\n",
    "!pip install scikit\n",
    "!pip install imbalanced-learn\n",
    "from sklearn.preprocessing import (\n",
    "    MaxAbsScaler,\n",
    "    MinMaxScaler,\n",
    "    Normalizer,\n",
    "    PowerTransformer,\n",
    "    QuantileTransformer,\n",
    "    RobustScaler,\n",
    "    StandardScaler,\n",
    "    minmax_scale,\n",
    ")\n",
    "from sklearn.metrics import recall_score, accuracy_score,f1_score, precision_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import warnings\n",
    "import optuna\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e29038ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "random_state = 42\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "raw_dataset = pd.read_csv(\"./data/processed_data.csv\") #data has X and Y\n",
    "X = raw_dataset.drop(columns=[\"DR\"])\n",
    "Y = pd.DataFrame(raw_dataset[\"DR\"])\n",
    "# print(X.describe())\n",
    "# X.drop(columns = ['Age', 'Gender', 'UAlb', 'Ucr', 'UACR', 'LDLC', 'HDLC'], inplace=True)\n",
    "\n",
    "# [Age,Gender,UAlb,Ucr,UACR,TC,TG,TCTG,LDLC,HDLC,Scr,BUN,FPG,HbA1c,Height,Weight,BMI,Duration,DR,Community_baihe,Community_chonggu,Community_huaxin,Community_jinze,Community_liantang,Community_xianghuaqiao,Community_xujin,Community_yingpu,Community_zhaoxian,Community_zhujiajiao]\n",
    "#* 90/10 split for training and final test\n",
    "X_FOR_FOLDS, X_FINAL_TEST, Y_FOR_FOLDS, Y_FINAL_TEST = train_test_split(X, Y, test_size=0.1, random_state=random_state, stratify=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63312025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from modularModels1 import BlockMaker, modularNN, BasicModel\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using\", device)\n",
    "\n",
    "def init_weights(model): #tested already\n",
    "    if isinstance(model, nn.Linear):  # Apply only to linear layers\n",
    "        nn.init.xavier_uniform_(model.weight)\n",
    "        if model.bias is not None:\n",
    "            nn.init.zeros_(model.bias)\n",
    "            \n",
    "def fold_to_dataloader_tensor(train_x, test_x, train_y, test_y, batch_size=64, device=device):\n",
    "    train_dataset = TensorDataset(\n",
    "        torch.tensor(train_x.values,dtype=torch.float32).to(device), \n",
    "        torch.tensor(train_y.values,dtype=torch.float32).to(device))\n",
    "    val_dataset = TensorDataset(\n",
    "        torch.tensor(test_x.values,dtype=torch.float32).to(device), \n",
    "        torch.tensor(test_y.values,dtype=torch.float32).to(device))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=False, drop_last=True)\n",
    "    return train_loader, val_loader \n",
    "\n",
    "def get_feature_count(loader):\n",
    "    \"\"\"returns the number of features in the dataset\"\"\"\n",
    "    return next(iter(loader))[0].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d0d47d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Criterion_Models import *\n",
    "def criterion_mapping(criterion_choice:str, pos_weight:float=None, alpha:float=None, gamma:float=None):\n",
    "    \"\"\"\n",
    "    Feel free to add any custom loss functions here.\n",
    "    returns function for criterion\n",
    "    \"\"\"\n",
    "    if criterion_choice == \"FocalLoss\":\n",
    "        return FocalLoss(alpha =alpha, gamma=gamma)\n",
    "    elif criterion_choice == \"DiceLoss\":\n",
    "        return DiceLoss()\n",
    "    elif criterion_choice == \"BCEWithLogitsLoss\":\n",
    "        return nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight])) if pos_weight else nn.BCEWithLogitsLoss()\n",
    "    return nn.BCEWithLogitsLoss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e400764c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def augment_data_in_place(X, X_test, normalisation_method=MinMaxScaler()):\n",
    "    all_numerical_columns = [\n",
    "        'Age', 'Height', 'Weight', 'Duration',\n",
    "        'UAlb', 'Ucr', 'UACR', 'TC', 'TG', \n",
    "        'TCTG', 'LDLC', 'HDLC', 'Scr', 'BUN', 'FPG', 'HbA1c'\n",
    "    ]\n",
    "\n",
    "    # Find which of those columns actually exist in both X and X_test\n",
    "    existing_columns = [col for col in all_numerical_columns if col in X.columns and col in X_test.columns]\n",
    "    \n",
    "    if not existing_columns:\n",
    "        print(\"No matching columns found for augmentation. Normalised data only.\")\n",
    "        X = normalisation_method.fit_transform(X)\n",
    "        X_test = normalisation_method.transform(X_test)\n",
    "        return X, X_test\n",
    "\n",
    "    # 1. Log-transform\n",
    "    X_copy = X.copy()\n",
    "    X_test_copy = X_test.copy()\n",
    "    X_copy.loc[:, existing_columns] = X_copy.loc[:, existing_columns].apply(np.log1p)\n",
    "    X_test_copy.loc[:, existing_columns] = X_test_copy.loc[:, existing_columns].apply(np.log1p)\n",
    "\n",
    "    # 2. Add Gaussian noise to training data only\n",
    "    noise = np.random.normal(0, 0.1, X_copy[existing_columns].shape)\n",
    "    X_copy.loc[:, existing_columns] = X_copy.loc[:, existing_columns] + noise\n",
    "\n",
    "    # 3. Fit scaler on train, transform both\n",
    "    scaler = normalisation_method\n",
    "    X_copy.loc[:, existing_columns] = scaler.fit_transform(X_copy.loc[:, existing_columns])\n",
    "    X_test_copy.loc[:, existing_columns] = scaler.transform(X_test_copy.loc[:, existing_columns])\n",
    "\n",
    "    return X_copy, X_test_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "165aa4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FOLDS_GENERATOR(X, Y, normalisation_method=MinMaxScaler(), n_splits=5, random_state=None, oversampler=None, contamination=0.05):\n",
    "    \"\"\"\n",
    "    Generates stratified folds with specified normalization.\n",
    "\n",
    "    normalisation_method should be an instance of a scaler, e.g.,\n",
    "    - MinMaxScaler()\n",
    "    - MaxAbsScaler()\n",
    "    - QuantileTransformer(output_distribution='uniform')\n",
    "\n",
    "    Returns a list of tuples, each containing:\n",
    "    (X_train_scaled, X_test_scaled, Y_train, Y_test), representing data for each fold\n",
    "    \"\"\"\n",
    "    kF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    kFolds_list = []\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(kF.split(X, Y)):\n",
    "        # Split the data into training and testing sets for this fold\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        Y_train, Y_test = Y.iloc[train_idx], Y.iloc[test_idx]\n",
    "        # print(\"Original\\n\", X_train.shape, Y_train.shape, X_test.shape, Y_test.shape)\n",
    "        X_train_cleaned, Y_train_cleaned = X_train.copy(), Y_train.copy()\n",
    "        if contamination is not None and contamination > 0: #? using contamination = 0.0 works\n",
    "            X_train_zeros = X_train[Y_train.iloc[:, 0] == 0]\n",
    "            X_train_ones = X_train[Y_train.iloc[:, 0] == 1]\n",
    "            Y_train_zeros = Y_train[Y_train.iloc[:, 0] == 0]\n",
    "            Y_train_ones = Y_train[Y_train.iloc[:, 0] == 1] \n",
    "            # print(\"Ones and zeros\\n\", X_train_zeros.shape, Y_train_zeros.shape, X_train_ones.shape, Y_train_ones.shape)\n",
    "            #only class 0s\n",
    "            if X_train_zeros.isna().any().any():\n",
    "                print(\"got NaN values in the training set\")\n",
    "            \n",
    "            # Apply Isolation Forest to majority class only\n",
    "            iso_forest = IsolationForest(contamination=contamination, random_state=random_state)\n",
    "            try:\n",
    "                outliers = iso_forest.fit_predict(X_train_zeros)\n",
    "            except UserWarning as e:\n",
    "                print(\"Caught warning during IsolationForest fitting:\", e)\n",
    "                outliers = np.ones(len(X_train_zeros))  # If warning occurs, keep all data\n",
    "            # Keep only non-outlier majority samples\n",
    "            X_train_zeros = X_train_zeros[outliers == 1]\n",
    "            Y_train_zeros = Y_train_zeros[outliers == 1]\n",
    "            # print(\"After iso:\\n\", X_train_zeros.shape, Y_train_zeros.shape, X_train_ones.shape, Y_train_ones.shape)\n",
    "            \n",
    "            # Combine the cleaned majority class with the untouched minority class\n",
    "            X_train_cleaned = pd.concat([X_train_zeros, X_train_ones])\n",
    "            Y_train_cleaned = pd.concat([Y_train_zeros, Y_train_ones])\n",
    "        #? data augmentation on leftover data\n",
    "        X_train_scaled, X_test_scaled = augment_data_in_place(X_train_cleaned, X_test, normalisation_method=normalisation_method)\n",
    "        \n",
    "        # Handle oversampling if needed\n",
    "        #! use X_train_scaled and Y_train_cleaned for oversampling becasue y_train_cleaned no changes after augmentation\n",
    "        if oversampler:\n",
    "            X_train_scaled, Y_train_cleaned = oversampler.fit_resample(X_train_scaled, Y_train_cleaned)\n",
    "\n",
    "        # Convert scaled data back to DataFrame with the correct column names\n",
    "        X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train_cleaned.columns)\n",
    "        X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "\n",
    "        # Handle community columns\n",
    "        community_cols = [col for col in X_train_scaled.columns if col.startswith('Community')]\n",
    "        if community_cols:\n",
    "            X_train_scaled[community_cols] = X_train_scaled[community_cols].apply(\n",
    "                lambda row: pd.Series(np.eye(len(row))[row.argmax()]), axis=1\n",
    "            ).set_axis(community_cols, axis=1)\n",
    "        # print(X_train_scaled[community_cols].describe())\n",
    "\n",
    "        # Ensure 'Gender' is still binary (0 or 1)\n",
    "        if 'Gender' in X_train_scaled.columns:\n",
    "            X_train_scaled['Gender'] = (X_train_scaled['Gender'] > 0.5).astype(int)\n",
    "            X_test_scaled['Gender'] = (X_test_scaled['Gender'] > 0.5).astype(int)\n",
    "\n",
    "        # Append the processed fold to the list\n",
    "        kFolds_list.append((X_train_scaled, X_test_scaled, Y_train_cleaned, Y_test))\n",
    "\n",
    "        print(f\"Fold: {fold+1}, Train: {X_train_scaled.shape}, Test: {X_test_scaled.shape}\")\n",
    "\n",
    "    return kFolds_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "efd0b120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# oversampler = None\n",
    "# contamination = 0.05\n",
    "# normalisation_method = QuantileTransformer()\n",
    "# kFolds = FOLDS_GENERATOR(X_FOR_FOLDS, Y_FOR_FOLDS, \n",
    "#                          normalisation_method = normalisation_method, \n",
    "#                          n_splits=5, \n",
    "#                          oversampler = oversampler, random_state=42, contamination=contamination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d61863d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, criterion, optimiser, scheduler, train_loader, val_loader, epochs=20, patience=5, device=device, threshold = 0.5):\n",
    "    if isinstance(model.last_layer(), nn.Sigmoid) and isinstance(criterion, nn.BCEWithLogitsLoss):\n",
    "        raise ValueError(\"Model output is Sigmoid but criterion is BCEWithLogitsLoss. Please check your model and criterion compatibility.\")\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    wait = 0\n",
    "\n",
    "    \n",
    "    #* Epoch Training loop for this fold\n",
    "    for epoch in range(1,epochs+1):\n",
    "        #* Set model to training mode: essential for dropout and batch norm layers\n",
    "        model.train()\n",
    "        running_loss = 0.0 #? loss for this epoch\n",
    "        #* Mini-batch training loop\n",
    "        for batch, (inputs, labels) in enumerate(train_loader,start=1):\n",
    "            optimiser.zero_grad() #? Zero the gradients\n",
    "            outputs = model(inputs) #? Forward pass through the model\n",
    "            loss = criterion(outputs, labels) #? Calculate loss\n",
    "            loss.backward() #? Backpropagation\n",
    "            running_loss += loss.item()\n",
    "            optimiser.step() #? Update weights\n",
    "            if scheduler:\n",
    "                if isinstance(scheduler,torch.optim.lr_scheduler.OneCycleLR):\n",
    "                    scheduler.step()\n",
    "        # if scheduler: \n",
    "        #     if not isinstance(scheduler,torch.optim.lr_scheduler.OneCycleLR):\n",
    "        #         scheduler.step()\n",
    "                \n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        # print(f\"Epoch: {epoch}, training loss: {train_loss:.4f}\")\n",
    "    \n",
    "        #* Now we evaluate the model on the validation set, to track training vs validation loss\n",
    "        model.eval() #? Set model to evaluation mode\n",
    "        with torch.no_grad(): #? No need to track gradients during evaluation\n",
    "            val_loss = 0.0    \n",
    "            for batch, (inputs, labels) in enumerate(val_loader,start=1):#! one pass because val_loader batch size is all, if you want to do it in mini-batches, you MUST change the metric calculations to accept mini-batches\n",
    "                outputs = model(inputs)\n",
    "                # labels = labels.cpu() \n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() #? Calculate loss\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            scheduler.step(avg_val_loss) if scheduler and not isinstance(scheduler,torch.optim.lr_scheduler.OneCycleLR) else None\n",
    "            # print(scheduler.get_last_lr())\n",
    "            # print(f\"Epoch: {epoch}, training loss: {train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "            # print(f\"Epoch: {epoch}\".ljust(12), f\"training loss:{train_loss:.4f}\".ljust(12), f\"Val Loss: {avg_val_loss:.4f}\",end=\"\\r\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model_state = model.state_dict()\n",
    "            wait = 0\n",
    "        elif avg_val_loss*0.95 <= best_val_loss:\n",
    "                wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "        if wait >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch}, best val loss: {best_val_loss:.4f}\")\n",
    "            break\n",
    "        print(f\"Epoch: {epoch}\".ljust(12), f\"training loss:{train_loss:.4f}\".ljust(12), f\"best_val_loss:{best_val_loss:.4f}\".ljust(12), f\"Val Loss: {avg_val_loss:.4f}\", f\"Scheduler lr: {scheduler.get_last_lr()}\",end=\"\\r\")\n",
    "    #* Use best model to calculate metrics on the validation set\n",
    "    #! must be outside epoch loop, it comes after the training and cv loop\n",
    "    model.load_state_dict(best_model_state) #? Load the best model state\n",
    "    with torch.no_grad():\n",
    "        for batch, (inputs, labels) in enumerate(val_loader,start=1):#! one pass because val_loader batch size is all, if you want to do it in mini-batches, you MUST change the metric calculations to accept mini-batches\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                labels = labels.cpu() \n",
    "                # predictions = (torch.sigmoid(outputs) < 0.5).float().cpu().numpy()\n",
    "                predictions = (torch.sigmoid(outputs) >= threshold).float().cpu().numpy()\n",
    "\n",
    "                \n",
    "                val_loss += loss.item() #? Calculate loss\n",
    "                \n",
    "    #! The following should have length equal to fold number           \n",
    "    accuracy=accuracy_score(labels, predictions) \n",
    "    precision=precision_score(labels, predictions, pos_label=1, zero_division=0)\n",
    "    recall=recall_score(labels, predictions, pos_label=1)\n",
    "    f1=f1_score(labels, predictions, pos_label=1)\n",
    "    auc=roc_auc_score(labels, predictions)\n",
    "    \n",
    "    return model, accuracy, precision, recall, f1, auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d775e045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyModel(\n",
      "  (block1): FeedForwardBlock(\n",
      "    (block): Sequential(\n",
      "      (0): Linear(in_features=8, out_features=64, bias=True)\n",
      "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (block2): FeedForwardBlock(\n",
      "    (block): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=16, bias=True)\n",
      "      (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (output_layer): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FeedForwardBlock(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout=None, activation=nn.ReLU):\n",
    "        super().__init__()\n",
    "        layers = [\n",
    "            nn.Linear(in_features, out_features),\n",
    "            nn.BatchNorm1d(out_features),\n",
    "            activation()\n",
    "        ]\n",
    "        if dropout and dropout > 0:\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "        \n",
    "        self.block = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout= None, hidden_dim2= None):\n",
    "        super().__init__()\n",
    "        \n",
    "        # A couple of FeedForward blocks\n",
    "        self.block1 = FeedForwardBlock(input_dim, hidden_dim, dropout)\n",
    "        self.block2 = FeedForwardBlock(hidden_dim, output_dim, dropout)\n",
    "        # self.block3 = FeedForwardBlock(hidden_dim2, output_dim, dropout = None)\n",
    "\n",
    "        # Final output layer (could be softmax, sigmoid, or whatever your target is)\n",
    "        self.output_layer = nn.Linear(output_dim, 1)  # Just in case you're doing regression or binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        # x = self.block3(x)\n",
    "        x = self.output_layer(x)  # Final linear layer\n",
    "        return x\n",
    "    \n",
    "    def last_layer(self):\n",
    "        return self.output_layer\n",
    "test_model = MyModel(8,64,16,0)\n",
    "print(test_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "53f7599a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximise_combined_score(trial):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "    epochs = 10000\n",
    "    random_state = 42\n",
    "    oversampler = ADASYN(sampling_strategy='minority', n_neighbors=5, random_state=random_state)\n",
    "    if isinstance(oversampler, ADASYN):\n",
    "        n_neighbours = trial.suggest_int(\"n_neighbours\", 2, 10)\n",
    "        oversampler = ADASYN(sampling_strategy='minority', n_neighbors=n_neighbours, random_state=random_state)\n",
    "    \n",
    "    normalisation_method = trial.suggest_categorical(\"normalisation_method\", [\n",
    "        \"MinMaxScaler\",\n",
    "        # \"MaxAbsScaler\",\n",
    "    #     \"StandardScaler\",\n",
    "    #     # \"PowerTransformer\",\n",
    "        # \"QuantileTransformer\",\n",
    "    ])\n",
    "    if normalisation_method:\n",
    "        if normalisation_method == \"MinMaxScaler\":\n",
    "            normalisation_method = MinMaxScaler()\n",
    "\n",
    "        elif normalisation_method == \"StandardScaler\":\n",
    "            normalisation_method = StandardScaler()\n",
    "        elif normalisation_method == \"RobustScaler\":\n",
    "            normalisation_method = RobustScaler()\n",
    "        elif normalisation_method == \"PowerTransformer\":\n",
    "            normalisation_method = PowerTransformer()\n",
    "        elif normalisation_method == \"QuantileTransformer\":\n",
    "            normalisation_method = QuantileTransformer(output_distribution='uniform')\n",
    "        else:\n",
    "            normalisation_method = MinMaxScaler()\n",
    "    contamination = trial.suggest_float(\"contamination\", 0.01, 0.2)\n",
    "    kFolds = FOLDS_GENERATOR(X_FOR_FOLDS, Y_FOR_FOLDS, \n",
    "                         normalisation_method = normalisation_method, \n",
    "                         n_splits=5, \n",
    "                         oversampler = oversampler, random_state=42, contamination=contamination)\n",
    "                        \n",
    "    # Model hyperparameters (first-level optimization)\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 16, 128)\n",
    "    # hidden_dim2 = trial.suggest_int(\"hidden_dim2\", 20, 100)\n",
    "    hidden_dim2 = None\n",
    "    output_dim = trial.suggest_int(\"output_dim\", 2, hidden_dim)\n",
    "    \n",
    "    dropout = trial.suggest_float(\"dropout\", 0.00, 0.3)\n",
    "    threshold = trial.suggest_float(\"threshold\", 0.1, 0.9)\n",
    "    # dropout = None\n",
    "    initial_lr = trial.suggest_float(\"initial_lr\", 1e-4, 1e-3, log=True)\n",
    "    max_lr = trial.suggest_float(\"max_lr\", 5e-3, 5e-3, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-5, 1e-2, log=True)\n",
    "    \n",
    "    # Loss function hyperparameters\n",
    "    # criterion_choice = trial.suggest_categorical(\"criterion\", [\"BCEWithLogitsLoss\", \"FocalLoss\"]) \n",
    "    criterion_choice = trial.suggest_categorical(\"criterion\", [\"BCEWithLogitsLoss\"]) \n",
    "    \n",
    "    # Hyperparameter exploration optimization\n",
    "    if criterion_choice == \"BCEWithLogitsLoss\":\n",
    "        pos_weight = trial.suggest_int(\"pos_weight\", 1, 10)\n",
    "        alpha = None\n",
    "        gamma = None\n",
    "    elif criterion_choice == \"FocalLoss\":\n",
    "        pos_weight= None\n",
    "        alpha = trial.suggest_float(\"alpha\", 1, 2)\n",
    "        gamma = trial.suggest_float(\"gamma\", 0.25, 2)\n",
    "    else:\n",
    "        pos_weight = None\n",
    "    \n",
    "    # Initialize lists for metrics across folds\n",
    "    accuracy_list = []\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    f1_list = []\n",
    "    auc_list = []\n",
    "\n",
    "    # Cross-validation loop\n",
    "    for fold, (train_x, test_x, train_y, test_y) in enumerate(kFolds, start=1):\n",
    "        # Create DataLoader for current fold\n",
    "        train_loader, val_loader = fold_to_dataloader_tensor(train_x, test_x, train_y, test_y, batch_size=64, device=device)\n",
    "        # Calculate steps_per_epoch from the current fold's train_loader\n",
    "        train_loader_len = len(train_loader)\n",
    "        \n",
    "        # Instantiate and initialize the model\n",
    "        # model = BinaryClassifier(input_dim=get_feature_count(train_loader), hidden_dim=hidden_dim, dropout=dropout)\n",
    "        model = MyModel(input_dim=get_feature_count(train_loader), hidden_dim=hidden_dim, output_dim=hidden_dim, dropout=dropout)\n",
    "        model.to(device)\n",
    "        model.apply(init_weights)\n",
    "        \n",
    "        # Map the choice to the actual loss function\n",
    "        criterion = criterion_mapping(criterion_choice, pos_weight, alpha, gamma).to(device)\n",
    "        optimiser = optim.Adam(model.parameters(), lr=initial_lr, weight_decay=weight_decay)\n",
    "\n",
    "        \n",
    "        # Initialize scheduler\n",
    "        # scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        #     optimiser,\n",
    "        #     max_lr=max_lr,\n",
    "        #     steps_per_epoch=train_loader_len,\n",
    "        #     epochs=epochs,\n",
    "        #     anneal_strategy='linear'\n",
    "        # )\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimiser, mode='min', patience=7, factor=0.5, min_lr=1e-6)\n",
    "        print(f\"Fold {fold}:\")\n",
    "        # Train and evaluate the model on the current fold\n",
    "        model, accuracy, precision, recall, f1, auc = train_and_evaluate(\n",
    "            model, criterion, optimiser, scheduler, train_loader, val_loader, epochs=epochs, patience=40, device=device, threshold = 0.5\n",
    "        )\n",
    "        print(f\"Accuracy: {accuracy:.4f}, precision: {precision:.4f}, recall: {recall:.4f}, f1: {f1:.4f}, auc: {auc:.4f}\")\n",
    "\n",
    "        # Append the metrics from the current fold\n",
    "        accuracy_list.append(accuracy)\n",
    "        precision_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "        f1_list.append(f1)\n",
    "        auc_list.append(auc)\n",
    "\n",
    "    # Calculate the average metrics across all folds\n",
    "    avg_accuracy = np.sum(accuracy_list) / len(accuracy_list)\n",
    "    avg_precision = np.sum(precision_list) / len(precision_list)\n",
    "    avg_recall = np.sum(recall_list) / len(recall_list)\n",
    "    avg_f1 = np.sum(f1_list) / len(f1_list)\n",
    "    avg_auc = np.sum(auc_list) / len(auc_list)\n",
    "\n",
    "    # Combine metrics into a single \"score\"\n",
    "    # combined_score = (avg_f1 + avg_precision + avg_recall + avg_accuracy + avg_auc) / 5\n",
    "    combined_score = avg_f1\n",
    "\n",
    "    return combined_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437925bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-15 14:27:58,246] A new study created in memory with name: Basic\n",
      "Bottle v0.13.2 server starting up (using WSGIRefServer())...\n",
      "Listening on http://localhost:8080/\n",
      "Hit Ctrl-C to quit.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Fold: 1, Train: (7288, 28), Test: (1149, 28)\n",
      "Fold: 2, Train: (7246, 28), Test: (1149, 28)\n",
      "Fold: 3, Train: (7240, 28), Test: (1148, 28)\n",
      "Fold: 4, Train: (7208, 28), Test: (1148, 28)\n",
      "Fold: 5, Train: (7252, 28), Test: (1148, 28)\n",
      "Fold 1:\n",
      "Early stopping triggered at epoch 57, best val loss: 0.3895Loss: 0.6803 Scheduler lr: [4.437408082682723e-05]]\n",
      "Accuracy: 0.6858, precision: 0.2005, recall: 0.7069, f1: 0.3124, auc: 0.6952\n",
      "Fold 2:\n",
      "Early stopping triggered at epoch 66, best val loss: 0.3380Loss: 0.6635 Scheduler lr: [4.437408082682723e-05]]\n",
      "Accuracy: 0.6928, precision: 0.2015, recall: 0.6897, f1: 0.3119, auc: 0.6914\n",
      "Fold 3:\n",
      "Early stopping triggered at epoch 68, best val loss: 0.3718Loss: 0.6095 Scheduler lr: [2.2187040413413616e-05]\n",
      "Accuracy: 0.7186, precision: 0.1796, recall: 0.5000, f1: 0.2642, auc: 0.6216\n",
      "Fold 4:\n",
      "Early stopping triggered at epoch 68, best val loss: 0.3798Loss: 0.6370 Scheduler lr: [4.437408082682723e-05]]\n",
      "Accuracy: 0.6951, precision: 0.1750, recall: 0.5431, f1: 0.2647, auc: 0.6277\n",
      "Fold 5:\n",
      "Epoch: 57    training loss:0.3514 best_val_loss:0.3370 Val Loss: 0.6597 Scheduler lr: [4.437408082682723e-05]]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-15 14:29:28,331] Trial 0 finished with value: 0.28834272006635314 and parameters: {'n_neighbours': 4, 'normalisation_method': 'MinMaxScaler', 'contamination': 0.13248425123950647, 'hidden_dim': 83, 'output_dim': 58, 'dropout': 0.2866079398541896, 'threshold': 0.16035256862318034, 'initial_lr': 0.0007099852932292357, 'max_lr': 0.005, 'weight_decay': 0.005329682180071191, 'criterion': 'BCEWithLogitsLoss', 'pos_weight': 1}. Best is trial 0 with value: 0.28834272006635314.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 58, best val loss: 0.3370\n",
      "Accuracy: 0.6821, precision: 0.1864, recall: 0.6379, f1: 0.2885, auc: 0.6625\n",
      "Using device: cuda\n",
      "Fold: 1, Train: (6855, 28), Test: (1149, 28)\n",
      "Fold: 2, Train: (6859, 28), Test: (1149, 28)\n",
      "Fold: 3, Train: (6833, 28), Test: (1148, 28)\n",
      "Fold: 4, Train: (6851, 28), Test: (1148, 28)\n",
      "Fold: 5, Train: (6878, 28), Test: (1148, 28)\n",
      "Fold 1:\n",
      "Early stopping triggered at epoch 63, best val loss: 0.7039Loss: 0.8785 Scheduler lr: [1.1735255237958168e-05]\n",
      "Accuracy: 0.6214, precision: 0.1725, recall: 0.7241, f1: 0.2786, auc: 0.6670\n",
      "Fold 2:\n",
      "Early stopping triggered at epoch 64, best val loss: 0.6781Loss: 0.9603 Scheduler lr: [4.694102095183267e-05]\n",
      "Accuracy: 0.5997, precision: 0.1705, recall: 0.7672, f1: 0.2790, auc: 0.6740\n",
      "Fold 3:\n",
      "Early stopping triggered at epoch 66, best val loss: 0.6891Loss: 0.8463 Scheduler lr: [4.694102095183267e-05]\n",
      "Accuracy: 0.6463, precision: 0.1792, recall: 0.6983, f1: 0.2852, auc: 0.6694\n",
      "Fold 4:\n",
      "Early stopping triggered at epoch 59, best val loss: 0.7256Loss: 0.9062 Scheduler lr: [2.3470510475916336e-05]\n",
      "Accuracy: 0.6054, precision: 0.1568, recall: 0.6638, f1: 0.2537, auc: 0.6313\n",
      "Fold 5:\n",
      "Epoch: 56    training loss:0.5247 best_val_loss:0.7270 Val Loss: 0.9033 Scheduler lr: [4.694102095183267e-05]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-15 14:30:48,234] Trial 1 finished with value: 0.27644722764565766 and parameters: {'n_neighbours': 10, 'normalisation_method': 'MinMaxScaler', 'contamination': 0.17031717395020485, 'hidden_dim': 27, 'output_dim': 17, 'dropout': 0.01836115753896502, 'threshold': 0.3672740014381496, 'initial_lr': 0.0007510563352293228, 'max_lr': 0.005, 'weight_decay': 2.5855293605803765e-05, 'criterion': 'BCEWithLogitsLoss', 'pos_weight': 2}. Best is trial 0 with value: 0.28834272006635314.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 57, best val loss: 0.7270\n",
      "Accuracy: 0.6124, precision: 0.1755, recall: 0.7672, f1: 0.2857, auc: 0.6811\n",
      "Using device: cuda\n",
      "Fold: 1, Train: (7519, 28), Test: (1149, 28)\n",
      "Fold: 2, Train: (7459, 28), Test: (1149, 28)\n",
      "Fold: 3, Train: (7476, 28), Test: (1148, 28)\n",
      "Fold: 4, Train: (7789, 28), Test: (1148, 28)\n",
      "Fold: 5, Train: (7508, 28), Test: (1148, 28)\n",
      "Fold 1:\n",
      "Early stopping triggered at epoch 5694, best val loss: 0.7874ss: 0.8816 Scheduler lr: [1e-06]704397904931e-06]\n",
      "Accuracy: 0.6362, precision: 0.1841, recall: 0.7586, f1: 0.2963, auc: 0.6905\n",
      "Fold 2:\n",
      "Early stopping triggered at epoch 41, best val loss: 0.6812Loss: 0.8869 Scheduler lr: [9.001363518323945e-06]]\n",
      "Accuracy: 0.5222, precision: 0.1514, recall: 0.8103, f1: 0.2551, auc: 0.6501\n",
      "Fold 3:\n",
      "Early stopping triggered at epoch 8699, best val loss: 0.7491ss: 0.8218 Scheduler lr: [1e-06]704397904931e-06]\n",
      "Accuracy: 0.6298, precision: 0.1634, recall: 0.6466, f1: 0.2609, auc: 0.6372\n",
      "Fold 4:\n",
      "Early stopping triggered at epoch 6136, best val loss: 0.8060ss: 0.8533 Scheduler lr: [1e-06]704397904931e-06]\n",
      "Accuracy: 0.6019, precision: 0.1610, recall: 0.6983, f1: 0.2617, auc: 0.6447\n",
      "Fold 5:\n",
      "Epoch: 3662  training loss:0.4987 best_val_loss:0.7974 Val Loss: 0.8662 Scheduler lr: [1e-06]704397904931e-06]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-15 16:26:04,565] Trial 2 finished with value: 0.2688649605015465 and parameters: {'n_neighbours': 2, 'normalisation_method': 'MinMaxScaler', 'contamination': 0.07310606210035579, 'hidden_dim': 117, 'output_dim': 66, 'dropout': 0.14136576518190958, 'threshold': 0.36816401687849765, 'initial_lr': 0.00014402181629318312, 'max_lr': 0.005, 'weight_decay': 4.577095377910826e-05, 'criterion': 'BCEWithLogitsLoss', 'pos_weight': 2}. Best is trial 0 with value: 0.28834272006635314.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 3663, best val loss: 0.7974\n",
      "Accuracy: 0.6098, precision: 0.1667, recall: 0.7155, f1: 0.2704, auc: 0.6567\n",
      "Using device: cuda\n",
      "Fold: 1, Train: (6563, 28), Test: (1149, 28)\n",
      "Fold: 2, Train: (6565, 28), Test: (1149, 28)\n",
      "Fold: 3, Train: (6585, 28), Test: (1148, 28)\n",
      "Fold: 4, Train: (6598, 28), Test: (1148, 28)\n",
      "Fold: 5, Train: (6591, 28), Test: (1148, 28)\n",
      "Fold 1:\n",
      "Early stopping triggered at epoch 54, best val loss: 1.4234Loss: 1.5763 Scheduler lr: [4.4947044746620285e-06]\n",
      "Accuracy: 0.3481, precision: 0.1298, recall: 0.9569, f1: 0.2286, auc: 0.6183\n",
      "Fold 2:\n",
      "Early stopping triggered at epoch 178, best val loss: 1.4117oss: 1.5381 Scheduler lr: [1e-06]761186655071e-06]\n",
      "Accuracy: 0.3934, precision: 0.1337, recall: 0.9138, f1: 0.2332, auc: 0.6244\n",
      "Fold 3:\n",
      "Early stopping triggered at epoch 41, best val loss: 1.2348Loss: 1.5288 Scheduler lr: [1.7978817898648114e-05]\n",
      "Accuracy: 0.3589, precision: 0.1265, recall: 0.9052, f1: 0.2220, auc: 0.6013\n",
      "Fold 4:\n",
      "Early stopping triggered at epoch 57, best val loss: 1.4205Loss: 1.5882 Scheduler lr: [4.4947044746620285e-06]\n",
      "Accuracy: 0.3859, precision: 0.1305, recall: 0.8966, f1: 0.2278, auc: 0.6125\n",
      "Fold 5:\n",
      "Epoch: 40    training loss:1.1002 best_val_loss:1.1315 Val Loss: 1.5515 Scheduler lr: [1.7978817898648114e-05]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-15 16:27:44,276] Trial 3 finished with value: 0.22650652301192947 and parameters: {'n_neighbours': 8, 'normalisation_method': 'MinMaxScaler', 'contamination': 0.1896045488070047, 'hidden_dim': 105, 'output_dim': 30, 'dropout': 0.17036582293550362, 'threshold': 0.6454742768745008, 'initial_lr': 0.0002876610863783698, 'max_lr': 0.005, 'weight_decay': 0.0007633329722882002, 'criterion': 'BCEWithLogitsLoss', 'pos_weight': 7}. Best is trial 0 with value: 0.28834272006635314.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 41, best val loss: 1.1315\n",
      "Accuracy: 0.3301, precision: 0.1251, recall: 0.9397, f1: 0.2209, auc: 0.6006\n",
      "Using device: cuda\n",
      "Fold: 1, Train: (7091, 28), Test: (1149, 28)\n",
      "Fold: 2, Train: (7114, 28), Test: (1149, 28)\n",
      "Fold: 3, Train: (7059, 28), Test: (1148, 28)\n",
      "Fold: 4, Train: (7083, 28), Test: (1148, 28)\n",
      "Fold: 5, Train: (7103, 28), Test: (1148, 28)\n",
      "Fold 1:\n",
      "Accuracy: 0.4404, precision: 0.1444, recall: 0.9224, f1: 0.2497, auc: 0.6543duler lr: [1e-06]351551271068e-06]\n",
      "Fold 2:\n",
      "Early stopping triggered at epoch 41, best val loss: 1.0903Loss: 1.4174 Scheduler lr: [9.537881241016854e-06]]\n",
      "Accuracy: 0.3185, precision: 0.1249, recall: 0.9569, f1: 0.2209, auc: 0.6019\n",
      "Fold 3:\n",
      "Early stopping triggered at epoch 41, best val loss: 1.0707Loss: 1.4978 Scheduler lr: [9.537881241016854e-06]]\n",
      "Accuracy: 0.3249, precision: 0.1234, recall: 0.9310, f1: 0.2180, auc: 0.5939\n",
      "Fold 4:\n",
      "Early stopping triggered at epoch 41, best val loss: 1.0608Loss: 1.4541 Scheduler lr: [9.537881241016854e-06]]\n",
      "Accuracy: 0.3336, precision: 0.1257, recall: 0.9397, f1: 0.2218, auc: 0.6026\n",
      "Fold 5:\n",
      "Epoch: 40    training loss:1.1753 best_val_loss:1.0218 Val Loss: 1.4802 Scheduler lr: [9.537881241016854e-06]]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-15 17:14:39,695] Trial 4 finished with value: 0.22625539121005667 and parameters: {'n_neighbours': 10, 'normalisation_method': 'MinMaxScaler', 'contamination': 0.1498316305673345, 'hidden_dim': 100, 'output_dim': 45, 'dropout': 0.2527958820029409, 'threshold': 0.5353706424873137, 'initial_lr': 0.00015260609985626967, 'max_lr': 0.005, 'weight_decay': 7.69568443341603e-05, 'criterion': 'BCEWithLogitsLoss', 'pos_weight': 6}. Best is trial 0 with value: 0.28834272006635314.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 41, best val loss: 1.0218\n",
      "Accuracy: 0.2936, precision: 0.1243, recall: 0.9914, f1: 0.2209, auc: 0.6032\n",
      "Using device: cuda\n",
      "Fold: 1, Train: (7376, 28), Test: (1149, 28)\n",
      "Fold: 2, Train: (7390, 28), Test: (1149, 28)\n",
      "Fold: 3, Train: (7348, 28), Test: (1148, 28)\n",
      "Fold: 4, Train: (7319, 28), Test: (1148, 28)\n",
      "Fold: 5, Train: (7358, 28), Test: (1148, 28)\n",
      "Fold 1:\n",
      "Early stopping triggered at epoch 41, best val loss: 1.3682Loss: 1.6106 Scheduler lr: [1.4049323527560213e-05]\n",
      "Accuracy: 0.2837, precision: 0.1211, recall: 0.9741, f1: 0.2154, auc: 0.5902\n",
      "Fold 2:\n",
      "Early stopping triggered at epoch 41, best val loss: 1.2792Loss: 1.6585 Scheduler lr: [1.4049323527560213e-05]\n",
      "Accuracy: 0.2916, precision: 0.1223, recall: 0.9741, f1: 0.2173, auc: 0.5945\n",
      "Fold 3:\n",
      "Early stopping triggered at epoch 41, best val loss: 1.3098Loss: 1.7181 Scheduler lr: [1.4049323527560213e-05]\n",
      "Accuracy: 0.3014, precision: 0.1206, recall: 0.9397, f1: 0.2137, auc: 0.5847\n",
      "Fold 4:\n",
      "Early stopping triggered at epoch 41, best val loss: 1.2389Loss: 1.6969 Scheduler lr: [1.4049323527560213e-05]\n",
      "Accuracy: 0.2848, precision: 0.1214, recall: 0.9741, f1: 0.2159, auc: 0.5908\n",
      "Fold 5:\n",
      "Epoch: 40    training loss:1.2947 best_val_loss:1.3057 Val Loss: 1.6910 Scheduler lr: [1.4049323527560213e-05]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-15 17:15:41,448] Trial 5 finished with value: 0.21573001414016607 and parameters: {'n_neighbours': 6, 'normalisation_method': 'MinMaxScaler', 'contamination': 0.11720660635612144, 'hidden_dim': 116, 'output_dim': 44, 'dropout': 0.27566006698813056, 'threshold': 0.2957679027550518, 'initial_lr': 0.0002247891764409634, 'max_lr': 0.005, 'weight_decay': 0.00011117296628065934, 'criterion': 'BCEWithLogitsLoss', 'pos_weight': 9}. Best is trial 0 with value: 0.28834272006635314.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 41, best val loss: 1.3057\n",
      "Accuracy: 0.2805, precision: 0.1215, recall: 0.9828, f1: 0.2163, auc: 0.5922\n",
      "Using device: cuda\n",
      "Fold: 1, Train: (7135, 28), Test: (1149, 28)\n",
      "Fold: 2, Train: (7147, 28), Test: (1149, 28)\n",
      "Fold: 3, Train: (7164, 28), Test: (1148, 28)\n",
      "Fold: 4, Train: (7120, 28), Test: (1148, 28)\n",
      "Fold: 5, Train: (7228, 28), Test: (1148, 28)\n",
      "Fold 1:\n",
      "Early stopping triggered at epoch 41, best val loss: 1.1130Loss: 1.3968 Scheduler lr: [3.12402241550529e-05]]\n",
      "Accuracy: 0.4282, precision: 0.1427, recall: 0.9310, f1: 0.2474, auc: 0.6514\n",
      "Fold 2:\n",
      "Early stopping triggered at epoch 370, best val loss: 1.3250oss: 1.4120 Scheduler lr: [1e-06]140096908064e-06]\n",
      "Accuracy: 0.4665, precision: 0.1435, recall: 0.8621, f1: 0.2460, auc: 0.6421\n",
      "Fold 3:\n",
      "Early stopping triggered at epoch 56, best val loss: 1.0788Loss: 1.4226 Scheduler lr: [3.12402241550529e-05]]\n",
      "Accuracy: 0.5113, precision: 0.1474, recall: 0.8017, f1: 0.2490, auc: 0.6402\n",
      "Fold 4:\n",
      "Early stopping triggered at epoch 5691, best val loss: 1.3675ss: 1.4679 Scheduler lr: [1e-06]140096908064e-06]\n",
      "Accuracy: 0.4634, precision: 0.1387, recall: 0.8276, f1: 0.2376, auc: 0.6250\n",
      "Fold 5:\n",
      "Epoch: 9050  training loss:0.7214 best_val_loss:1.2996 Val Loss: 1.3845 Scheduler lr: [1e-06]140096908064e-06]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-15 18:19:18,530] Trial 6 finished with value: 0.24734315778181074 and parameters: {'n_neighbours': 2, 'normalisation_method': 'MinMaxScaler', 'contamination': 0.14084238413454167, 'hidden_dim': 96, 'output_dim': 13, 'dropout': 0.1969441792158558, 'threshold': 0.17559527142335513, 'initial_lr': 0.0004998435864808464, 'max_lr': 0.005, 'weight_decay': 0.0016983542896893723, 'criterion': 'BCEWithLogitsLoss', 'pos_weight': 6}. Best is trial 0 with value: 0.28834272006635314.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 9051, best val loss: 1.2996\n",
      "Accuracy: 0.4904, precision: 0.1505, recall: 0.8707, f1: 0.2567, auc: 0.6592\n",
      "Using device: cuda\n",
      "Fold: 1, Train: (7469, 28), Test: (1149, 28)\n",
      "Fold: 2, Train: (7449, 28), Test: (1149, 28)\n",
      "Fold: 3, Train: (7435, 28), Test: (1148, 28)\n",
      "Fold: 4, Train: (7419, 28), Test: (1148, 28)\n",
      "Fold: 5, Train: (7434, 28), Test: (1148, 28)\n",
      "Fold 1:\n",
      "Early stopping triggered at epoch 41, best val loss: 1.2891Loss: 1.5221 Scheduler lr: [2.4909564860978997e-05]\n",
      "Accuracy: 0.2768, precision: 0.1193, recall: 0.9655, f1: 0.2123, auc: 0.5825\n",
      "Fold 2:\n",
      "Early stopping triggered at epoch 41, best val loss: 1.3075Loss: 1.6400 Scheduler lr: [2.4909564860978997e-05]\n",
      "Accuracy: 0.2881, precision: 0.1218, recall: 0.9741, f1: 0.2165, auc: 0.5926\n",
      "Fold 3:\n",
      "Early stopping triggered at epoch 114, best val loss: 1.4069oss: 1.5235 Scheduler lr: [1e-06]478038111873e-06]\n",
      "Accuracy: 0.3328, precision: 0.1256, recall: 0.9397, f1: 0.2215, auc: 0.6021\n",
      "Fold 4:\n",
      "Early stopping triggered at epoch 41, best val loss: 1.2922Loss: 1.6079 Scheduler lr: [2.4909564860978997e-05]\n",
      "Accuracy: 0.3066, precision: 0.1239, recall: 0.9655, f1: 0.2196, auc: 0.5990\n",
      "Fold 5:\n",
      "Epoch: 40    training loss:1.3090 best_val_loss:1.2833 Val Loss: 1.6691 Scheduler lr: [2.4909564860978997e-05]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-15 18:20:33,109] Trial 7 finished with value: 0.21738621226536767 and parameters: {'n_neighbours': 8, 'normalisation_method': 'MinMaxScaler', 'contamination': 0.09262424225599869, 'hidden_dim': 53, 'output_dim': 25, 'dropout': 0.27205196370208534, 'threshold': 0.8193544515680589, 'initial_lr': 0.00039855303777566395, 'max_lr': 0.005, 'weight_decay': 0.0022726007022374354, 'criterion': 'BCEWithLogitsLoss', 'pos_weight': 8}. Best is trial 0 with value: 0.28834272006635314.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 41, best val loss: 1.2833\n",
      "Accuracy: 0.2770, precision: 0.1218, recall: 0.9914, f1: 0.2170, auc: 0.5940\n",
      "Using device: cuda\n",
      "Fold: 1, Train: (7614, 28), Test: (1149, 28)\n",
      "Fold: 2, Train: (7560, 28), Test: (1149, 28)\n",
      "Fold: 3, Train: (7601, 28), Test: (1148, 28)\n",
      "Fold: 4, Train: (7584, 28), Test: (1148, 28)\n",
      "Fold: 5, Train: (7621, 28), Test: (1148, 28)\n",
      "Fold 1:\n",
      "Early stopping triggered at epoch 41, best val loss: 1.0921Loss: 1.5916 Scheduler lr: [7.932817137590985e-06]]\n",
      "Accuracy: 0.2959, precision: 0.1213, recall: 0.9569, f1: 0.2153, auc: 0.5893\n",
      "Fold 2:\n",
      "Early stopping triggered at epoch 41, best val loss: 1.3167Loss: 1.5478 Scheduler lr: [7.932817137590985e-06]]\n",
      "Accuracy: 0.3255, precision: 0.1277, recall: 0.9741, f1: 0.2258, auc: 0.6134\n",
      "Fold 3:\n",
      "Early stopping triggered at epoch 496, best val loss: 1.4497oss: 1.5443 Scheduler lr: [1e-06]04284397746e-06]]\n",
      "Accuracy: 0.3833, precision: 0.1291, recall: 0.8879, f1: 0.2254, auc: 0.6072\n",
      "Fold 4:\n",
      "Early stopping triggered at epoch 41, best val loss: 1.1861Loss: 1.6006 Scheduler lr: [7.932817137590985e-06]]\n",
      "Accuracy: 0.3232, precision: 0.1249, recall: 0.9483, f1: 0.2207, auc: 0.6006\n",
      "Fold 5:\n",
      "Epoch: 40    training loss:1.2459 best_val_loss:1.1802 Val Loss: 1.6271 Scheduler lr: [7.932817137590985e-06]]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-15 18:23:30,430] Trial 8 finished with value: 0.22007032366466098 and parameters: {'n_neighbours': 6, 'normalisation_method': 'MinMaxScaler', 'contamination': 0.07654792854624155, 'hidden_dim': 116, 'output_dim': 37, 'dropout': 0.1384030489293469, 'threshold': 0.6109231905473368, 'initial_lr': 0.00012692507420145575, 'max_lr': 0.005, 'weight_decay': 9.094607647143013e-05, 'criterion': 'BCEWithLogitsLoss', 'pos_weight': 8}. Best is trial 0 with value: 0.28834272006635314.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 41, best val loss: 1.1802\n",
      "Accuracy: 0.2735, precision: 0.1197, recall: 0.9741, f1: 0.2132, auc: 0.5845\n",
      "Using device: cuda\n",
      "Fold: 1, Train: (6689, 28), Test: (1149, 28)\n",
      "Fold: 2, Train: (6789, 28), Test: (1149, 28)\n",
      "Fold: 3, Train: (6802, 28), Test: (1148, 28)\n",
      "Fold: 4, Train: (6777, 28), Test: (1148, 28)\n",
      "Fold: 5, Train: (6808, 28), Test: (1148, 28)\n",
      "Fold 1:\n",
      "Early stopping triggered at epoch 41, best val loss: 1.2384Loss: 1.6097 Scheduler lr: [1.4675702072821036e-05]\n",
      "Accuracy: 0.2820, precision: 0.1233, recall: 1.0000, f1: 0.2195, auc: 0.6007\n",
      "Fold 2:\n",
      "Accuracy: 0.3864, precision: 0.1323, recall: 0.9138, f1: 0.2312, auc: 0.6205duler lr: [1e-06]627591026296e-06]\n",
      "Fold 3:\n",
      "Early stopping triggered at epoch 42, best val loss: 1.1672Loss: 1.6481 Scheduler lr: [1.4675702072821036e-05]\n",
      "Accuracy: 0.2909, precision: 0.1182, recall: 0.9310, f1: 0.2097, auc: 0.5750\n",
      "Fold 4:\n",
      "Early stopping triggered at epoch 41, best val loss: 1.2675Loss: 1.6586 Scheduler lr: [1.4675702072821036e-05]\n",
      "Accuracy: 0.3031, precision: 0.1225, recall: 0.9569, f1: 0.2172, auc: 0.5933\n",
      "Fold 5:\n",
      "Epoch: 41    training loss:1.2568 best_val_loss:1.1508 Val Loss: 1.7099 Scheduler lr: [1.4675702072821036e-05]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-15 19:03:11,583] Trial 9 finished with value: 0.21839198700643583 and parameters: {'n_neighbours': 4, 'normalisation_method': 'MinMaxScaler', 'contamination': 0.1740378103170311, 'hidden_dim': 58, 'output_dim': 8, 'dropout': 0.2060698001374215, 'threshold': 0.31497790624844435, 'initial_lr': 0.00023481123316513658, 'max_lr': 0.005, 'weight_decay': 0.000929689999501736, 'criterion': 'BCEWithLogitsLoss', 'pos_weight': 8}. Best is trial 0 with value: 0.28834272006635314.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 42, best val loss: 1.1508\n",
      "Accuracy: 0.2657, precision: 0.1202, recall: 0.9914, f1: 0.2144, auc: 0.5877\n",
      "Using device: cuda\n",
      "Fold: 1, Train: (8262, 28), Test: (1149, 28)\n",
      "Fold: 2, Train: (8192, 28), Test: (1149, 28)\n",
      "Fold: 3, Train: (8152, 28), Test: (1148, 28)\n",
      "Fold: 4, Train: (8186, 28), Test: (1148, 28)\n",
      "Fold: 5, Train: (8171, 28), Test: (1148, 28)\n",
      "Fold 1:\n",
      "Early stopping triggered at epoch 50, best val loss: 0.7268Loss: 1.1602 Scheduler lr: [5.550655153023176e-05]]\n",
      "Accuracy: 0.6110, precision: 0.1697, recall: 0.7328, f1: 0.2755, auc: 0.6650\n",
      "Fold 2:\n",
      "Early stopping triggered at epoch 53, best val loss: 0.8022Loss: 1.0687 Scheduler lr: [2.775327576511588e-05]]\n",
      "Accuracy: 0.5962, precision: 0.1615, recall: 0.7155, f1: 0.2635, auc: 0.6491\n",
      "Fold 3:\n",
      "Early stopping triggered at epoch 52, best val loss: 0.8828Loss: 1.1647 Scheduler lr: [5.550655153023176e-05]]\n",
      "Accuracy: 0.5984, precision: 0.1501, recall: 0.6379, f1: 0.2430, auc: 0.6160\n",
      "Fold 4:\n",
      "Early stopping triggered at epoch 49, best val loss: 0.8433Loss: 1.0829 Scheduler lr: [5.550655153023176e-05]]\n",
      "Accuracy: 0.6672, precision: 0.1641, recall: 0.5603, f1: 0.2539, auc: 0.6198\n",
      "Fold 5:\n",
      "Epoch: 53    training loss:0.4637 best_val_loss:0.8414 Val Loss: 1.1163 Scheduler lr: [5.550655153023176e-05]]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-15 19:04:28,349] Trial 10 finished with value: 0.26452486224015004 and parameters: {'n_neighbours': 4, 'normalisation_method': 'MinMaxScaler', 'contamination': 0.016906804835630534, 'hidden_dim': 79, 'output_dim': 79, 'dropout': 0.062352395074204175, 'threshold': 0.11898690729204145, 'initial_lr': 0.0008881048244837081, 'max_lr': 0.005, 'weight_decay': 0.008473280970515746, 'criterion': 'BCEWithLogitsLoss', 'pos_weight': 4}. Best is trial 0 with value: 0.28834272006635314.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 54, best val loss: 0.8414\n",
      "Accuracy: 0.6315, precision: 0.1782, recall: 0.7328, f1: 0.2867, auc: 0.6765\n",
      "Using device: cuda\n",
      "Fold: 1, Train: (7088, 28), Test: (1149, 28)\n",
      "Fold: 2, Train: (7066, 28), Test: (1149, 28)\n",
      "Fold: 3, Train: (7041, 28), Test: (1148, 28)\n",
      "Fold: 4, Train: (7057, 28), Test: (1148, 28)\n",
      "Fold: 5, Train: (7075, 28), Test: (1148, 28)\n",
      "Fold 1:\n",
      "Early stopping triggered at epoch 52, best val loss: 0.4644Loss: 0.6370 Scheduler lr: [3.0446025234027104e-05]\n",
      "Accuracy: 0.6876, precision: 0.2044, recall: 0.7241, f1: 0.3188, auc: 0.7038\n",
      "Fold 2:\n",
      "Early stopping triggered at epoch 63, best val loss: 0.4477Loss: 0.6578 Scheduler lr: [6.089205046805421e-05]]\n",
      "Accuracy: 0.6710, precision: 0.1851, recall: 0.6638, f1: 0.2895, auc: 0.6678\n",
      "Fold 3:\n",
      "Early stopping triggered at epoch 60, best val loss: 0.4634Loss: 0.6192 Scheduler lr: [6.089205046805421e-05]]\n",
      "Accuracy: 0.6855, precision: 0.1785, recall: 0.5862, f1: 0.2736, auc: 0.6415\n",
      "Fold 4:\n",
      "Early stopping triggered at epoch 51, best val loss: 0.4832Loss: 0.6461 Scheduler lr: [3.0446025234027104e-05]\n",
      "Accuracy: 0.6969, precision: 0.1979, recall: 0.6552, f1: 0.3040, auc: 0.6784\n",
      "Fold 5:\n",
      "Epoch: 53    training loss:0.4031 best_val_loss:0.5507 Val Loss: 0.6397 Scheduler lr: [6.089205046805421e-05]]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-15 19:05:39,671] Trial 11 finished with value: 0.29452789306385496 and parameters: {'n_neighbours': 10, 'normalisation_method': 'MinMaxScaler', 'contamination': 0.15289884637902493, 'hidden_dim': 18, 'output_dim': 2, 'dropout': 0.015579037185162556, 'threshold': 0.4341714470166267, 'initial_lr': 0.0009742728074888673, 'max_lr': 0.005, 'weight_decay': 1.2724744609435793e-05, 'criterion': 'BCEWithLogitsLoss', 'pos_weight': 1}. Best is trial 11 with value: 0.29452789306385496.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 54, best val loss: 0.5507\n",
      "Accuracy: 0.6533, precision: 0.1810, recall: 0.6897, f1: 0.2867, auc: 0.6694\n",
      "Using device: cuda\n",
      "Fold: 1, Train: (7309, 28), Test: (1149, 28)\n",
      "Fold: 2, Train: (7297, 28), Test: (1149, 28)\n",
      "Fold: 3, Train: (7298, 28), Test: (1148, 28)\n",
      "Fold: 4, Train: (7262, 28), Test: (1148, 28)\n",
      "Fold: 5, Train: (7278, 28), Test: (1148, 28)\n",
      "Fold 1:\n",
      "Early stopping triggered at epoch 57, best val loss: 0.3851Loss: 0.6203 Scheduler lr: [1.9507748179945178e-05]\n",
      "Accuracy: 0.6806, precision: 0.1976, recall: 0.7069, f1: 0.3089, auc: 0.6923\n",
      "Fold 2:\n",
      "Early stopping triggered at epoch 70, best val loss: 0.3523Loss: 0.6483 Scheduler lr: [3.9015496359890356e-05]\n",
      "Accuracy: 0.6745, precision: 0.1759, recall: 0.6034, f1: 0.2724, auc: 0.6430\n",
      "Fold 3:\n",
      "Early stopping triggered at epoch 80, best val loss: 0.4535Loss: 0.5932 Scheduler lr: [4.8769370449862944e-06]\n",
      "Accuracy: 0.7012, precision: 0.1907, recall: 0.6034, f1: 0.2899, auc: 0.6578\n",
      "Fold 4:\n",
      "Early stopping triggered at epoch 80, best val loss: 0.5104Loss: 0.5777 Scheduler lr: [4.8769370449862944e-06]\n",
      "Accuracy: 0.6838, precision: 0.1858, recall: 0.6293, f1: 0.2868, auc: 0.6596\n",
      "Fold 5:\n",
      "Epoch: 58    training loss:0.4214 best_val_loss:0.3678 Val Loss: 0.5974 Scheduler lr: [3.9015496359890356e-05]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-15 19:07:09,295] Trial 12 finished with value: 0.2914681913706582 and parameters: {'n_neighbours': 4, 'normalisation_method': 'MinMaxScaler', 'contamination': 0.1259069834495496, 'hidden_dim': 22, 'output_dim': 2, 'dropout': 0.07563111536171382, 'threshold': 0.7931066881162929, 'initial_lr': 0.0006242479417582457, 'max_lr': 0.005, 'weight_decay': 0.009550520795139903, 'criterion': 'BCEWithLogitsLoss', 'pos_weight': 1}. Best is trial 11 with value: 0.29452789306385496.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 59, best val loss: 0.3678\n",
      "Accuracy: 0.6821, precision: 0.1926, recall: 0.6724, f1: 0.2994, auc: 0.6778\n",
      "Using device: cuda\n",
      "Fold: 1, Train: (7397, 28), Test: (1149, 28)\n",
      "Fold: 2, Train: (7396, 28), Test: (1149, 28)\n",
      "Fold: 3, Train: (7365, 28), Test: (1148, 28)\n",
      "Fold: 4, Train: (7378, 28), Test: (1148, 28)\n",
      "Fold: 5, Train: (7379, 28), Test: (1148, 28)\n",
      "Fold 1:\n",
      "Early stopping triggered at epoch 79, best val loss: 1.0347Loss: 1.1993 Scheduler lr: [3.514681547503269e-05]]\n",
      "Accuracy: 0.4708, precision: 0.1466, recall: 0.8793, f1: 0.2512, auc: 0.6521\n",
      "Fold 2:\n",
      "Early stopping triggered at epoch 203, best val loss: 1.0806oss: 1.1768 Scheduler lr: [1e-06]379835947715e-06]\n",
      "Accuracy: 0.4726, precision: 0.1460, recall: 0.8707, f1: 0.2500, auc: 0.6493\n",
      "Fold 3:\n",
      "Early stopping triggered at epoch 41, best val loss: 0.8133Loss: 1.2497 Scheduler lr: [3.514681547503269e-05]]\n",
      "Accuracy: 0.4155, precision: 0.1363, recall: 0.8966, f1: 0.2366, auc: 0.6290\n",
      "Fold 4:\n",
      "Early stopping triggered at epoch 41, best val loss: 0.9894Loss: 1.2284 Scheduler lr: [3.514681547503269e-05]]\n",
      "Accuracy: 0.4251, precision: 0.1344, recall: 0.8621, f1: 0.2326, auc: 0.6190\n",
      "Fold 5:\n",
      "Epoch: 40    training loss:0.9202 best_val_loss:0.9991 Val Loss: 1.2603 Scheduler lr: [3.514681547503269e-05]]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-15 19:08:55,013] Trial 13 finished with value: 0.23875869097899427 and parameters: {'n_neighbours': 6, 'normalisation_method': 'MinMaxScaler', 'contamination': 0.11292185730914271, 'hidden_dim': 19, 'output_dim': 2, 'dropout': 0.0724514035845064, 'threshold': 0.8889187810515001, 'initial_lr': 0.000562349047600523, 'max_lr': 0.005, 'weight_decay': 0.00030391921066428993, 'criterion': 'BCEWithLogitsLoss', 'pos_weight': 4}. Best is trial 11 with value: 0.29452789306385496.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 41, best val loss: 0.9991\n",
      "Accuracy: 0.3458, precision: 0.1269, recall: 0.9310, f1: 0.2234, auc: 0.6055\n",
      "Using device: cuda\n",
      "Fold: 1, Train: (7864, 28), Test: (1149, 28)\n",
      "Fold: 2, Train: (7856, 28), Test: (1149, 28)\n",
      "Fold: 3, Train: (7851, 28), Test: (1148, 28)\n",
      "Fold: 4, Train: (8015, 28), Test: (1148, 28)\n",
      "Fold: 5, Train: (8006, 28), Test: (1148, 28)\n",
      "Fold 1:\n",
      "Early stopping triggered at epoch 47, best val loss: 0.3912Loss: 0.5263 Scheduler lr: [6.210070780197365e-05]\n",
      "Accuracy: 0.7354, precision: 0.1928, recall: 0.5086, f1: 0.2796, auc: 0.6348\n",
      "Fold 2:\n",
      "Early stopping triggered at epoch 70, best val loss: 0.4205Loss: 0.5951 Scheduler lr: [3.1050353900986827e-05]\n",
      "Accuracy: 0.7215, precision: 0.1812, recall: 0.5000, f1: 0.2661, auc: 0.6232\n",
      "Fold 3:\n",
      "Early stopping triggered at epoch 54, best val loss: 0.4141Loss: 0.6058 Scheduler lr: [6.210070780197365e-05]\n",
      "Accuracy: 0.7230, precision: 0.1883, recall: 0.5259, f1: 0.2773, auc: 0.6355\n",
      "Fold 4:\n",
      "Early stopping triggered at epoch 49, best val loss: 0.4370Loss: 0.6290 Scheduler lr: [6.210070780197365e-05]\n",
      "Accuracy: 0.7265, precision: 0.1765, recall: 0.4655, f1: 0.2559, auc: 0.6107\n",
      "Fold 5:\n",
      "Epoch: 52    training loss:0.2968 best_val_loss:0.4616 Val Loss: 0.5981 Scheduler lr: [3.1050353900986827e-05]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-15 19:10:12,583] Trial 14 finished with value: 0.2717942153878499 and parameters: {'n_neighbours': 8, 'normalisation_method': 'MinMaxScaler', 'contamination': 0.04260460660794355, 'hidden_dim': 37, 'output_dim': 2, 'dropout': 0.0065964547489934106, 'threshold': 0.7261200550528455, 'initial_lr': 0.0009936113248315784, 'max_lr': 0.005, 'weight_decay': 1.0124221727419189e-05, 'criterion': 'BCEWithLogitsLoss', 'pos_weight': 1}. Best is trial 11 with value: 0.29452789306385496.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 53, best val loss: 0.4616\n",
      "Accuracy: 0.7448, precision: 0.1959, recall: 0.4914, f1: 0.2801, auc: 0.6323\n",
      "Using device: cuda\n",
      "Fold: 1, Train: (7124, 28), Test: (1149, 28)\n",
      "Fold: 2, Train: (7103, 28), Test: (1149, 28)\n",
      "Fold: 3, Train: (7106, 28), Test: (1148, 28)\n",
      "Fold: 4, Train: (7039, 28), Test: (1148, 28)\n",
      "Fold: 5, Train: (7119, 28), Test: (1148, 28)\n",
      "Fold 1:\n",
      "Early stopping triggered at epoch 62, best val loss: 0.8631Loss: 1.1735 Scheduler lr: [3.565710454882526e-05]]\n",
      "Accuracy: 0.5352, precision: 0.1607, recall: 0.8534, f1: 0.2705, auc: 0.6765\n",
      "Fold 2:\n",
      "Early stopping triggered at epoch 57, best val loss: 1.0176Loss: 1.2231 Scheduler lr: [1.782855227441263e-05]]\n",
      "Accuracy: 0.5422, precision: 0.1606, recall: 0.8362, f1: 0.2694, auc: 0.6727\n",
      "Fold 3:\n",
      "Early stopping triggered at epoch 59, best val loss: 1.0583Loss: 1.2060 Scheduler lr: [1.782855227441263e-05]]\n",
      "Accuracy: 0.4991, precision: 0.1453, recall: 0.8103, f1: 0.2464, auc: 0.6372\n",
      "Fold 4:\n",
      "Early stopping triggered at epoch 41, best val loss: 0.9564Loss: 1.2253 Scheduler lr: [3.565710454882526e-05]]\n",
      "Accuracy: 0.4503, precision: 0.1419, recall: 0.8793, f1: 0.2443, auc: 0.6407\n",
      "Fold 5:\n",
      "Epoch: 9656  training loss:0.6716 best_val_loss:1.1799 Val Loss: 1.2578 Scheduler lr: [1e-06]845171507894e-06]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-15 19:53:24,672] Trial 15 finished with value: 0.2554860126776442 and parameters: {'n_neighbours': 3, 'normalisation_method': 'MinMaxScaler', 'contamination': 0.15444028351316474, 'hidden_dim': 45, 'output_dim': 9, 'dropout': 0.0775387355375054, 'threshold': 0.49631967357318946, 'initial_lr': 0.0005705136727812042, 'max_lr': 0.005, 'weight_decay': 0.0002334500359623925, 'criterion': 'BCEWithLogitsLoss', 'pos_weight': 4}. Best is trial 11 with value: 0.29452789306385496.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 9657, best val loss: 1.1799\n",
      "Accuracy: 0.4895, precision: 0.1450, recall: 0.8276, f1: 0.2468, auc: 0.6396\n",
      "Using device: cuda\n",
      "Fold: 1, Train: (6658, 28), Test: (1149, 28)\n",
      "Fold: 2, Train: (6599, 28), Test: (1149, 28)\n",
      "Fold: 3, Train: (6623, 28), Test: (1148, 28)\n",
      "Fold: 4, Train: (6609, 28), Test: (1148, 28)\n",
      "Fold: 5, Train: (6635, 28), Test: (1148, 28)\n",
      "Fold 1:\n",
      "Early stopping triggered at epoch 68, best val loss: 1.0302Loss: 1.0954 Scheduler lr: [1.6322064598762537e-06]\n",
      "Accuracy: 0.4386, precision: 0.1440, recall: 0.9224, f1: 0.2491, auc: 0.6534\n",
      "Fold 2:\n",
      "Accuracy: 0.4839, precision: 0.1508, recall: 0.8879, f1: 0.2578, auc: 0.6632duler lr: [1e-06]064598762537e-06]\n",
      "Fold 3:\n",
      "Accuracy: 0.4983, precision: 0.1505, recall: 0.8534, f1: 0.2558, auc: 0.6559duler lr: [1e-06]064598762537e-06]\n",
      "Fold 4:\n",
      "Epoch: 9753  training loss:0.8119 best_val_loss:1.0404 Val Loss: 1.0816 Scheduler lr: [1e-06]064598762537e-06]\r"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import optuna\n",
    "from optuna_dashboard import run_server\n",
    "\n",
    "def start_dashboard():\n",
    "    run_server(storage)\n",
    "\n",
    "storage = optuna.storages.InMemoryStorage()\n",
    "study = optuna.create_study(direction=\"maximize\", storage=storage, study_name=\"Basic\")\n",
    "\n",
    "# Start dashboard in a separate thread\n",
    "dashboard_thread = threading.Thread(target=start_dashboard, daemon=True)\n",
    "dashboard_thread.start()\n",
    "\n",
    "# Run optimization\n",
    "study.optimize(maximise_combined_score, n_trials=30)\n",
    "\n",
    "# After optimization, print results\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(f\"  Combined score: {trial.value}\")\n",
    "print(\"  Best hyperparameters:\")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fbf56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import threading\n",
    "# import optuna\n",
    "# from optuna_dashboard import run_server\n",
    "# # !fuser -k 8080/tcp\n",
    "\n",
    "# # Define your persistent storage\n",
    "# storage = \"sqlite:///jya.db\"\n",
    "\n",
    "# # Create or load your study\n",
    "# study_name = \"jya\"\n",
    "# try:\n",
    "#     study = optuna.load_study(study_name=study_name, storage=storage)\n",
    "# except KeyError:\n",
    "#     study = optuna.create_study(study_name=study_name, direction=\"maximize\", storage=storage)\n",
    "\n",
    "# # Start Optuna Dashboard in a separate thread\n",
    "# dashboard_thread = threading.Thread(target=lambda: run_server(storage), daemon=True)\n",
    "# dashboard_thread.start()\n",
    "\n",
    "# # Run optimization\n",
    "# study.optimize(maximise_combined_score, n_trials=2000)\n",
    "\n",
    "# # Print results\n",
    "# print(\"Best trial:\")\n",
    "# trial = study.best_trial\n",
    "# print(f\"  Combined score: {trial.value}\")\n",
    "# print(\"  Best hyperparameters:\")\n",
    "# for key, value in trial.params.items():\n",
    "#     print(f\"    {key}: {value}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
