{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 90/10 initial split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomState = 42\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "raw_dataset = pd.read_csv(\"./data/processed_data.csv\") #data has X and Y\n",
    "X = raw_dataset.drop(columns=[\"DR\"])\n",
    "Y = pd.DataFrame(raw_dataset[\"DR\"])\n",
    "\n",
    "#* 90/10 split for training and final test\n",
    "X_FOR_FOLDS, X_FINAL_TEST, Y_FOR_FOLDS, Y_FINAL_TEST = train_test_split(X, Y, test_size=0.1, random_state=randomState, stratify=Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing & folds generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import (\n",
    "    MaxAbsScaler,\n",
    "    MinMaxScaler,\n",
    "    Normalizer,\n",
    "    PowerTransformer,\n",
    "    QuantileTransformer,\n",
    "    RobustScaler,\n",
    "    StandardScaler,\n",
    "    minmax_scale,\n",
    ")\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from imblearn.pipeline import Pipeline\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FOLDS_GENERATOR(X, Y, normalisation_method=PowerTransformer(method=\"yeo-johnson\"), n_splits=5, randomState=None, oversample=False, n_features_to_select=15):\n",
    "    \n",
    "    \"\"\"\n",
    "    Generates stratified folds with specified normalization.\n",
    "    \n",
    "    For list of scalers, see:\n",
    "    https://scikit-learn.org/stable/api/sklearn.preprocessing.html\n",
    "    \n",
    "    For more details on scaling and normalization effects, see:\n",
    "    https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#\n",
    "    \n",
    "    normalisation_method should be an instance of a scaler, e.g.,\n",
    "    - MinMaxScaler()\n",
    "    - MaxAbsScaler()\n",
    "    - Quantile_Transform(output_distribution='uniform')\n",
    "    \n",
    "    Returns a list of tuples, each containing:\n",
    "    (X_train_scaled, X_test_scaled, Y_train, Y_test), representing data for each fold\n",
    "    \"\"\"\n",
    "    kF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=randomState)\n",
    "    kFolds_list = []\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(kF.split(X, Y)):\n",
    "        # Split the data into training and testing sets for this fold\n",
    "        X_train, X_test = X.iloc[train_idx].copy(), X.iloc[test_idx].copy()\n",
    "        Y_train, Y_test = Y.iloc[train_idx].copy(), Y.iloc[test_idx].copy()\n",
    "                \n",
    "        # Outlier Removal\n",
    "        iso_forest = IsolationForest(contamination=0.05, random_state=randomState)\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\", UserWarning)\n",
    "            outliers = iso_forest.fit_predict(X_train)\n",
    "            \n",
    "        outliers = iso_forest.fit_predict(X_train)\n",
    "        X_train = X_train[outliers == 1]\n",
    "        Y_train = Y_train[outliers == 1]\n",
    "\n",
    "        # RFE feature selection\n",
    "        estimator = LogisticRegression(max_iter=15000, class_weight='balanced', random_state=randomState, solver='saga', penalty='elasticnet', l1_ratio=0.5)\n",
    "        selector = RFE(estimator, n_features_to_select=n_features_to_select, step=1)\n",
    "        selector.fit(X_train, Y_train.values.ravel())\n",
    "\n",
    "        selected_features = X.columns[selector.support_]\n",
    "        print(f\"Fold {fold + 1} RFE selected features: {list(selected_features)}\")\n",
    "\n",
    "        X_train = X_train[selected_features]\n",
    "        X_test = X_test[selected_features]\n",
    "        \n",
    "        # Oversampling (if specified)\n",
    "        if oversample == 'smote':\n",
    "            smote = SMOTE(random_state=randomState)\n",
    "            X_train, Y_train = smote.fit_resample(X_train, Y_train)\n",
    "        elif oversample == 'adasyn':\n",
    "            ada = ADASYN(random_state=randomState)\n",
    "            X_train, Y_train = ada.fit_resample(X_train, Y_train)\n",
    "\n",
    "        # Separate Gender before scaling\n",
    "        gender_train = X_train['Gender']\n",
    "        gender_test = X_test['Gender']\n",
    "\n",
    "        X_train_no_gender = X_train.drop(columns=['Gender'])\n",
    "        X_test_no_gender = X_test.drop(columns=['Gender'])\n",
    "\n",
    "        # Fit the scaler on the training data and transform both train and test sets\n",
    "        X_train_scaled = normalisation_method.fit_transform(X_train_no_gender)\n",
    "        X_test_scaled = normalisation_method.transform(X_test_no_gender)\n",
    "\n",
    "        # Convert back to DataFrame to maintain column names (without gender)\n",
    "        X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train_no_gender.columns, index=X_train.index)\n",
    "        X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test_no_gender, index=X_test.index)\n",
    "        \n",
    "        # Add back gender column (not scaled)\n",
    "        X_train_scaled['Gender'] = gender_train.values\n",
    "        X_test_scaled['Gender'] = gender_test.values\n",
    "\n",
    "        # Ensure 'gender' is still binary (0 or 1)\n",
    "        if 'Gender' in X_train_scaled.columns:\n",
    "            if X_train_scaled['Gender'].isin([0, 1]).all():\n",
    "                kFolds_list.append((X_train_scaled, X_test_scaled, Y_train, Y_test))\n",
    "            else:\n",
    "                print(\"Warning: 'gender' column contains unexpected values after scaling.\") \n",
    "                kFolds_list.append((X_train_scaled, X_test_scaled, Y_train, Y_test))\n",
    "        else:\n",
    "            print(f\"Note: Fold {fold+1} — 'Gender' column was not selected by RFE.\")\n",
    "            kFolds_list.append((X_train_scaled, X_test_scaled, Y_train, Y_test))\n",
    "               \n",
    "        print(f\"Fold: {fold+1}, Train: {kFolds_list[fold][0].shape}, Test: {kFolds_list[fold][1].shape}\")   \n",
    "    return kFolds_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'iso_forest' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# kFolds = FOLDS_GENERATOR(X_FOR_FOLDS, Y_FOR_FOLDS, normalisation_method=MinMaxScaler(), n_splits=5, randomState=randomState)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# SMOTE\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m kFolds_smote \u001b[38;5;241m=\u001b[39m \u001b[43mFOLDS_GENERATOR\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_FOR_FOLDS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_FOR_FOLDS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mnormalisation_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPowerTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m                              \u001b[49m\u001b[43moversample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msmote\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mrandomState\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandomState\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# ADASYN\u001b[39;00m\n\u001b[0;32m     11\u001b[0m kFolds_adasyn \u001b[38;5;241m=\u001b[39m FOLDS_GENERATOR(X_FOR_FOLDS, Y_FOR_FOLDS, \n\u001b[0;32m     12\u001b[0m                                normalisation_method\u001b[38;5;241m=\u001b[39mPowerTransformer(),\n\u001b[0;32m     13\u001b[0m                                oversample\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madasyn\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     14\u001b[0m                                n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, \n\u001b[0;32m     15\u001b[0m                                randomState\u001b[38;5;241m=\u001b[39mrandomState)\n",
      "Cell \u001b[1;32mIn[38], line 30\u001b[0m, in \u001b[0;36mFOLDS_GENERATOR\u001b[1;34m(X, Y, normalisation_method, n_splits, randomState, oversample, n_features_to_select)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mcatch_warnings():\n\u001b[0;32m     29\u001b[0m     warnings\u001b[38;5;241m.\u001b[39msimplefilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mUserWarning\u001b[39;00m)\n\u001b[1;32m---> 30\u001b[0m     outliers \u001b[38;5;241m=\u001b[39m \u001b[43miso_forest\u001b[49m\u001b[38;5;241m.\u001b[39mfit_predict(X_train)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Outlier Removal\u001b[39;00m\n\u001b[0;32m     33\u001b[0m iso_forest \u001b[38;5;241m=\u001b[39m IsolationForest(contamination\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m, random_state\u001b[38;5;241m=\u001b[39mrandomState)\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'iso_forest' referenced before assignment"
     ]
    }
   ],
   "source": [
    "# kFolds = FOLDS_GENERATOR(X_FOR_FOLDS, Y_FOR_FOLDS, normalisation_method=MinMaxScaler(), n_splits=5, randomState=randomState)\n",
    "\n",
    "# SMOTE\n",
    "kFolds_smote = FOLDS_GENERATOR(X_FOR_FOLDS, Y_FOR_FOLDS, \n",
    "                              normalisation_method=PowerTransformer(),\n",
    "                              oversample='smote',\n",
    "                              n_splits=5, \n",
    "                              randomState=randomState)\n",
    "\n",
    "# ADASYN\n",
    "kFolds_adasyn = FOLDS_GENERATOR(X_FOR_FOLDS, Y_FOR_FOLDS, \n",
    "                               normalisation_method=PowerTransformer(),\n",
    "                               oversample='adasyn',\n",
    "                               n_splits=5, \n",
    "                               randomState=randomState)\n",
    "\n",
    "# Baseline (no oversampling)\n",
    "kFolds_baseline = FOLDS_GENERATOR(X_FOR_FOLDS, Y_FOR_FOLDS, \n",
    "                                 normalisation_method=PowerTransformer(),\n",
    "                                 oversample=None,\n",
    "                                 n_splits=5, \n",
    "                                 randomState=randomState)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from modularModels1 import BlockMaker, modularNN, BasicModel\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using\", device)\n",
    "\n",
    "def init_weights(model): #tested already\n",
    "    if isinstance(model, nn.Linear):  # Apply only to linear layers\n",
    "        nn.init.xavier_uniform_(model.weight)\n",
    "        if model.bias is not None:\n",
    "            nn.init.zeros_(model.bias)\n",
    "            \n",
    "def fold_to_dataloader_tensor(train_x, test_x, train_y, test_y, batch_size=64, device=device):\n",
    "    train_dataset = TensorDataset(\n",
    "        torch.tensor(train_x.values,dtype=torch.float32).to(device), \n",
    "        torch.tensor(train_y.values,dtype=torch.float32).to(device))\n",
    "    val_dataset = TensorDataset(\n",
    "        torch.tensor(test_x.values,dtype=torch.float32).to(device), \n",
    "        torch.tensor(test_y.values,dtype=torch.float32).to(device))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=False)\n",
    "    return train_loader, val_loader "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "#! If you use BCEWithLogitsLoss, you need to ensure that the model outputs logits (raw scores) rather than probabilities.\n",
    "#! If you make a loss function that m.\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    The alpha parameter adjusts the weight for the minority class.\n",
    "    The gamma parameter adjusts how much to focus on hard examples (higher values will focus more on adifficult-to-classify samples).\n",
    "    \n",
    "    criterion = FocalLoss(alpha=0.25, gamma=2.0).to(device) \n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=0.25, gamma=2.0):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = nn.BCEWithLogitsLoss(reduction='none')(inputs, targets)\n",
    "        pt = torch.exp(-BCE_loss)  # pt is the probability for each class\n",
    "        F_loss = self.alpha * (1 - pt) ** self.gamma * BCE_loss\n",
    "        return F_loss.mean()\n",
    "    \n",
    "class DiceLoss(nn.Module): #! untested\n",
    "    \"\"\"\n",
    "    Dice loss is a metric commonly used for imbalanced datasets, especially in segmentation tasks. It measures the overlap between the predicted and true classes. While it’s more often used in segmentation, it can be adapted for binary classification tasks.\n",
    "    \n",
    "    criterion = DiceLoss().to(device) \n",
    "    \"\"\"\n",
    "    def __init__(self, smooth=1e-6):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        intersection = torch.sum(inputs * targets)\n",
    "        union = torch.sum(inputs) + torch.sum(targets)\n",
    "        dice = (2. * intersection + self.smooth) / (union + self.smooth)\n",
    "        return 1 - dice\n",
    "\n",
    "#! default_criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([2.0]).to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Model + loss + optimiser__ definition & initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BasicModel(\n",
       "  (block): Sequential(\n",
       "    (0): Linear(in_features=28, out_features=512, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (3): LeakyReLU(negative_slope=0.01)\n",
       "    (4): Linear(in_features=256, out_features=32, bias=True)\n",
       "    (5): LeakyReLU(negative_slope=0.01)\n",
       "    (6): Linear(in_features=32, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1 = BasicModel(28,[512,256,32],1,\n",
    "                   [nn.Tanh(),nn.LeakyReLU()])\n",
    "# print(model_1)\n",
    "model = model_1.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([2.0]).to(device)) #! IMPORTANT .to(device) the tensor for GPU\n",
    "optimiser = optim.Adagrad(model.parameters(),lr=0.005)\n",
    "\n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1      AccuracyScore: 0.8773  RecallScore: 0.35    f1Score: 0.25        f1Score: 0.29       \n",
      "Fold: 2      AccuracyScore: 0.8869  RecallScore: 0.40    f1Score: 0.23        f1Score: 0.29       \n",
      "Fold: 3      AccuracyScore: 0.8955  RecallScore: 0.45    f1Score: 0.15        f1Score: 0.22       \n",
      "Fold: 4      AccuracyScore: 0.8929  RecallScore: 0.44    f1Score: 0.24        f1Score: 0.31       \n",
      "Fold: 5      AccuracyScore: 0.8981  RecallScore: 0.49    f1Score: 0.22        f1Score: 0.31       \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import recall_score, accuracy_score,f1_score, precision_score, confusion_matrix, roc_auc_score, classification_report, roc_curve\n",
    "\n",
    "epochs = 10\n",
    "batchsize = 64\n",
    "\n",
    "#* Fold Training Loop, repeat for each fold\n",
    "for fold, (train_x, test_x, train_y, test_y) in enumerate(kFolds, start=1):\n",
    "    # print(f\"Fold: {fold}\")\n",
    "    #* Convert the fold to PyTorch tensors and create DataLoader objects\n",
    "    train_loader, val_loader = fold_to_dataloader_tensor(train_x, test_x, train_y, test_y, batchsize, device)\n",
    "\n",
    "    #* Set model to training mode: essential for dropout and batch norm layers\n",
    "    model.train()\n",
    "    \n",
    "    #* Epoch Training loop for this fold\n",
    "    for epoch in range(1,epochs+1):\n",
    "        running_loss = 0.0 #! for future loss tracking\n",
    "        #* Mini-batch training loop\n",
    "        for batch, (inputs, labels) in enumerate(train_loader,start=1):\n",
    "            optimiser.zero_grad() #? Zero the gradients\n",
    "            \n",
    "            outputs = model(inputs) #? Forward pass through the model\n",
    "            loss = criterion(outputs, labels) #? Calculate loss\n",
    "            loss.backward() #? Backpropagation\n",
    "            running_loss += loss.item()\n",
    "            optimiser.step() #? Update weights\n",
    "    \n",
    "    #* Now we evaluate the model on the validation set        \n",
    "    model.eval() #? Set model to evaluation mode\n",
    "    with torch.no_grad(): #? No need to track gradients during evaluation       \n",
    "        for batch, (inputs, labels) in enumerate(val_loader,start=1):#! one pass because val_loader batch size is all, if you want to do it in mini-batches, you MUST change the metric calculations to accept mini-batches\n",
    "            outputs = model(inputs)  \n",
    "            predictions = (torch.sigmoid(outputs) > 0.5).float().cpu()#? Convert logits to binary predictions\n",
    "            labels = labels.cpu() #? Move labels to CPU for compatibility with sklearn metrics\n",
    "            \n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision = precision_score(labels, predictions, pos_label=1)\n",
    "    recall = recall_score(labels, predictions, pos_label=1)\n",
    "    f1 = f1_score(labels, predictions, pos_label=1)\n",
    "    auc = roc_auc_score(labels, predictions)\n",
    "\n",
    "    print(f\"Fold: {fold}\".ljust(12),\n",
    "            f\"AccuracyScore: {(accuracy):.4f}\".ljust(22),\n",
    "            f\"RecallScore: {(precision):.2f}\".ljust(20),\n",
    "            f\"f1Score: {(recall):.2f}\".ljust(20),   \n",
    "            f\"f1Score: {(f1):.2f}\".ljust(20),\n",
    "                )\n",
    "        \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
