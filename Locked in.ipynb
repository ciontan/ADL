{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "556439f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from pandas) (2.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: optuna in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (4.2.1)\n",
      "Requirement already satisfied: alembic>=1.5.0 in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from optuna) (1.15.2)\n",
      "Requirement already satisfied: colorlog in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from optuna) (6.9.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from optuna) (2.2.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from optuna) (24.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from optuna) (2.0.40)\n",
      "Requirement already satisfied: tqdm in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from optuna) (6.0.2)\n",
      "Requirement already satisfied: Mako in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from alembic>=1.5.0->optuna) (1.3.9)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from colorlog->optuna) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
      "Requirement already satisfied: optuna-dashboard in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (0.18.0)\n",
      "Requirement already satisfied: bottle>=0.13.0 in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from optuna-dashboard) (0.13.2)\n",
      "Requirement already satisfied: optuna>=3.1.0 in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from optuna-dashboard) (4.2.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from optuna-dashboard) (24.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from optuna-dashboard) (1.6.1)\n",
      "Requirement already satisfied: alembic>=1.5.0 in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from optuna>=3.1.0->optuna-dashboard) (1.15.2)\n",
      "Requirement already satisfied: colorlog in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from optuna>=3.1.0->optuna-dashboard) (6.9.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from optuna>=3.1.0->optuna-dashboard) (2.2.4)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from optuna>=3.1.0->optuna-dashboard) (2.0.40)\n",
      "Requirement already satisfied: tqdm in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from optuna>=3.1.0->optuna-dashboard) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from optuna>=3.1.0->optuna-dashboard) (6.0.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from scikit-learn->optuna-dashboard) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from scikit-learn->optuna-dashboard) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from scikit-learn->optuna-dashboard) (3.6.0)\n",
      "Requirement already satisfied: Mako in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from alembic>=1.5.0->optuna>=3.1.0->optuna-dashboard) (1.3.9)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from alembic>=1.5.0->optuna>=3.1.0->optuna-dashboard) (4.12.2)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from sqlalchemy>=1.4.2->optuna>=3.1.0->optuna-dashboard) (3.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from colorlog->optuna>=3.1.0->optuna-dashboard) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna>=3.1.0->optuna-dashboard) (3.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement scikit (from versions: none)\n",
      "ERROR: No matching distribution found for scikit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imbalanced-learn in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (0.13.0)\n",
      "Requirement already satisfied: numpy<3,>=1.24.3 in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from imbalanced-learn) (2.2.4)\n",
      "Requirement already satisfied: scipy<2,>=1.10.1 in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from imbalanced-learn) (1.15.2)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.3.2 in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from imbalanced-learn) (1.6.1)\n",
      "Requirement already satisfied: sklearn-compat<1,>=0.1 in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from imbalanced-learn) (0.1.3)\n",
      "Requirement already satisfied: joblib<2,>=1.1.1 in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from imbalanced-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in c:\\users\\tan le zhan\\documents\\github\\adl\\.venv\\lib\\site-packages (from imbalanced-learn) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install optuna\n",
    "!pip install optuna-dashboard\n",
    "!pip install scikit\n",
    "!pip install imbalanced-learn\n",
    "from sklearn.preprocessing import (\n",
    "    MaxAbsScaler,\n",
    "    MinMaxScaler,\n",
    "    Normalizer,\n",
    "    PowerTransformer,\n",
    "    QuantileTransformer,\n",
    "    RobustScaler,\n",
    "    StandardScaler,\n",
    "    minmax_scale,\n",
    ")\n",
    "from sklearn.metrics import recall_score, accuracy_score,f1_score, precision_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import warnings\n",
    "import optuna\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "625cfb3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using\", device)\n",
    "def init_weights(model): #tested already\n",
    "    if isinstance(model, nn.Linear):  # Apply only to linear layers\n",
    "        nn.init.xavier_uniform_(model.weight)\n",
    "        if model.bias is not None:\n",
    "            nn.init.zeros_(model.bias)\n",
    "            \n",
    "def fold_to_dataloader_tensor(train_x, test_x, train_y, test_y, batch_size=64, device=device):\n",
    "    train_dataset = TensorDataset(\n",
    "        torch.tensor(train_x.values,dtype=torch.float32).to(device), \n",
    "        torch.tensor(train_y.values,dtype=torch.float32).to(device))\n",
    "    val_dataset = TensorDataset(\n",
    "        torch.tensor(test_x.values,dtype=torch.float32).to(device), \n",
    "        torch.tensor(test_y.values,dtype=torch.float32).to(device))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=False, drop_last=True)\n",
    "    return train_loader, val_loader \n",
    "\n",
    "def get_feature_count(loader):\n",
    "    \"\"\"returns the number of features in the dataset\"\"\"\n",
    "    return next(iter(loader))[0].shape[1]\n",
    "\n",
    "from Criterion_Models import *\n",
    "def criterion_mapping(criterion_choice:str, pos_weight:float=None, alpha:float=None, gamma:float=None):\n",
    "    \"\"\"\n",
    "    Feel free to add any custom loss functions here.\n",
    "    returns function for criterion\n",
    "    \"\"\"\n",
    "    if criterion_choice == \"FocalLoss\":\n",
    "        return FocalLoss(alpha =alpha, gamma=gamma)\n",
    "    elif criterion_choice == \"DiceLoss\":\n",
    "        return DiceLoss()\n",
    "    elif criterion_choice == \"BCEWithLogitsLoss\":\n",
    "        return nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight])) if pos_weight else nn.BCEWithLogitsLoss()\n",
    "    return nn.BCEWithLogitsLoss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "646878a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "raw_dataset = pd.read_csv(\"./data/processed_data.csv\") #data has X and Y\n",
    "X = raw_dataset.drop(columns=[\"DR\"])\n",
    "Y = pd.DataFrame(raw_dataset[\"DR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40c058ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop cols here\n",
    "# numeric_columns = ['Age', 'UAlb', 'Ucr', 'UACR', 'TC', 'TG', 'TCTG', 'LDLC', 'HDLC', 'Scr', 'BUN', 'FPG', 'HbA1c', 'Height', 'Weight', 'BMI', 'Duration']\n",
    "# binary_columns = ['Gender', 'DR', 'Community_baihe', 'Community_chonggu', 'Community_huaxin', 'Community_jinze', 'Community_liantang', 'Community_xianghuaqiao', 'Community_xujin', 'Community_yingpu', 'Community_zhaoxian', 'Community_zhujiajiao']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4c9b002",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_FOR_FOLDS, X_FINAL_TEST, Y_FOR_FOLDS, Y_FINAL_TEST = train_test_split(X, Y, test_size=0.1, random_state=random_state, stratify=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7f4dd31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data_in_place(X, X_test, Y=None, normalisation_method=MinMaxScaler(), noise=None):\n",
    "    all_numerical_columns = ['Age', 'UAlb', 'Ucr', 'UACR', 'TC', 'TG', 'TCTG', 'LDLC', 'HDLC', 'Scr', 'BUN', 'FPG', 'HbA1c', 'Height', 'Weight', 'BMI', 'Duration']\n",
    "    binary_columns = ['Gender', 'DR', 'Community_baihe', 'Community_chonggu', 'Community_huaxin', 'Community_jinze', 'Community_liantang', 'Community_xianghuaqiao', 'Community_xujin', 'Community_yingpu', 'Community_zhaoxian', 'Community_zhujiajiao']\n",
    "    \n",
    "    existing_columns = [col for col in all_numerical_columns if col in X.columns and col in X_test.columns]\n",
    "\n",
    "    if not existing_columns:\n",
    "        print(\"No matching columns found for augmentation. Normalised data only.\")\n",
    "        X = normalisation_method.fit_transform(X)\n",
    "        X_test = normalisation_method.transform(X_test)\n",
    "        return X, X_test\n",
    "\n",
    "    X_copy = X.copy()\n",
    "    X_test_copy = X_test.copy()\n",
    "    \n",
    "    # Log-transform\n",
    "    X_copy.loc[:, existing_columns] = X_copy.loc[:, existing_columns].apply(np.log1p)\n",
    "    X_test_copy.loc[:, existing_columns] = X_test_copy.loc[:, existing_columns].apply(np.log1p)\n",
    "\n",
    "    # Add noise ONLY to negatives (class 0) if Y is provided and noise is set\n",
    "    if noise and noise > 0:\n",
    "        if Y is None:\n",
    "            raise ValueError(\"Y must be provided if noise is being added selectively.\")\n",
    "        # Identify negative class indices (class 0)\n",
    "        negative_indices = Y[Y.iloc[:, 0] == 0].index\n",
    "        noise_matrix = np.random.normal(0, noise, X_copy.loc[negative_indices, existing_columns].shape)\n",
    "        X_copy.loc[negative_indices, existing_columns] += noise_matrix\n",
    "\n",
    "    # Scale\n",
    "    scaler = normalisation_method\n",
    "    X_copy.loc[:, existing_columns] = scaler.fit_transform(X_copy.loc[:, existing_columns])\n",
    "    X_test_copy.loc[:, existing_columns] = scaler.transform(X_test_copy.loc[:, existing_columns])\n",
    "\n",
    "    return X_copy, X_test_copy\n",
    "\n",
    "\n",
    "def iso_forest(X_train, Y_train, contamination=None, random_state=42):\n",
    "    # print(\"Original\\n\", X_train.shape, Y_train.shape, X_test.shape, Y_test.shape)\n",
    "    X_train_cleaned, Y_train_cleaned = X_train.copy(), Y_train.copy()\n",
    "    \n",
    "    X_train_zeros = X_train[Y_train.iloc[:, 0] == 0]\n",
    "    X_train_ones = X_train[Y_train.iloc[:, 0] == 1]\n",
    "    Y_train_zeros = Y_train[Y_train.iloc[:, 0] == 0]\n",
    "    Y_train_ones = Y_train[Y_train.iloc[:, 0] == 1] \n",
    "    # print(\"Ones and zeros\\n\", X_train_zeros.shape, Y_train_zeros.shape, X_train_ones.shape, Y_train_ones.shape)\n",
    "    #only class 0s\n",
    "    if X_train_zeros.isna().any().any():\n",
    "        print(\"got NaN values in the training set\")\n",
    "    \n",
    "    # Apply Isolation Forest to majority class only\n",
    "    iso_forest = IsolationForest(contamination=contamination, random_state=random_state)\n",
    "    try:\n",
    "        outliers = iso_forest.fit_predict(X_train_zeros)\n",
    "    except UserWarning as e:\n",
    "        print(\"Caught warning during IsolationForest fitting:\", e)\n",
    "        outliers = np.ones(len(X_train_zeros))  # If warning occurs, keep all data\n",
    "    # Keep only non-outlier majority samples\n",
    "    X_train_zeros = X_train_zeros[outliers == 1]\n",
    "    Y_train_zeros = Y_train_zeros[outliers == 1]\n",
    "    # print(\"After iso:\\n\", X_train_zeros.shape, Y_train_zeros.shape, X_train_ones.shape, Y_train_ones.shape)\n",
    "    \n",
    "    # Combine the cleaned majority class with the untouched minority class\n",
    "    X_train_cleaned = pd.concat([X_train_zeros, X_train_ones])\n",
    "    Y_train_cleaned = pd.concat([Y_train_zeros, Y_train_ones])\n",
    "    return X_train_cleaned, Y_train_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8f98c91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FOLDS_GENERATOR(X, Y, normalisation_method=MinMaxScaler(), n_splits=5, random_state=None, oversampler=None, contamination=0.05, noise = None):\n",
    "    \"\"\"\n",
    "    Generates stratified folds with specified normalization.\n",
    "    normalisation_method should be an instance of a scaler, e.g.,\n",
    "    - MinMaxScaler()\n",
    "    Returns a list of tuples, each containing:\n",
    "    (X_train_scaled, X_test_scaled, Y_train, Y_test), representing data for each fold\n",
    "    \"\"\"\n",
    "    kF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    kFolds_list = []\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(kF.split(X, Y)):\n",
    "        # Split the data into training and testing sets for this fold\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        Y_train, Y_test = Y.iloc[train_idx], Y.iloc[test_idx]\n",
    "        # print(\"Original\\n\", X_train.shape, Y_train.shape, X_test.shape, Y_test.shape)\n",
    "        \n",
    "        if contamination is not None and contamination > 0: #? using contamination = 0.0 works\n",
    "            X_train_cleaned, Y_train_cleaned = iso_forest(X_train, Y_train, contamination=contamination, random_state=random_state)\n",
    "        \n",
    "        #? data augmentation on leftover data\n",
    "        X_train_scaled, X_test_scaled = augment_data_in_place(X_train_cleaned, X_test, Y_train_cleaned,normalisation_method=normalisation_method, noise = noise)\n",
    "        \n",
    "        # Handle oversampling if needed\n",
    "        #! use X_train_scaled and Y_train_cleaned for oversampling becasue y_train_cleaned no changes after augmentation\n",
    "        print(\"Before oversampling class distribution:\")\n",
    "        print(Y_train_cleaned.value_counts())\n",
    "        if oversampler:\n",
    "            X_train_scaled, Y_train_cleaned = oversampler.fit_resample(X_train_scaled, Y_train_cleaned)\n",
    "        print(\"\\nAfter oversampling class distribution:\")\n",
    "        print(Y_train_cleaned.value_counts())\n",
    "        # Convert scaled data back to DataFrame with the correct column names\n",
    "        X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train_cleaned.columns)\n",
    "        X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "\n",
    "        # Handle community columns\n",
    "        community_cols = [col for col in X_train_scaled.columns if col.startswith('Community')]\n",
    "        if community_cols:\n",
    "            X_train_scaled[community_cols] = X_train_scaled[community_cols].apply(\n",
    "                lambda row: pd.Series(np.eye(len(row))[row.argmax()]), axis=1\n",
    "            ).set_axis(community_cols, axis=1)\n",
    "        # print(X_train_scaled[community_cols].describe())\n",
    "\n",
    "        # Ensure 'Gender' is still binary (0 or 1)\n",
    "        if 'Gender' in X_train_scaled.columns:\n",
    "            X_train_scaled['Gender'] = (X_train_scaled['Gender'] > 0.5).astype(int)\n",
    "            X_test_scaled['Gender'] = (X_test_scaled['Gender'] > 0.5).astype(int)\n",
    "\n",
    "        # Append the processed fold to the list\n",
    "        kFolds_list.append((X_train_scaled, X_test_scaled, Y_train_cleaned, Y_test))\n",
    "\n",
    "        print(f\"Fold: {fold+1}, Train: {X_train_scaled.shape}, Test: {X_test_scaled.shape}\")\n",
    "\n",
    "    return kFolds_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "67a2c06b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before oversampling class distribution:\n",
      "DR \n",
      "0.0    3922\n",
      "1.0     464\n",
      "Name: count, dtype: int64\n",
      "\n",
      "After oversampling class distribution:\n",
      "DR \n",
      "1.0    3979\n",
      "0.0    3922\n",
      "Name: count, dtype: int64\n",
      "Fold: 1, Train: (7901, 28), Test: (1149, 28)\n",
      "Before oversampling class distribution:\n",
      "DR \n",
      "0.0    3922\n",
      "1.0     464\n",
      "Name: count, dtype: int64\n",
      "\n",
      "After oversampling class distribution:\n",
      "DR \n",
      "1.0    3960\n",
      "0.0    3922\n",
      "Name: count, dtype: int64\n",
      "Fold: 2, Train: (7882, 28), Test: (1149, 28)\n",
      "Before oversampling class distribution:\n",
      "DR \n",
      "0.0    3923\n",
      "1.0     464\n",
      "Name: count, dtype: int64\n",
      "\n",
      "After oversampling class distribution:\n",
      "DR \n",
      "1.0    3963\n",
      "0.0    3923\n",
      "Name: count, dtype: int64\n",
      "Fold: 3, Train: (7886, 28), Test: (1148, 28)\n",
      "Before oversampling class distribution:\n",
      "DR \n",
      "0.0    3923\n",
      "1.0     464\n",
      "Name: count, dtype: int64\n",
      "\n",
      "After oversampling class distribution:\n",
      "DR \n",
      "1.0    3958\n",
      "0.0    3923\n",
      "Name: count, dtype: int64\n",
      "Fold: 4, Train: (7881, 28), Test: (1148, 28)\n",
      "Before oversampling class distribution:\n",
      "DR \n",
      "0.0    3923\n",
      "1.0     464\n",
      "Name: count, dtype: int64\n",
      "\n",
      "After oversampling class distribution:\n",
      "DR \n",
      "1.0    3956\n",
      "0.0    3923\n",
      "Name: count, dtype: int64\n",
      "Fold: 5, Train: (7879, 28), Test: (1148, 28)\n"
     ]
    }
   ],
   "source": [
    "oversampler = ADASYN(sampling_strategy='minority', n_neighbors=5, random_state=42)\n",
    "contamination = 0.05\n",
    "normalisation_method = MinMaxScaler()\n",
    "kFolds = FOLDS_GENERATOR(X_FOR_FOLDS, Y_FOR_FOLDS, \n",
    "                         normalisation_method = normalisation_method, \n",
    "                         n_splits=5, \n",
    "                         oversampler = oversampler, random_state=42, contamination=contamination, noise = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "14cf28b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7901, 28) (1149, 28) (7901, 1) (1149, 1)\n",
      "               Age       Gender         UAlb          Ucr         UACR  \\\n",
      "count  7901.000000  7901.000000  7901.000000  7901.000000  7901.000000   \n",
      "mean      0.594660     0.537780     0.389796     0.465419     0.356018   \n",
      "std       0.122816     0.498602     0.191754     0.373218     0.165705   \n",
      "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "25%       0.524505     0.000000     0.260280     0.134245     0.245180   \n",
      "50%       0.609557     1.000000     0.361727     0.200581     0.330989   \n",
      "75%       0.676143     1.000000     0.501705     0.882666     0.445693   \n",
      "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
      "\n",
      "                TC           TG         TCTG         LDLC         HDLC  ...  \\\n",
      "count  7901.000000  7901.000000  7901.000000  7901.000000  7901.000000  ...   \n",
      "mean      0.435022     0.185039     0.434814     0.562047     0.424684  ...   \n",
      "std       0.097940     0.082591     0.155653     0.119372     0.135046  ...   \n",
      "min       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
      "25%       0.372252     0.128549     0.330813     0.488008     0.330208  ...   \n",
      "50%       0.436436     0.170170     0.433936     0.574145     0.417008  ...   \n",
      "75%       0.500955     0.225500     0.543564     0.645381     0.511798  ...   \n",
      "max       1.000000     1.000000     1.000000     1.000000     1.000000  ...   \n",
      "\n",
      "       Community_baihe  Community_chonggu  Community_huaxin  Community_jinze  \\\n",
      "count      7901.000000        7901.000000       7901.000000      7901.000000   \n",
      "mean          0.146817           0.067713          0.124035         0.084546   \n",
      "std           0.353946           0.251269          0.329642         0.278223   \n",
      "min           0.000000           0.000000          0.000000         0.000000   \n",
      "25%           0.000000           0.000000          0.000000         0.000000   \n",
      "50%           0.000000           0.000000          0.000000         0.000000   \n",
      "75%           0.000000           0.000000          0.000000         0.000000   \n",
      "max           1.000000           1.000000          1.000000         1.000000   \n",
      "\n",
      "       Community_liantang  Community_xianghuaqiao  Community_xujin  \\\n",
      "count         7901.000000             7901.000000      7901.000000   \n",
      "mean             0.112771                0.087837         0.088976   \n",
      "std              0.316332                0.283076         0.284727   \n",
      "min              0.000000                0.000000         0.000000   \n",
      "25%              0.000000                0.000000         0.000000   \n",
      "50%              0.000000                0.000000         0.000000   \n",
      "75%              0.000000                0.000000         0.000000   \n",
      "max              1.000000                1.000000         1.000000   \n",
      "\n",
      "       Community_yingpu  Community_zhaoxian  Community_zhujiajiao  \n",
      "count       7901.000000         7901.000000           7901.000000  \n",
      "mean           0.079484            0.112264              0.095558  \n",
      "std            0.270509            0.315711              0.294002  \n",
      "min            0.000000            0.000000              0.000000  \n",
      "25%            0.000000            0.000000              0.000000  \n",
      "50%            0.000000            0.000000              0.000000  \n",
      "75%            0.000000            0.000000              0.000000  \n",
      "max            1.000000            1.000000              1.000000  \n",
      "\n",
      "[8 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "for list in kFolds:\n",
    "    print(list[0].shape, list[1].shape, list[2].shape, list[3].shape)\n",
    "    print(list[0].describe())\n",
    "    a = list[0]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5c2bca64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1, Train: (7825, 28), Test: (1149, 28)\n",
      "Fold: 2, Train: (7886, 28), Test: (1149, 28)\n",
      "Fold: 3, Train: (7822, 28), Test: (1148, 28)\n",
      "Fold: 4, Train: (7836, 28), Test: (1148, 28)\n",
      "Fold: 5, Train: (7876, 28), Test: (1148, 28)\n"
     ]
    }
   ],
   "source": [
    "oversampler = ADASYN(sampling_strategy='minority', n_neighbors=5, random_state=42)\n",
    "contamination = 0.05\n",
    "normalisation_method = MinMaxScaler()\n",
    "kFolds2 = FOLDS_GENERATOR(X_FOR_FOLDS, Y_FOR_FOLDS, \n",
    "                         normalisation_method = normalisation_method, \n",
    "                         n_splits=5, \n",
    "                         oversampler = oversampler, random_state=42, contamination=contamination, noise = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933d76a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7941, 28) (1149, 28) (7941, 1) (1149, 1)\n",
      "               Age       Gender         UAlb          Ucr         UACR  \\\n",
      "count  7941.000000  7941.000000  7941.000000  7941.000000  7941.000000   \n",
      "mean      0.550838     0.539227     0.418078     0.458959     0.368753   \n",
      "std       0.119223     0.498490     0.175028     0.347252     0.157979   \n",
      "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "25%       0.472435     0.000000     0.300738     0.153257     0.261809   \n",
      "50%       0.553392     1.000000     0.396253     0.216133     0.347300   \n",
      "75%       0.629549     1.000000     0.521173     0.845490     0.457313   \n",
      "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
      "\n",
      "                TC           TG         TCTG         LDLC         HDLC  ...  \\\n",
      "count  7941.000000  7941.000000  7941.000000  7941.000000  7941.000000  ...   \n",
      "mean      0.446019     0.265299     0.473273     0.541935     0.476491  ...   \n",
      "std       0.104849     0.095436     0.148022     0.122427     0.136539  ...   \n",
      "min       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
      "25%       0.376099     0.200881     0.372701     0.464717     0.384459  ...   \n",
      "50%       0.445182     0.251808     0.478591     0.542573     0.476166  ...   \n",
      "75%       0.514317     0.316140     0.574692     0.623046     0.566126  ...   \n",
      "max       1.000000     1.000000     1.000000     1.000000     1.000000  ...   \n",
      "\n",
      "       Community_baihe  Community_chonggu  Community_huaxin  Community_jinze  \\\n",
      "count      7941.000000        7941.000000       7941.000000      7941.000000   \n",
      "mean          0.148470           0.066616          0.122655         0.083491   \n",
      "std           0.355588           0.249372          0.328061         0.276640   \n",
      "min           0.000000           0.000000          0.000000         0.000000   \n",
      "25%           0.000000           0.000000          0.000000         0.000000   \n",
      "50%           0.000000           0.000000          0.000000         0.000000   \n",
      "75%           0.000000           0.000000          0.000000         0.000000   \n",
      "max           1.000000           1.000000          1.000000         1.000000   \n",
      "\n",
      "       Community_liantang  Community_xianghuaqiao  Community_xujin  \\\n",
      "count         7941.000000             7941.000000      7941.000000   \n",
      "mean             0.111321                0.088654         0.089535   \n",
      "std              0.314549                0.284261         0.285533   \n",
      "min              0.000000                0.000000         0.000000   \n",
      "25%              0.000000                0.000000         0.000000   \n",
      "50%              0.000000                0.000000         0.000000   \n",
      "75%              0.000000                0.000000         0.000000   \n",
      "max              1.000000                1.000000         1.000000   \n",
      "\n",
      "       Community_yingpu  Community_zhaoxian  Community_zhujiajiao  \n",
      "count       7941.000000         7941.000000           7941.000000  \n",
      "mean           0.079461            0.114469              0.095328  \n",
      "std            0.270474            0.318400              0.293686  \n",
      "min            0.000000            0.000000              0.000000  \n",
      "25%            0.000000            0.000000              0.000000  \n",
      "50%            0.000000            0.000000              0.000000  \n",
      "75%            0.000000            0.000000              0.000000  \n",
      "max            1.000000            1.000000              1.000000  \n",
      "\n",
      "[8 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "for list in kFolds2:\n",
    "    print(list[0].shape, list[1].shape, list[2].shape, list[3].shape)\n",
    "    print(list[0].describe())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4504db28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
