{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daf8f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import (\n",
    "    MaxAbsScaler,\n",
    "    MinMaxScaler,\n",
    "    Normalizer,\n",
    "    PowerTransformer,\n",
    "    QuantileTransformer,\n",
    "    RobustScaler,\n",
    "    StandardScaler,\n",
    "    minmax_scale,\n",
    ")\n",
    "from sklearn.metrics import recall_score, accuracy_score,f1_score, precision_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import warnings\n",
    "import optuna\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb93124",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "raw_dataset = pd.read_csv(\"./data/processed_data.csv\") #data has X and Y\n",
    "X = raw_dataset.drop(columns=[\"DR\"])\n",
    "Y = pd.DataFrame(raw_dataset[\"DR\"])\n",
    "# print(X.describe())\n",
    "# X.drop(columns = ['Age', 'Gender', 'UAlb', 'Ucr', 'UACR', 'LDLC', 'HDLC'], inplace=True)\n",
    "\n",
    "# [Age,Gender,UAlb,Ucr,UACR,TC,TG,TCTG,LDLC,HDLC,Scr,BUN,FPG,HbA1c,Height,Weight,BMI,Duration,DR,Community_baihe,Community_chonggu,Community_huaxin,Community_jinze,Community_liantang,Community_xianghuaqiao,Community_xujin,Community_yingpu,Community_zhaoxian,Community_zhujiajiao]\n",
    "#* 90/10 split for training and final test\n",
    "X_FOR_FOLDS, X_FINAL_TEST, Y_FOR_FOLDS, Y_FINAL_TEST = train_test_split(X, Y, test_size=0.1, random_state=random_state, stratify=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c95bb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! reference code for scaling to implement\n",
    "\n",
    "# from sklearn.preprocessing import RobustScaler\n",
    "# import pandas as pd\n",
    "\n",
    "# # Let's assume your data is in a DataFrame called df\n",
    "\n",
    "# # Step 1: Separate numeric and categorical columns\n",
    "# numeric_columns = ['Age', 'UAlb', 'Ucr', 'UACR', 'TC', 'TG', 'TCTG', 'LDLC', 'HDLC', 'Scr', 'BUN', 'FPG', 'HbA1c', 'Height', 'Weight', 'BMI', 'Duration']\n",
    "# binary_columns = ['Gender', 'DR', 'Community_baihe', 'Community_chonggu', 'Community_huaxin', 'Community_jinze', 'Community_liantang', 'Community_xianghuaqiao', 'Community_xujin', 'Community_yingpu', 'Community_zhaoxian', 'Community_zhujiajiao']\n",
    "\n",
    "# # Separate numeric features and binary/categorical features\n",
    "# X_numeric = df[numeric_columns]\n",
    "# X_binary = df[binary_columns]\n",
    "\n",
    "# # Step 2: Apply RobustScaler to numeric features\n",
    "# scaler = RobustScaler()\n",
    "# X_numeric_scaled = scaler.fit_transform(X_numeric)\n",
    "\n",
    "# # Step 3: Combine scaled numeric data with the original binary/categorical features\n",
    "# X_scaled_df = pd.DataFrame(X_numeric_scaled, columns=numeric_columns)\n",
    "# X_final = pd.concat([X_scaled_df, X_binary.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# # Now X_final has the numeric features scaled and binary/categorical features untouched\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9e1009",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data_in_place(X, X_test, normalisation_method=MinMaxScaler()):\n",
    "    all_numerical_columns = [\n",
    "        'Age', 'Height', 'Weight', 'Duration',\n",
    "        'UAlb', 'Ucr', 'UACR', 'TC', 'TG', \n",
    "        'TCTG', 'LDLC', 'HDLC', 'Scr', 'BUN', 'FPG', 'HbA1c'\n",
    "    ]\n",
    "\n",
    "    # Find which of those columns actually exist in both X and X_test\n",
    "    existing_columns = [col for col in all_numerical_columns if col in X.columns and col in X_test.columns]\n",
    "    \n",
    "    if not existing_columns:\n",
    "        print(\"No matching columns found for augmentation. Normalised data only.\")\n",
    "        X= normalisation_method.fit_transform(X)\n",
    "        X_test = normalisation_method.transform(X_test)\n",
    "        return X, X_test\n",
    "\n",
    "    # 1. Log-transform\n",
    "    X.loc[:, existing_columns] = X.loc[:, existing_columns].apply(np.log1p)\n",
    "    X_test.loc[:, existing_columns] = X_test.loc[:, existing_columns].apply(np.log1p)\n",
    "\n",
    "    # 2. Add Gaussian noise to training data only\n",
    "    noise = np.random.normal(0, 0.1, X[existing_columns].shape)\n",
    "    X.loc[:, existing_columns] = X.loc[:, existing_columns] + noise\n",
    "\n",
    "    # 3. Fit scaler on train, transform both\n",
    "    scaler = normalisation_method\n",
    "    X.loc[:, existing_columns] = scaler.fit_transform(X.loc[:, existing_columns])\n",
    "    X_test.loc[:, existing_columns] = scaler.transform(X_test.loc[:, existing_columns])\n",
    "\n",
    "    return X, X_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85559eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FOLDS_GENERATOR(X, Y, normalisation_method=MinMaxScaler(), n_splits=5, random_state=None, oversampler=None, contamination=0.05):\n",
    "    \"\"\"\n",
    "    Generates stratified folds with specified normalization.\n",
    "\n",
    "    normalisation_method should be an instance of a scaler, e.g.,\n",
    "    - MinMaxScaler()\n",
    "    - MaxAbsScaler()\n",
    "    - QuantileTransformer(output_distribution='uniform')\n",
    "\n",
    "    Returns a list of tuples, each containing:\n",
    "    (X_train_scaled, X_test_scaled, Y_train, Y_test), representing data for each fold\n",
    "    \"\"\"\n",
    "    kF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    kFolds_list = []\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(kF.split(X, Y)):\n",
    "        # Split the data into training and testing sets for this fold\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        Y_train, Y_test = Y.iloc[train_idx], Y.iloc[test_idx]\n",
    "        # print(\"Original\\n\", X_train.shape, Y_train.shape, X_test.shape, Y_test.shape)\n",
    "        X_train_cleaned, Y_train_cleaned = X_train.copy(), Y_train.copy()\n",
    "        if contamination is not None and contamination > 0: #? using contamination = 0.0 works\n",
    "            X_train_zeros = X_train[Y_train.iloc[:, 0] == 0]\n",
    "            X_train_ones = X_train[Y_train.iloc[:, 0] == 1]\n",
    "            Y_train_zeros = Y_train[Y_train.iloc[:, 0] == 0]\n",
    "            Y_train_ones = Y_train[Y_train.iloc[:, 0] == 1] \n",
    "            # print(\"Ones and zeros\\n\", X_train_zeros.shape, Y_train_zeros.shape, X_train_ones.shape, Y_train_ones.shape)\n",
    "            #only class 0s\n",
    "            if X_train_zeros.isna().any().any():\n",
    "                print(\"got NaN values in the training set\")\n",
    "            \n",
    "            # Apply Isolation Forest to majority class only\n",
    "            iso_forest = IsolationForest(contamination=contamination, random_state=random_state)\n",
    "            try:\n",
    "                outliers = iso_forest.fit_predict(X_train_zeros)\n",
    "            except UserWarning as e:\n",
    "                print(\"Caught warning during IsolationForest fitting:\", e)\n",
    "                outliers = np.ones(len(X_train_zeros))  # If warning occurs, keep all data\n",
    "            # Keep only non-outlier majority samples\n",
    "            X_train_zeros = X_train_zeros[outliers == 1]\n",
    "            Y_train_zeros = Y_train_zeros[outliers == 1]\n",
    "            # print(\"After iso:\\n\", X_train_zeros.shape, Y_train_zeros.shape, X_train_ones.shape, Y_train_ones.shape)\n",
    "            \n",
    "            # Combine the cleaned majority class with the untouched minority class\n",
    "            X_train_cleaned = pd.concat([X_train_zeros, X_train_ones])\n",
    "            Y_train_cleaned = pd.concat([Y_train_zeros, Y_train_ones])\n",
    "        #? data augmentation on leftover data\n",
    "        X_train_scaled, X_test_scaled = augment_data_in_place(X_train_cleaned, X_test, normalisation_method=normalisation_method)\n",
    "        \n",
    "        # Handle oversampling if needed\n",
    "        #! use X_train_scaled and Y_train_cleaned for oversampling becasue y_train_cleaned no changes after augmentation\n",
    "        if oversampler:\n",
    "            X_train_scaled, Y_train_cleaned = oversampler.fit_resample(X_train_scaled, Y_train_cleaned)\n",
    "\n",
    "        # Convert scaled data back to DataFrame with the correct column names\n",
    "        X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train_cleaned.columns)\n",
    "        X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "\n",
    "        # Handle community columns\n",
    "        community_cols = [col for col in X_train_scaled.columns if col.startswith('Community')]\n",
    "        if community_cols:\n",
    "            X_train_scaled[community_cols] = X_train_scaled[community_cols].apply(\n",
    "                lambda row: pd.Series(np.eye(len(row))[row.argmax()]), axis=1\n",
    "            ).set_axis(community_cols, axis=1)\n",
    "        # print(X_train_scaled[community_cols].describe())\n",
    "\n",
    "        # Ensure 'Gender' is still binary (0 or 1)\n",
    "        if 'Gender' in X_train_scaled.columns: #! robust scaler will not work for this\n",
    "            X_train_scaled['Gender'] = (X_train_scaled['Gender'] > 0.5).astype(int)\n",
    "            X_test_scaled['Gender'] = (X_test_scaled['Gender'] > 0.5).astype(int)\n",
    "\n",
    "        # Append the processed fold to the list\n",
    "        kFolds_list.append((X_train_scaled, X_test_scaled, Y_train_cleaned, Y_test))\n",
    "\n",
    "        print(f\"Fold: {fold+1}, Train: {X_train_scaled.shape}, Test: {X_test_scaled.shape}\")\n",
    "\n",
    "    return kFolds_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc8c47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# oversampler = None\n",
    "# contamination = 0.05\n",
    "# normalisation_method = QuantileTransformer()\n",
    "# kFolds = FOLDS_GENERATOR(X_FOR_FOLDS, Y_FOR_FOLDS, \n",
    "#                          normalisation_method = normalisation_method, \n",
    "#                          n_splits=5, \n",
    "#                          oversampler = oversampler, random_state=42, contamination=contamination)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
