{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7ea3e5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import (\n",
    "    MaxAbsScaler,\n",
    "    MinMaxScaler,\n",
    "    Normalizer,\n",
    "    PowerTransformer,\n",
    "    QuantileTransformer,\n",
    "    RobustScaler,\n",
    "    StandardScaler,\n",
    "    minmax_scale,\n",
    ")\n",
    "from sklearn.metrics import recall_score, accuracy_score,f1_score, precision_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import optuna\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "befd96c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "randomState = 42\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "raw_dataset = pd.read_csv(\"./data/processed_data.csv\") #data has X and Y\n",
    "X = raw_dataset.drop(columns=[\"DR\"])\n",
    "Y = pd.DataFrame(raw_dataset[\"DR\"])\n",
    "\n",
    "#* 90/10 split for training and final test\n",
    "X_FOR_FOLDS, X_FINAL_TEST, Y_FOR_FOLDS, Y_FINAL_TEST = train_test_split(X, Y, test_size=0.1, random_state=randomState, stratify=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9c9c23e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FOLDS_GENERATOR(X, Y, normalisation_method=MinMaxScaler(), n_splits=5, randomState=None, oversample=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    Generates stratified folds with specified normalization.\n",
    "    \n",
    "    For list of scalers, see:\n",
    "    https://scikit-learn.org/stable/api/sklearn.preprocessing.html\n",
    "    \n",
    "    For more details on scaling and normalization effects, see:\n",
    "    https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#\n",
    "    \n",
    "    normalisation_method should be an instance of a scaler, e.g.,\n",
    "    - MinMaxScaler()\n",
    "    - MaxAbsScaler()\n",
    "    - Quantile_Transform(output_distribution='uniform')\n",
    "    \n",
    "    Returns a list of tuples, each containing:\n",
    "    (X_train_scaled, X_test_scaled, Y_train, Y_test), representing data for each fold\n",
    "    \"\"\"\n",
    "    kF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=randomState)\n",
    "    kFolds_list = []\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(kF.split(X, Y)):\n",
    "        # Split the data into training and testing sets for this fold\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        Y_train, Y_test = Y.iloc[train_idx], Y.iloc[test_idx]\n",
    "        \n",
    "        # Fit the scaler on the training data and transform both train and test sets\n",
    "        X_train_scaled = normalisation_method.fit_transform(X_train)\n",
    "        X_test_scaled = normalisation_method.transform(X_test)\n",
    "        \n",
    "        if oversample:\n",
    "            # Oversample the training data if needed (e.g., using SMOTE or similar techniques)\n",
    "            # This is a placeholder; actual oversampling code should be implemented here\n",
    "            # X_train_scaled....\n",
    "            pass\n",
    "        \n",
    "        # Convert back to DataFrame to maintain column names\n",
    "        X_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns, index=X_train.index)\n",
    "        X_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns, index=X_test.index)\n",
    "        \n",
    "        # Ensure 'gender' is still binary (0 or 1)\n",
    "        if X_train_scaled['Gender'].isin([0, 1]).all():\n",
    "            kFolds_list.append((X_train_scaled, X_test_scaled, Y_train, Y_test))\n",
    "        else:\n",
    "            print(\"Warning: 'gender' column contains unexpected values after scaling.\") \n",
    "               \n",
    "        print(f\"Fold: {fold+1}, Train: {kFolds_list[fold][0].shape}, Test: {kFolds_list[fold][1].shape}\")   \n",
    "    return kFolds_list\n",
    "\n",
    "def init_weights(model): #tested already\n",
    "    if isinstance(model, nn.Linear):  # Apply only to linear layers\n",
    "        nn.init.xavier_uniform_(model.weight)\n",
    "        if model.bias is not None:\n",
    "            nn.init.zeros_(model.bias)\n",
    "            \n",
    "def fold_to_dataloader_tensor(train_x, test_x, train_y, test_y, batch_size=64, device=device):\n",
    "    train_dataset = TensorDataset(\n",
    "        torch.tensor(train_x.values,dtype=torch.float32).to(device), \n",
    "        torch.tensor(train_y.values,dtype=torch.float32).to(device))\n",
    "    val_dataset = TensorDataset(\n",
    "        torch.tensor(test_x.values,dtype=torch.float32).to(device), \n",
    "        torch.tensor(test_y.values,dtype=torch.float32).to(device))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=False)\n",
    "    return train_loader, val_loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e6f6eff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1, Train: (4593, 28), Test: (1149, 28)\n",
      "Fold: 2, Train: (4593, 28), Test: (1149, 28)\n",
      "Fold: 3, Train: (4594, 28), Test: (1148, 28)\n",
      "Fold: 4, Train: (4594, 28), Test: (1148, 28)\n",
      "Fold: 5, Train: (4594, 28), Test: (1148, 28)\n"
     ]
    }
   ],
   "source": [
    "kFolds = FOLDS_GENERATOR(X_FOR_FOLDS, Y_FOR_FOLDS, normalisation_method=MinMaxScaler(), n_splits=5, randomState=randomState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e65d95ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from modularModels1 import BlockMaker, modularNN, BasicModel\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using\", device)\n",
    "\n",
    "def init_weights(model): #tested already\n",
    "    if isinstance(model, nn.Linear):  # Apply only to linear layers\n",
    "        nn.init.xavier_uniform_(model.weight)\n",
    "        if model.bias is not None:\n",
    "            nn.init.zeros_(model.bias)\n",
    "            \n",
    "def fold_to_dataloader_tensor(train_x, test_x, train_y, test_y, batch_size=64, device=device):\n",
    "    train_dataset = TensorDataset(\n",
    "        torch.tensor(train_x.values,dtype=torch.float32).to(device), \n",
    "        torch.tensor(train_y.values,dtype=torch.float32).to(device))\n",
    "    val_dataset = TensorDataset(\n",
    "        torch.tensor(test_x.values,dtype=torch.float32).to(device), \n",
    "        torch.tensor(test_y.values,dtype=torch.float32).to(device))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=False)\n",
    "    return train_loader, val_loader \n",
    "\n",
    "\n",
    "def get_feature_count(loader):\n",
    "    \"\"\"returns the number of features in the dataset\"\"\"\n",
    "    return next(iter(loader))[0].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9231fa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Criterion_Models import *\n",
    "def criterion_mapping(criterion_choice:str, pos_weight:float=None):\n",
    "    \"\"\"\n",
    "    Feel free to add any custom loss functions here.\n",
    "    returns function for criterion\n",
    "    \"\"\"\n",
    "    if criterion_choice == \"FocalLoss\":\n",
    "        return FocalLoss()\n",
    "    elif criterion_choice == \"DiceLoss\":\n",
    "        return DiceLoss()\n",
    "    elif criterion_choice == \"BCEWithLogitsLoss\":\n",
    "        return nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight])) if pos_weight else nn.BCEWithLogitsLoss()\n",
    "    return nn.BCEWithLogitsLoss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2f5043f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            # nn.Sigmoid()\n",
    "   \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "    def last_layer(self):\n",
    "        return self.net[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2c9450e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_model = BinaryClassifier(input_dim=get_feature_count(train_loader), hidden_dim=64, dropout=0.5).to(device)\n",
    "# print(get_feature_count(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "568e7848",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, criterion, optimiser, scheduler, train_loader, val_loader, epochs=20, patience=5, device=device):\n",
    "    if isinstance(model.last_layer(), nn.Sigmoid) and isinstance(criterion, nn.BCEWithLogitsLoss):\n",
    "        raise ValueError(\"Model output is Sigmoid but criterion is BCEWithLogitsLoss. Please check your model and criterion compatibility.\")\n",
    "\n",
    "    \n",
    "    accuracy_list = []\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    f1_list = []\n",
    "    auc_list = []\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    wait = 0\n",
    "\n",
    "    \n",
    "    #* Epoch Training loop for this fold\n",
    "    for epoch in range(1,epochs+1):\n",
    "        #* Set model to training mode: essential for dropout and batch norm layers\n",
    "        model.train()\n",
    "        running_loss = 0.0 #? loss for this epoch\n",
    "        #* Mini-batch training loop\n",
    "        for batch, (inputs, labels) in enumerate(train_loader,start=1):\n",
    "            optimiser.zero_grad() #? Zero the gradients\n",
    "            outputs = model(inputs) #? Forward pass through the model\n",
    "            loss = criterion(outputs, labels) #? Calculate loss\n",
    "            loss.backward() #? Backpropagation\n",
    "            running_loss += loss.item()\n",
    "            optimiser.step() #? Update weights\n",
    "            if scheduler:\n",
    "                scheduler.step()\n",
    "                \n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch: {epoch}, training loss: {train_loss:.4f}\")\n",
    "    \n",
    "        #* Now we evaluate the model on the validation set, to track training vs validation loss\n",
    "        model.eval() #? Set model to evaluation mode\n",
    "        with torch.no_grad(): #? No need to track gradients during evaluation\n",
    "            val_loss = 0.0    \n",
    "            for batch, (inputs, labels) in enumerate(val_loader,start=1):#! one pass because val_loader batch size is all, if you want to do it in mini-batches, you MUST change the metric calculations to accept mini-batches\n",
    "                outputs = model(inputs)\n",
    "                # labels = labels.cpu() \n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() #? Calculate loss\n",
    "                avg_val_loss = val_loss / len(val_loader)\n",
    "                print(f\"Epoch {epoch}, Val Loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model_state = model.state_dict()\n",
    "            wait = 0\n",
    "        else:\n",
    "            if avg_val_loss*0.95 <= best_val_loss:\n",
    "                wait += 1\n",
    "            if wait >= patience:\n",
    "                print(f\"Early stopping triggered at epoch {epoch}\")\n",
    "                break\n",
    "    \n",
    "    #* Use best model to calculate metrics on the validation set\n",
    "    #! must be outside epoch loop, it comes after the training and cv loop\n",
    "    model.load_state_dict(best_model_state) #? Load the best model state\n",
    "    with torch.no_grad():\n",
    "        for batch, (inputs, labels) in enumerate(val_loader,start=1):#! one pass because val_loader batch size is all, if you want to do it in mini-batches, you MUST change the metric calculations to accept mini-batches\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                labels = labels.cpu() \n",
    "                predictions = (torch.sigmoid(outputs) < 0.5).float().cpu().numpy()\n",
    "                \n",
    "                val_loss += loss.item() #? Calculate loss\n",
    "                \n",
    "    #! The following should have length equal to fold number           \n",
    "    accuracy_list.append(accuracy_score(labels, predictions)) \n",
    "    precision_list.append(precision_score(labels, predictions, pos_label=1, zero_division=0)) \n",
    "    recall_list.append(recall_score(labels, predictions, pos_label=1))\n",
    "    f1_list.append(f1_score(labels, predictions, pos_label=1))\n",
    "    auc_list.append(roc_auc_score(labels, predictions)) \n",
    "\n",
    "    return model, accuracy_list, precision_list, recall_list, f1_list, auc_list \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "437927c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximise_combined_score(trial):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "    # Model hyperparameters (first-level optimization)\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 28, 128)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    initial_lr = trial.suggest_float(\"initial_lr\", 1e-5, 1e-3, log=True)\n",
    "    max_lr = trial.suggest_float(\"max_lr\", 1e-3, 1e-1, log=True)\n",
    "    \n",
    "    # Loss function hyperparameters\n",
    "    criterion_choice = trial.suggest_categorical(\"criterion\", [\"BCEWithLogitsLoss\"]) #, \"FocalLoss\", \"DiceLoss\"])\n",
    "    \n",
    "    # Hyperparameter exploration optimization\n",
    "    if criterion_choice == \"BCEWithLogitsLoss\":\n",
    "        pos_weight = trial.suggest_int(\"pos_weight\", 1, 10)\n",
    "    else:\n",
    "        pos_weight = None\n",
    "    \n",
    "    # Initialize lists for metrics across folds\n",
    "    accuracy_list = []\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    f1_list = []\n",
    "    auc_list = []\n",
    "\n",
    "    # Cross-validation loop\n",
    "    for fold, (train_x, test_x, train_y, test_y) in enumerate(kFolds, start=1):\n",
    "        # Create DataLoader for current fold\n",
    "        train_loader, val_loader = fold_to_dataloader_tensor(train_x, test_x, train_y, test_y, batch_size=64, device=device)\n",
    "        # Calculate steps_per_epoch from the current fold's train_loader\n",
    "        train_loader_len = len(train_loader)\n",
    "        \n",
    "        # Instantiate and initialize the model\n",
    "        model = BinaryClassifier(input_dim=get_feature_count(train_loader), hidden_dim=hidden_dim, dropout=dropout)\n",
    "        model.to(device)\n",
    "        model.apply(init_weights)\n",
    "        \n",
    "        # Map the choice to the actual loss function\n",
    "        criterion = criterion_mapping(criterion_choice, pos_weight).to(device)\n",
    "        optimiser = optim.Adam(model.parameters(), lr=initial_lr)\n",
    "        \n",
    "        # Initialize scheduler\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimiser,\n",
    "            max_lr=max_lr,\n",
    "            steps_per_epoch=train_loader_len,\n",
    "            epochs=100,\n",
    "            anneal_strategy='linear'\n",
    "        )\n",
    "        print(f\"Fold {fold}:\")\n",
    "        # Train and evaluate the model on the current fold\n",
    "        model, accuracy, precision, recall, f1, auc = train_and_evaluate(\n",
    "            model, criterion, optimiser, scheduler, train_loader, val_loader, epochs=10, patience=10, device=device\n",
    "        )\n",
    "\n",
    "        # Append the metrics from the current fold\n",
    "        accuracy_list.append(accuracy)\n",
    "        precision_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "        f1_list.append(f1)\n",
    "        auc_list.append(auc)\n",
    "\n",
    "    # Calculate the average metrics across all folds\n",
    "    avg_accuracy = np.sum(accuracy_list) / len(accuracy_list)\n",
    "    avg_precision = np.sum(precision_list) / len(precision_list)\n",
    "    avg_recall = np.sum(recall_list) / len(recall_list)\n",
    "    avg_f1 = np.sum(f1_list) / len(f1_list)\n",
    "    avg_auc = np.sum(auc_list) / len(auc_list)\n",
    "\n",
    "    # Combine metrics into a single \"score\"\n",
    "    combined_score = (avg_f1 + avg_precision + avg_recall + avg_accuracy + avg_auc) / 5\n",
    "\n",
    "    return combined_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5a926611",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-07 19:17:24,154] A new study created in memory with name: Basic\n",
      "Bottle v0.13.2 server starting up (using WSGIRefServer())...\n",
      "Listening on http://localhost:8080/\n",
      "Hit Ctrl-C to quit.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Fold 1:\n",
      "Epoch: 1, training loss: 1.3699\n",
      "Epoch 1, Val Loss: 1.3111\n",
      "Epoch: 2, training loss: 1.2935\n",
      "Epoch 2, Val Loss: 1.2683\n",
      "Epoch: 3, training loss: 1.2696\n",
      "Epoch 3, Val Loss: 1.2338\n",
      "Epoch: 4, training loss: 1.2409\n",
      "Epoch 4, Val Loss: 1.1977\n",
      "Epoch: 5, training loss: 1.2212\n",
      "Epoch 5, Val Loss: 1.1694\n",
      "Epoch: 6, training loss: 1.1983\n",
      "Epoch 6, Val Loss: 1.1422\n",
      "Epoch: 7, training loss: 1.1789\n",
      "Epoch 7, Val Loss: 1.1223\n",
      "Epoch: 8, training loss: 1.1598\n",
      "Epoch 8, Val Loss: 1.1172\n",
      "Epoch: 9, training loss: 1.1425\n",
      "Epoch 9, Val Loss: 1.1056\n",
      "Epoch: 10, training loss: 1.1482\n",
      "Epoch 10, Val Loss: 1.1254\n",
      "Fold 2:\n",
      "Epoch: 1, training loss: 1.3091\n",
      "Epoch 1, Val Loss: 1.2916\n",
      "Epoch: 2, training loss: 1.2821\n",
      "Epoch 2, Val Loss: 1.2648\n",
      "Epoch: 3, training loss: 1.2567\n",
      "Epoch 3, Val Loss: 1.2336\n",
      "Epoch: 4, training loss: 1.2230\n",
      "Epoch 4, Val Loss: 1.2053\n",
      "Epoch: 5, training loss: 1.1972\n",
      "Epoch 5, Val Loss: 1.1823\n",
      "Epoch: 6, training loss: 1.1755\n",
      "Epoch 6, Val Loss: 1.1640\n",
      "Epoch: 7, training loss: 1.1464\n",
      "Epoch 7, Val Loss: 1.1464\n",
      "Epoch: 8, training loss: 1.1453\n",
      "Epoch 8, Val Loss: 1.1385\n",
      "Epoch: 9, training loss: 1.1347\n",
      "Epoch 9, Val Loss: 1.1284\n",
      "Epoch: 10, training loss: 1.1273\n",
      "Epoch 10, Val Loss: 1.1297\n",
      "Fold 3:\n",
      "Epoch: 1, training loss: 1.3215\n",
      "Epoch 1, Val Loss: 1.2923\n",
      "Epoch: 2, training loss: 1.2862\n",
      "Epoch 2, Val Loss: 1.2663\n",
      "Epoch: 3, training loss: 1.2538\n",
      "Epoch 3, Val Loss: 1.2396\n",
      "Epoch: 4, training loss: 1.2273\n",
      "Epoch 4, Val Loss: 1.2165\n",
      "Epoch: 5, training loss: 1.1899\n",
      "Epoch 5, Val Loss: 1.2046\n",
      "Epoch: 6, training loss: 1.1776\n",
      "Epoch 6, Val Loss: 1.1855\n",
      "Epoch: 7, training loss: 1.1529\n",
      "Epoch 7, Val Loss: 1.1743\n",
      "Epoch: 8, training loss: 1.1280\n",
      "Epoch 8, Val Loss: 1.1669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [07/Apr/2025 19:17:27] \"GET / HTTP/1.1\" 302 0\n",
      "127.0.0.1 - - [07/Apr/2025 19:17:27] \"GET /dashboard HTTP/1.1\" 200 4145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, training loss: 1.1267\n",
      "Epoch 9, Val Loss: 1.1643\n",
      "Epoch: 10, training loss: 1.1252\n",
      "Epoch 10, Val Loss: 1.1587\n",
      "Fold 4:\n",
      "Epoch: 1, training loss: 1.3338\n",
      "Epoch 1, Val Loss: 1.2953\n",
      "Epoch: 2, training loss: 1.3009\n",
      "Epoch 2, Val Loss: 1.2696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [07/Apr/2025 19:17:27] \"GET /static/bundle.js HTTP/1.1\" 200 4140872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, training loss: 1.2737\n",
      "Epoch 3, Val Loss: 1.2418\n",
      "Epoch: 4, training loss: 1.2339\n",
      "Epoch 4, Val Loss: 1.2129\n",
      "Epoch: 5, training loss: 1.2088\n",
      "Epoch 5, Val Loss: 1.1854\n",
      "Epoch: 6, training loss: 1.1775\n",
      "Epoch 6, Val Loss: 1.1702\n",
      "Epoch: 7, training loss: 1.1536\n",
      "Epoch 7, Val Loss: 1.1523\n",
      "Epoch: 8, training loss: 1.1505\n",
      "Epoch 8, Val Loss: 1.1414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [07/Apr/2025 19:17:28] \"GET /api/studies HTTP/1.1\" 200 133\n",
      "127.0.0.1 - - [07/Apr/2025 19:17:28] \"GET /favicon.ico HTTP/1.1\" 304 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, training loss: 1.1334\n",
      "Epoch 9, Val Loss: 1.1364\n",
      "Epoch: 10, training loss: 1.1281\n",
      "Epoch 10, Val Loss: 1.1316\n",
      "Fold 5:\n",
      "Epoch: 1, training loss: 1.3412\n",
      "Epoch 1, Val Loss: 1.3077\n",
      "Epoch: 2, training loss: 1.3001\n",
      "Epoch 2, Val Loss: 1.2747\n",
      "Epoch: 3, training loss: 1.2519\n",
      "Epoch 3, Val Loss: 1.2374\n",
      "Epoch: 4, training loss: 1.2199\n",
      "Epoch 4, Val Loss: 1.2038\n",
      "Epoch: 5, training loss: 1.1888\n",
      "Epoch 5, Val Loss: 1.1784\n",
      "Epoch: 6, training loss: 1.1762\n",
      "Epoch 6, Val Loss: 1.1597\n",
      "Epoch: 7, training loss: 1.1637\n",
      "Epoch 7, Val Loss: 1.1432\n",
      "Epoch: 8, training loss: 1.1473\n",
      "Epoch 8, Val Loss: 1.1365\n",
      "Epoch: 9, training loss: 1.1403\n",
      "Epoch 9, Val Loss: 1.1343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-07 19:17:30,022] Trial 0 finished with value: 0.2204872629390219 and parameters: {'hidden_dim': 42, 'dropout': 0.3083545286897128, 'initial_lr': 0.00039112177343249527, 'max_lr': 0.0073776552210538734, 'criterion': 'BCEWithLogitsLoss', 'pos_weight': 10}. Best is trial 0 with value: 0.2204872629390219.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, training loss: 1.1216\n",
      "Epoch 10, Val Loss: 1.1286\n",
      "Using device: cuda\n",
      "Fold 1:\n",
      "Epoch: 1, training loss: 1.2531\n",
      "Epoch 1, Val Loss: 1.2021\n",
      "Epoch: 2, training loss: 1.1762\n",
      "Epoch 2, Val Loss: 1.1321\n",
      "Epoch: 3, training loss: 1.1357\n",
      "Epoch 3, Val Loss: 1.0783\n",
      "Epoch: 4, training loss: 1.0939\n",
      "Epoch 4, Val Loss: 1.0475\n",
      "Epoch: 5, training loss: 1.0843\n",
      "Epoch 5, Val Loss: 1.0448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [07/Apr/2025 19:17:30] \"GET /api/studies/0?after=0 HTTP/1.1\" 200 7255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, training loss: 1.0898\n",
      "Epoch 6, Val Loss: 1.0623\n",
      "Epoch: 7, training loss: 1.0739\n",
      "Epoch 7, Val Loss: 1.0290\n",
      "Epoch: 8, training loss: 1.0599\n",
      "Epoch 8, Val Loss: 1.0349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [07/Apr/2025 19:17:30] \"GET /api/studies/0/param_importances?evaluator=ped_anova HTTP/1.1\" 200 27\n",
      "127.0.0.1 - - [07/Apr/2025 19:17:30] \"GET /api/meta HTTP/1.1\" 200 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, training loss: 1.0738\n",
      "Epoch 9, Val Loss: 1.0307\n",
      "Epoch: 10, training loss: 1.0715\n",
      "Epoch 10, Val Loss: 1.0521\n",
      "Fold 2:\n",
      "Epoch: 1, training loss: 1.2359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [07/Apr/2025 19:17:30] \"GET /api/studies/0/param_importances?evaluator=ped_anova HTTP/1.1\" 200 27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Val Loss: 1.2006\n",
      "Epoch: 2, training loss: 1.1621\n",
      "Epoch 2, Val Loss: 1.1373\n",
      "Epoch: 3, training loss: 1.1066\n",
      "Epoch 3, Val Loss: 1.1037\n",
      "Epoch: 4, training loss: 1.0769\n",
      "Epoch 4, Val Loss: 1.0860\n",
      "Epoch: 5, training loss: 1.0619\n",
      "Epoch 5, Val Loss: 1.0697\n",
      "Epoch: 6, training loss: 1.0807\n",
      "Epoch 6, Val Loss: 1.1146\n",
      "Epoch: 7, training loss: 1.0573\n",
      "Epoch 7, Val Loss: 1.0689\n",
      "Epoch: 8, training loss: 1.0573\n",
      "Epoch 8, Val Loss: 1.0824\n",
      "Epoch: 9, training loss: 1.0437\n",
      "Epoch 9, Val Loss: 1.0719\n",
      "Epoch: 10, training loss: 1.0488\n",
      "Epoch 10, Val Loss: 1.0674\n",
      "Fold 3:\n",
      "Epoch: 1, training loss: 1.2082\n",
      "Epoch 1, Val Loss: 1.1837\n",
      "Epoch: 2, training loss: 1.1457\n",
      "Epoch 2, Val Loss: 1.1362\n",
      "Epoch: 3, training loss: 1.1019\n",
      "Epoch 3, Val Loss: 1.1092\n",
      "Epoch: 4, training loss: 1.0661\n",
      "Epoch 4, Val Loss: 1.1061\n",
      "Epoch: 5, training loss: 1.0637\n",
      "Epoch 5, Val Loss: 1.1118\n",
      "Epoch: 6, training loss: 1.0528\n",
      "Epoch 6, Val Loss: 1.0982\n",
      "Epoch: 7, training loss: 1.0402\n",
      "Epoch 7, Val Loss: 1.0993\n",
      "Epoch: 8, training loss: 1.0532\n",
      "Epoch 8, Val Loss: 1.0950\n",
      "Epoch: 9, training loss: 1.0570\n",
      "Epoch 9, Val Loss: 1.1127\n",
      "Epoch: 10, training loss: 1.0356\n",
      "Epoch 10, Val Loss: 1.0954\n",
      "Fold 4:\n",
      "Epoch: 1, training loss: 1.2286\n",
      "Epoch 1, Val Loss: 1.2037\n",
      "Epoch: 2, training loss: 1.1609\n",
      "Epoch 2, Val Loss: 1.1419\n",
      "Epoch: 3, training loss: 1.1038\n",
      "Epoch 3, Val Loss: 1.0973\n",
      "Epoch: 4, training loss: 1.0726\n",
      "Epoch 4, Val Loss: 1.0901\n",
      "Epoch: 5, training loss: 1.0773\n",
      "Epoch 5, Val Loss: 1.0762\n",
      "Epoch: 6, training loss: 1.0587\n",
      "Epoch 6, Val Loss: 1.0857\n",
      "Epoch: 7, training loss: 1.0628\n",
      "Epoch 7, Val Loss: 1.0794\n",
      "Epoch: 8, training loss: 1.0529\n",
      "Epoch 8, Val Loss: 1.0906\n",
      "Epoch: 9, training loss: 1.0494\n",
      "Epoch 9, Val Loss: 1.0730\n",
      "Epoch: 10, training loss: 1.0563\n",
      "Epoch 10, Val Loss: 1.1430\n",
      "Fold 5:\n",
      "Epoch: 1, training loss: 1.2236\n",
      "Epoch 1, Val Loss: 1.1928\n",
      "Epoch: 2, training loss: 1.1540\n",
      "Epoch 2, Val Loss: 1.1501\n",
      "Epoch: 3, training loss: 1.1105\n",
      "Epoch 3, Val Loss: 1.1113\n",
      "Epoch: 4, training loss: 1.0815\n",
      "Epoch 4, Val Loss: 1.0900\n",
      "Epoch: 5, training loss: 1.0727\n",
      "Epoch 5, Val Loss: 1.0857\n",
      "Epoch: 6, training loss: 1.0712\n",
      "Epoch 6, Val Loss: 1.0663\n",
      "Epoch: 7, training loss: 1.0542\n",
      "Epoch 7, Val Loss: 1.0713\n",
      "Epoch: 8, training loss: 1.0517\n",
      "Epoch 8, Val Loss: 1.1264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-07 19:17:42,116] Trial 1 finished with value: 0.21522238516904163 and parameters: {'hidden_dim': 110, 'dropout': 0.16237702329807358, 'initial_lr': 0.0008591051396263788, 'max_lr': 0.013937323730651315, 'criterion': 'BCEWithLogitsLoss', 'pos_weight': 9}. Best is trial 0 with value: 0.2204872629390219.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, training loss: 1.0545\n",
      "Epoch 9, Val Loss: 1.1085\n",
      "Epoch: 10, training loss: 1.0528\n",
      "Epoch 10, Val Loss: 1.0968\n",
      "Using device: cuda\n",
      "Fold 1:\n",
      "Epoch: 1, training loss: 0.8329\n",
      "Epoch 1, Val Loss: 0.8016\n",
      "Epoch: 2, training loss: 0.7940\n",
      "Epoch 2, Val Loss: 0.7776\n",
      "Epoch: 3, training loss: 0.7755\n",
      "Epoch 3, Val Loss: 0.7523\n",
      "Epoch: 4, training loss: 0.7500\n",
      "Epoch 4, Val Loss: 0.7287\n",
      "Epoch: 5, training loss: 0.7358\n",
      "Epoch 5, Val Loss: 0.7102\n",
      "Epoch: 6, training loss: 0.7157\n",
      "Epoch 6, Val Loss: 0.7003\n",
      "Epoch: 7, training loss: 0.7150\n",
      "Epoch 7, Val Loss: 0.6955\n",
      "Epoch: 8, training loss: 0.7019\n",
      "Epoch 8, Val Loss: 0.6848\n",
      "Epoch: 9, training loss: 0.6941\n",
      "Epoch 9, Val Loss: 0.6796\n",
      "Epoch: 10, training loss: 0.6949\n",
      "Epoch 10, Val Loss: 0.6844\n",
      "Fold 2:\n",
      "Epoch: 1, training loss: 0.8929\n",
      "Epoch 1, Val Loss: 0.8272\n",
      "Epoch: 2, training loss: 0.8114\n",
      "Epoch 2, Val Loss: 0.7936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [07/Apr/2025 19:17:43] \"GET /api/studies/0?after=1 HTTP/1.1\" 200 7256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, training loss: 0.7846\n",
      "Epoch 3, Val Loss: 0.7695\n",
      "Epoch: 4, training loss: 0.7601\n",
      "Epoch 4, Val Loss: 0.7480\n",
      "Epoch: 5, training loss: 0.7431\n",
      "Epoch 5, Val Loss: 0.7299\n",
      "Epoch: 6, training loss: 0.7209\n",
      "Epoch 6, Val Loss: 0.7160\n",
      "Epoch: 7, training loss: 0.7052\n",
      "Epoch 7, Val Loss: 0.7101\n",
      "Epoch: 8, training loss: 0.6964\n",
      "Epoch 8, Val Loss: 0.7033\n",
      "Epoch: 9, training loss: 0.6868\n",
      "Epoch 9, Val Loss: 0.7010\n",
      "Epoch: 10, training loss: 0.6893\n",
      "Epoch 10, Val Loss: 0.6976\n",
      "Fold 3:\n",
      "Epoch: 1, training loss: 0.8445\n",
      "Epoch 1, Val Loss: 0.8155\n",
      "Epoch: 2, training loss: 0.8007\n",
      "Epoch 2, Val Loss: 0.7919\n",
      "Epoch: 3, training loss: 0.7758\n",
      "Epoch 3, Val Loss: 0.7704\n",
      "Epoch: 4, training loss: 0.7538\n",
      "Epoch 4, Val Loss: 0.7511\n",
      "Epoch: 5, training loss: 0.7318\n",
      "Epoch 5, Val Loss: 0.7370\n",
      "Epoch: 6, training loss: 0.7180\n",
      "Epoch 6, Val Loss: 0.7254\n",
      "Epoch: 7, training loss: 0.7053\n",
      "Epoch 7, Val Loss: 0.7189\n",
      "Epoch: 8, training loss: 0.6967\n",
      "Epoch 8, Val Loss: 0.7149\n",
      "Epoch: 9, training loss: 0.6876\n",
      "Epoch 9, Val Loss: 0.7115\n",
      "Epoch: 10, training loss: 0.6797\n",
      "Epoch 10, Val Loss: 0.7104\n",
      "Fold 4:\n",
      "Epoch: 1, training loss: 0.8428\n",
      "Epoch 1, Val Loss: 0.8069\n",
      "Epoch: 2, training loss: 0.7997\n",
      "Epoch 2, Val Loss: 0.7839\n",
      "Epoch: 3, training loss: 0.7775\n",
      "Epoch 3, Val Loss: 0.7632\n",
      "Epoch: 4, training loss: 0.7575\n",
      "Epoch 4, Val Loss: 0.7435\n",
      "Epoch: 5, training loss: 0.7333\n",
      "Epoch 5, Val Loss: 0.7282\n",
      "Epoch: 6, training loss: 0.7235\n",
      "Epoch 6, Val Loss: 0.7133\n",
      "Epoch: 7, training loss: 0.7091\n",
      "Epoch 7, Val Loss: 0.7029\n",
      "Epoch: 8, training loss: 0.6991\n",
      "Epoch 8, Val Loss: 0.6947\n",
      "Epoch: 9, training loss: 0.6949\n",
      "Epoch 9, Val Loss: 0.6927\n",
      "Epoch: 10, training loss: 0.6927\n",
      "Epoch 10, Val Loss: 0.6964\n",
      "Fold 5:\n",
      "Epoch: 1, training loss: 0.8419\n",
      "Epoch 1, Val Loss: 0.8111\n",
      "Epoch: 2, training loss: 0.8021\n",
      "Epoch 2, Val Loss: 0.7937\n",
      "Epoch: 3, training loss: 0.7795\n",
      "Epoch 3, Val Loss: 0.7766\n",
      "Epoch: 4, training loss: 0.7595\n",
      "Epoch 4, Val Loss: 0.7611\n",
      "Epoch: 5, training loss: 0.7373\n",
      "Epoch 5, Val Loss: 0.7449\n",
      "Epoch: 6, training loss: 0.7281\n",
      "Epoch 6, Val Loss: 0.7327\n",
      "Epoch: 7, training loss: 0.7059\n",
      "Epoch 7, Val Loss: 0.7216\n",
      "Epoch: 8, training loss: 0.7053\n",
      "Epoch 8, Val Loss: 0.7156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-07 19:17:47,025] Trial 2 finished with value: 0.2808213828871316 and parameters: {'hidden_dim': 118, 'dropout': 0.31934716438193034, 'initial_lr': 1.9518104748504096e-05, 'max_lr': 0.004443585800868465, 'criterion': 'BCEWithLogitsLoss', 'pos_weight': 4}. Best is trial 2 with value: 0.2808213828871316.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, training loss: 0.6995\n",
      "Epoch 9, Val Loss: 0.7088\n",
      "Epoch: 10, training loss: 0.6925\n",
      "Epoch 10, Val Loss: 0.7115\n",
      "Using device: cuda\n",
      "Fold 1:\n",
      "Epoch: 1, training loss: 0.9345\n",
      "Epoch 1, Val Loss: 0.8821\n",
      "Epoch: 2, training loss: 0.8615\n",
      "Epoch 2, Val Loss: 0.8321\n",
      "Epoch: 3, training loss: 0.8223\n",
      "Epoch 3, Val Loss: 0.7821\n",
      "Epoch: 4, training loss: 0.8039\n",
      "Epoch 4, Val Loss: 0.7653\n",
      "Epoch: 5, training loss: 0.8020\n",
      "Epoch 5, Val Loss: 0.7853\n",
      "Epoch: 6, training loss: 0.7916\n",
      "Epoch 6, Val Loss: 0.7644\n",
      "Epoch: 7, training loss: 0.7856\n",
      "Epoch 7, Val Loss: 0.7592\n",
      "Epoch: 8, training loss: 0.7874\n",
      "Epoch 8, Val Loss: 0.7750\n",
      "Epoch: 9, training loss: 0.7897\n",
      "Epoch 9, Val Loss: 0.7644\n",
      "Epoch: 10, training loss: 0.7772\n",
      "Epoch 10, Val Loss: 0.7851\n",
      "Fold 2:\n",
      "Epoch: 1, training loss: 0.9247\n",
      "Epoch 1, Val Loss: 0.8804\n",
      "Epoch: 2, training loss: 0.8574\n",
      "Epoch 2, Val Loss: 0.8268\n",
      "Epoch: 3, training loss: 0.8086\n",
      "Epoch 3, Val Loss: 0.8000\n",
      "Epoch: 4, training loss: 0.7898\n",
      "Epoch 4, Val Loss: 0.7954\n",
      "Epoch: 5, training loss: 0.7863\n",
      "Epoch 5, Val Loss: 0.8213\n",
      "Epoch: 6, training loss: 0.7945\n",
      "Epoch 6, Val Loss: 0.7983\n",
      "Epoch: 7, training loss: 0.7740\n",
      "Epoch 7, Val Loss: 0.8078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [07/Apr/2025 19:17:53] \"GET /api/studies/0?after=2 HTTP/1.1\" 200 7253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, training loss: 0.7823\n",
      "Epoch 8, Val Loss: 0.7949\n",
      "Epoch: 9, training loss: 0.7861\n",
      "Epoch 9, Val Loss: 0.7926\n",
      "Epoch: 10, training loss: 0.7782\n",
      "Epoch 10, Val Loss: 0.8028\n",
      "Fold 3:\n",
      "Epoch: 1, training loss: 0.9036\n",
      "Epoch 1, Val Loss: 0.8684\n",
      "Epoch: 2, training loss: 0.8422\n",
      "Epoch 2, Val Loss: 0.8343\n",
      "Epoch: 3, training loss: 0.8134\n",
      "Epoch 3, Val Loss: 0.8096\n",
      "Epoch: 4, training loss: 0.7844\n",
      "Epoch 4, Val Loss: 0.8068\n",
      "Epoch: 5, training loss: 0.7828\n",
      "Epoch 5, Val Loss: 0.8073\n",
      "Epoch: 6, training loss: 0.7754\n",
      "Epoch 6, Val Loss: 0.8102\n",
      "Epoch: 7, training loss: 0.7811\n",
      "Epoch 7, Val Loss: 0.8177\n",
      "Epoch: 8, training loss: 0.7722\n",
      "Epoch 8, Val Loss: 0.8084\n",
      "Epoch: 9, training loss: 0.7683\n",
      "Epoch 9, Val Loss: 0.8071\n",
      "Epoch: 10, training loss: 0.7795\n",
      "Epoch 10, Val Loss: 0.8013\n",
      "Fold 4:\n",
      "Epoch: 1, training loss: 0.9034\n",
      "Epoch 1, Val Loss: 0.8678\n",
      "Epoch: 2, training loss: 0.8466\n",
      "Epoch 2, Val Loss: 0.8125\n",
      "Epoch: 3, training loss: 0.8176\n",
      "Epoch 3, Val Loss: 0.7963\n",
      "Epoch: 4, training loss: 0.7961\n",
      "Epoch 4, Val Loss: 0.7793\n",
      "Epoch: 5, training loss: 0.7927\n",
      "Epoch 5, Val Loss: 0.7961\n",
      "Epoch: 6, training loss: 0.7868\n",
      "Epoch 6, Val Loss: 0.7821\n",
      "Epoch: 7, training loss: 0.7847\n",
      "Epoch 7, Val Loss: 0.7850\n",
      "Epoch: 8, training loss: 0.7871\n",
      "Epoch 8, Val Loss: 0.7842\n",
      "Epoch: 9, training loss: 0.7802\n",
      "Epoch 9, Val Loss: 0.7867\n",
      "Epoch: 10, training loss: 0.7758\n",
      "Epoch 10, Val Loss: 0.7964\n",
      "Fold 5:\n",
      "Epoch: 1, training loss: 0.9058\n",
      "Epoch 1, Val Loss: 0.8836\n",
      "Epoch: 2, training loss: 0.8453\n",
      "Epoch 2, Val Loss: 0.8502\n",
      "Epoch: 3, training loss: 0.8200\n",
      "Epoch 3, Val Loss: 0.8280\n",
      "Epoch: 4, training loss: 0.8042\n",
      "Epoch 4, Val Loss: 0.8037\n",
      "Epoch: 5, training loss: 0.8007\n",
      "Epoch 5, Val Loss: 0.8095\n",
      "Epoch: 6, training loss: 0.7946\n",
      "Epoch 6, Val Loss: 0.8079\n",
      "Epoch: 7, training loss: 0.7905\n",
      "Epoch 7, Val Loss: 0.8158\n",
      "Epoch: 8, training loss: 0.7878\n",
      "Epoch 8, Val Loss: 0.8093\n",
      "Epoch: 9, training loss: 0.7786\n",
      "Epoch 9, Val Loss: 0.8399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-07 19:18:01,807] Trial 3 finished with value: 0.25889625896865953 and parameters: {'hidden_dim': 78, 'dropout': 0.27373305127140557, 'initial_lr': 1.489417816075987e-05, 'max_lr': 0.025821823969079016, 'criterion': 'BCEWithLogitsLoss', 'pos_weight': 5}. Best is trial 2 with value: 0.2808213828871316.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, training loss: 0.7824\n",
      "Epoch 10, Val Loss: 0.8116\n",
      "Using device: cuda\n",
      "Fold 1:\n",
      "Epoch: 1, training loss: 0.8487\n",
      "Epoch 1, Val Loss: 0.7998\n",
      "Epoch: 2, training loss: 0.7944\n",
      "Epoch 2, Val Loss: 0.7750\n",
      "Epoch: 3, training loss: 0.7773\n",
      "Epoch 3, Val Loss: 0.7521\n",
      "Epoch: 4, training loss: 0.7573\n",
      "Epoch 4, Val Loss: 0.7283\n",
      "Epoch: 5, training loss: 0.7412\n",
      "Epoch 5, Val Loss: 0.7134\n",
      "Epoch: 6, training loss: 0.7231\n",
      "Epoch 6, Val Loss: 0.6976\n",
      "Epoch: 7, training loss: 0.7141\n",
      "Epoch 7, Val Loss: 0.6915\n",
      "Epoch: 8, training loss: 0.7111\n",
      "Epoch 8, Val Loss: 0.6879\n",
      "Epoch: 9, training loss: 0.7075\n",
      "Epoch 9, Val Loss: 0.7036\n",
      "Epoch: 10, training loss: 0.7060\n",
      "Epoch 10, Val Loss: 0.6797\n",
      "Fold 2:\n",
      "Epoch: 1, training loss: 0.8575\n",
      "Epoch 1, Val Loss: 0.8006\n",
      "Epoch: 2, training loss: 0.7959\n",
      "Epoch 2, Val Loss: 0.7787\n",
      "Epoch: 3, training loss: 0.7687\n",
      "Epoch 3, Val Loss: 0.7588\n",
      "Epoch: 4, training loss: 0.7442\n",
      "Epoch 4, Val Loss: 0.7366\n",
      "Epoch: 5, training loss: 0.7307\n",
      "Epoch 5, Val Loss: 0.7234\n",
      "Epoch: 6, training loss: 0.7160\n",
      "Epoch 6, Val Loss: 0.7115\n",
      "Epoch: 7, training loss: 0.7128\n",
      "Epoch 7, Val Loss: 0.7052\n",
      "Epoch: 8, training loss: 0.7068\n",
      "Epoch 8, Val Loss: 0.7028\n",
      "Epoch: 9, training loss: 0.6987\n",
      "Epoch 9, Val Loss: 0.6986\n",
      "Epoch: 10, training loss: 0.6951\n",
      "Epoch 10, Val Loss: 0.7142\n",
      "Fold 3:\n",
      "Epoch: 1, training loss: 0.8764\n",
      "Epoch 1, Val Loss: 0.7986\n",
      "Epoch: 2, training loss: 0.7892\n",
      "Epoch 2, Val Loss: 0.7771\n",
      "Epoch: 3, training loss: 0.7633\n",
      "Epoch 3, Val Loss: 0.7586\n",
      "Epoch: 4, training loss: 0.7408\n",
      "Epoch 4, Val Loss: 0.7408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [07/Apr/2025 19:18:04] \"GET /api/studies/0?after=3 HTTP/1.1\" 200 7248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, training loss: 0.7274\n",
      "Epoch 5, Val Loss: 0.7279\n",
      "Epoch: 6, training loss: 0.7165\n",
      "Epoch 6, Val Loss: 0.7250\n",
      "Epoch: 7, training loss: 0.6969\n",
      "Epoch 7, Val Loss: 0.7154\n",
      "Epoch: 8, training loss: 0.6939\n",
      "Epoch 8, Val Loss: 0.7134\n",
      "Epoch: 9, training loss: 0.6913\n",
      "Epoch 9, Val Loss: 0.7170\n",
      "Epoch: 10, training loss: 0.6919\n",
      "Epoch 10, Val Loss: 0.7175\n",
      "Fold 4:\n",
      "Epoch: 1, training loss: 0.8377\n",
      "Epoch 1, Val Loss: 0.7966\n",
      "Epoch: 2, training loss: 0.7942\n",
      "Epoch 2, Val Loss: 0.7718\n",
      "Epoch: 3, training loss: 0.7690\n",
      "Epoch 3, Val Loss: 0.7460\n",
      "Epoch: 4, training loss: 0.7447\n",
      "Epoch 4, Val Loss: 0.7294\n",
      "Epoch: 5, training loss: 0.7237\n",
      "Epoch 5, Val Loss: 0.7107\n",
      "Epoch: 6, training loss: 0.7150\n",
      "Epoch 6, Val Loss: 0.7025\n",
      "Epoch: 7, training loss: 0.7055\n",
      "Epoch 7, Val Loss: 0.6954\n",
      "Epoch: 8, training loss: 0.7011\n",
      "Epoch 8, Val Loss: 0.7043\n",
      "Epoch: 9, training loss: 0.6977\n",
      "Epoch 9, Val Loss: 0.7021\n",
      "Epoch: 10, training loss: 0.6904\n",
      "Epoch 10, Val Loss: 0.6898\n",
      "Fold 5:\n",
      "Epoch: 1, training loss: 0.8462\n",
      "Epoch 1, Val Loss: 0.8100\n",
      "Epoch: 2, training loss: 0.7976\n",
      "Epoch 2, Val Loss: 0.7891\n",
      "Epoch: 3, training loss: 0.7712\n",
      "Epoch 3, Val Loss: 0.7718\n",
      "Epoch: 4, training loss: 0.7546\n",
      "Epoch 4, Val Loss: 0.7679\n",
      "Epoch: 5, training loss: 0.7368\n",
      "Epoch 5, Val Loss: 0.7465\n",
      "Epoch: 6, training loss: 0.7250\n",
      "Epoch 6, Val Loss: 0.7325\n",
      "Epoch: 7, training loss: 0.7052\n",
      "Epoch 7, Val Loss: 0.7269\n",
      "Epoch: 8, training loss: 0.7038\n",
      "Epoch 8, Val Loss: 0.7151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-07 19:18:07,071] Trial 4 finished with value: 0.28553544865251146 and parameters: {'hidden_dim': 127, 'dropout': 0.48219433657777855, 'initial_lr': 0.000397582730972269, 'max_lr': 0.00690830295915498, 'criterion': 'BCEWithLogitsLoss', 'pos_weight': 4}. Best is trial 4 with value: 0.28553544865251146.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, training loss: 0.6958\n",
      "Epoch 9, Val Loss: 0.7176\n",
      "Epoch: 10, training loss: 0.6969\n",
      "Epoch 10, Val Loss: 0.7151\n",
      "Using device: cuda\n",
      "Fold 1:\n",
      "Epoch: 1, training loss: 1.3946\n",
      "Epoch 1, Val Loss: 1.3335\n",
      "Epoch: 2, training loss: 1.3342\n",
      "Epoch 2, Val Loss: 1.2921\n",
      "Epoch: 3, training loss: 1.2984\n",
      "Epoch 3, Val Loss: 1.2587\n",
      "Epoch: 4, training loss: 1.2583\n",
      "Epoch 4, Val Loss: 1.2244\n",
      "Epoch: 5, training loss: 1.2445\n",
      "Epoch 5, Val Loss: 1.1969\n",
      "Epoch: 6, training loss: 1.2233\n",
      "Epoch 6, Val Loss: 1.1799\n",
      "Epoch: 7, training loss: 1.1937\n",
      "Epoch 7, Val Loss: 1.1588\n",
      "Epoch: 8, training loss: 1.2015\n",
      "Epoch 8, Val Loss: 1.1388\n",
      "Epoch: 9, training loss: 1.1917\n",
      "Epoch 9, Val Loss: 1.1331\n",
      "Epoch: 10, training loss: 1.1726\n",
      "Epoch 10, Val Loss: 1.1279\n",
      "Fold 2:\n",
      "Epoch: 1, training loss: 1.3592\n",
      "Epoch 1, Val Loss: 1.3241\n",
      "Epoch: 2, training loss: 1.3300\n",
      "Epoch 2, Val Loss: 1.3034\n",
      "Epoch: 3, training loss: 1.2968\n",
      "Epoch 3, Val Loss: 1.2806\n",
      "Epoch: 4, training loss: 1.2770\n",
      "Epoch 4, Val Loss: 1.2562\n",
      "Epoch: 5, training loss: 1.2521\n",
      "Epoch 5, Val Loss: 1.2349\n",
      "Epoch: 6, training loss: 1.2262\n",
      "Epoch 6, Val Loss: 1.2122\n",
      "Epoch: 7, training loss: 1.2062\n",
      "Epoch 7, Val Loss: 1.1894\n",
      "Epoch: 8, training loss: 1.1878\n",
      "Epoch 8, Val Loss: 1.1692\n",
      "Epoch: 9, training loss: 1.1784\n",
      "Epoch 9, Val Loss: 1.1649\n",
      "Epoch: 10, training loss: 1.1567\n",
      "Epoch 10, Val Loss: 1.1519\n",
      "Fold 3:\n",
      "Epoch: 1, training loss: 1.3499\n",
      "Epoch 1, Val Loss: 1.3206\n",
      "Epoch: 2, training loss: 1.3241\n",
      "Epoch 2, Val Loss: 1.2959\n",
      "Epoch: 3, training loss: 1.2934\n",
      "Epoch 3, Val Loss: 1.2737\n",
      "Epoch: 4, training loss: 1.2704\n",
      "Epoch 4, Val Loss: 1.2469\n",
      "Epoch: 5, training loss: 1.2444\n",
      "Epoch 5, Val Loss: 1.2213\n",
      "Epoch: 6, training loss: 1.2165\n",
      "Epoch 6, Val Loss: 1.2006\n",
      "Epoch: 7, training loss: 1.1979\n",
      "Epoch 7, Val Loss: 1.1930\n",
      "Epoch: 8, training loss: 1.1805\n",
      "Epoch 8, Val Loss: 1.1791\n",
      "Epoch: 9, training loss: 1.1609\n",
      "Epoch 9, Val Loss: 1.1711\n",
      "Epoch: 10, training loss: 1.1675\n",
      "Epoch 10, Val Loss: 1.1674\n",
      "Fold 4:\n",
      "Epoch: 1, training loss: 1.3706\n",
      "Epoch 1, Val Loss: 1.3222\n",
      "Epoch: 2, training loss: 1.3320\n",
      "Epoch 2, Val Loss: 1.2869\n",
      "Epoch: 3, training loss: 1.2921\n",
      "Epoch 3, Val Loss: 1.2685\n",
      "Epoch: 4, training loss: 1.2677\n",
      "Epoch 4, Val Loss: 1.2460\n",
      "Epoch: 5, training loss: 1.2552\n",
      "Epoch 5, Val Loss: 1.2294\n",
      "Epoch: 6, training loss: 1.2368\n",
      "Epoch 6, Val Loss: 1.2096\n",
      "Epoch: 7, training loss: 1.1992\n",
      "Epoch 7, Val Loss: 1.1899\n",
      "Epoch: 8, training loss: 1.1922\n",
      "Epoch 8, Val Loss: 1.1752\n",
      "Epoch: 9, training loss: 1.1762\n",
      "Epoch 9, Val Loss: 1.1649\n",
      "Epoch: 10, training loss: 1.1771\n",
      "Epoch 10, Val Loss: 1.1578\n",
      "Fold 5:\n",
      "Epoch: 1, training loss: 1.4435\n",
      "Epoch 1, Val Loss: 1.3664\n",
      "Epoch: 2, training loss: 1.3562\n",
      "Epoch 2, Val Loss: 1.3153\n",
      "Epoch: 3, training loss: 1.3050\n",
      "Epoch 3, Val Loss: 1.2898\n",
      "Epoch: 4, training loss: 1.2755\n",
      "Epoch 4, Val Loss: 1.2655\n",
      "Epoch: 5, training loss: 1.2593\n",
      "Epoch 5, Val Loss: 1.2500\n",
      "Epoch: 6, training loss: 1.2426\n",
      "Epoch 6, Val Loss: 1.2305\n",
      "Epoch: 7, training loss: 1.2197\n",
      "Epoch 7, Val Loss: 1.2182\n",
      "Epoch: 8, training loss: 1.1995\n",
      "Epoch 8, Val Loss: 1.2042\n",
      "Epoch: 9, training loss: 1.1895\n",
      "Epoch 9, Val Loss: 1.1905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-07 19:18:13,930] Trial 5 finished with value: 0.21895295961700384 and parameters: {'hidden_dim': 29, 'dropout': 0.4999581745237154, 'initial_lr': 1.1808467172044521e-05, 'max_lr': 0.007194212977741185, 'criterion': 'BCEWithLogitsLoss', 'pos_weight': 10}. Best is trial 4 with value: 0.28553544865251146.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, training loss: 1.1816\n",
      "Epoch 10, Val Loss: 1.1802\n",
      "Using device: cuda\n",
      "Fold 1:\n",
      "Epoch: 1, training loss: 0.8450\n",
      "Epoch 1, Val Loss: 0.7819\n",
      "Epoch: 2, training loss: 0.7780\n",
      "Epoch 2, Val Loss: 0.7423\n",
      "Epoch: 3, training loss: 0.7548\n",
      "Epoch 3, Val Loss: 0.7150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [07/Apr/2025 19:18:14] \"GET /api/studies/0?after=4 HTTP/1.1\" 200 9065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, training loss: 0.7418\n",
      "Epoch 4, Val Loss: 0.7017\n",
      "Epoch: 5, training loss: 0.7204\n",
      "Epoch 5, Val Loss: 0.6900\n",
      "Epoch: 6, training loss: 0.7149\n",
      "Epoch 6, Val Loss: 0.6866\n",
      "Epoch: 7, training loss: 0.7001\n",
      "Epoch 7, Val Loss: 0.6876\n",
      "Epoch: 8, training loss: 0.7106\n",
      "Epoch 8, Val Loss: 0.6791\n",
      "Epoch: 9, training loss: 0.7088\n",
      "Epoch 9, Val Loss: 0.6790\n",
      "Epoch: 10, training loss: 0.7180\n",
      "Epoch 10, Val Loss: 0.7008\n",
      "Fold 2:\n",
      "Epoch: 1, training loss: 0.8348\n",
      "Epoch 1, Val Loss: 0.7739\n",
      "Epoch: 2, training loss: 0.7724\n",
      "Epoch 2, Val Loss: 0.7401\n",
      "Epoch: 3, training loss: 0.7411\n",
      "Epoch 3, Val Loss: 0.7217\n",
      "Epoch: 4, training loss: 0.7294\n",
      "Epoch 4, Val Loss: 0.7151\n",
      "Epoch: 5, training loss: 0.7154\n",
      "Epoch 5, Val Loss: 0.7072\n",
      "Epoch: 6, training loss: 0.7062\n",
      "Epoch 6, Val Loss: 0.7043\n",
      "Epoch: 7, training loss: 0.7062\n",
      "Epoch 7, Val Loss: 0.7041\n",
      "Epoch: 8, training loss: 0.6992\n",
      "Epoch 8, Val Loss: 0.6984\n",
      "Epoch: 9, training loss: 0.6987\n",
      "Epoch 9, Val Loss: 0.6942\n",
      "Epoch: 10, training loss: 0.7000\n",
      "Epoch 10, Val Loss: 0.7101\n",
      "Fold 3:\n",
      "Epoch: 1, training loss: 0.8458\n",
      "Epoch 1, Val Loss: 0.7887\n",
      "Epoch: 2, training loss: 0.7854\n",
      "Epoch 2, Val Loss: 0.7606\n",
      "Epoch: 3, training loss: 0.7598\n",
      "Epoch 3, Val Loss: 0.7409\n",
      "Epoch: 4, training loss: 0.7272\n",
      "Epoch 4, Val Loss: 0.7295\n",
      "Epoch: 5, training loss: 0.7091\n",
      "Epoch 5, Val Loss: 0.7165\n",
      "Epoch: 6, training loss: 0.7098\n",
      "Epoch 6, Val Loss: 0.7148\n",
      "Epoch: 7, training loss: 0.6851\n",
      "Epoch 7, Val Loss: 0.7385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\optuna_dashboard\\_importance.py:96: ExperimentalWarning: PedAnovaImportanceEvaluator is experimental (supported from v3.6.0). The interface can change in the future.\n",
      "  study, target=target, evaluator=PedAnovaImportanceEvaluator()\n",
      "d:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\optuna\\importance\\_ped_anova\\evaluator.py:150: UserWarning: PedAnovaImportanceEvaluator computes the importances of params to achieve low `target` values. If this is not what you want, please modify target, e.g., by multiplying the output by -1.\n",
      "  warnings.warn(\n",
      "127.0.0.1 - - [07/Apr/2025 19:18:17] \"GET /api/studies/0/param_importances?evaluator=ped_anova HTTP/1.1\" 200 579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, training loss: 0.7126\n",
      "Epoch 8, Val Loss: 0.7264\n",
      "Epoch: 9, training loss: 0.7002\n",
      "Epoch 9, Val Loss: 0.7109\n",
      "Epoch: 10, training loss: 0.6891\n",
      "Epoch 10, Val Loss: 0.7116\n",
      "Fold 4:\n",
      "Epoch: 1, training loss: 0.8480\n",
      "Epoch 1, Val Loss: 0.7914\n",
      "Epoch: 2, training loss: 0.7782\n",
      "Epoch 2, Val Loss: 0.7508\n",
      "Epoch: 3, training loss: 0.7467\n",
      "Epoch 3, Val Loss: 0.7197\n",
      "Epoch: 4, training loss: 0.7296\n",
      "Epoch 4, Val Loss: 0.7143\n",
      "Epoch: 5, training loss: 0.7149\n",
      "Epoch 5, Val Loss: 0.6979\n",
      "Epoch: 6, training loss: 0.7045\n",
      "Epoch 6, Val Loss: 0.7038\n",
      "Epoch: 7, training loss: 0.6999\n",
      "Epoch 7, Val Loss: 0.6969\n",
      "Epoch: 8, training loss: 0.6974\n",
      "Epoch 8, Val Loss: 0.6916\n",
      "Epoch: 9, training loss: 0.7017\n",
      "Epoch 9, Val Loss: 0.6933\n",
      "Epoch: 10, training loss: 0.7025\n",
      "Epoch 10, Val Loss: 0.7023\n",
      "Fold 5:\n",
      "Epoch: 1, training loss: 0.8311\n",
      "Epoch 1, Val Loss: 0.7836\n",
      "Epoch: 2, training loss: 0.7798\n",
      "Epoch 2, Val Loss: 0.7606\n",
      "Epoch: 3, training loss: 0.7522\n",
      "Epoch 3, Val Loss: 0.7417\n",
      "Epoch: 4, training loss: 0.7147\n",
      "Epoch 4, Val Loss: 0.7238\n",
      "Epoch: 5, training loss: 0.7093\n",
      "Epoch 5, Val Loss: 0.7210\n",
      "Epoch: 6, training loss: 0.7045\n",
      "Epoch 6, Val Loss: 0.7353\n",
      "Epoch: 7, training loss: 0.6997\n",
      "Epoch 7, Val Loss: 0.7115\n",
      "Epoch: 8, training loss: 0.7046\n",
      "Epoch 8, Val Loss: 0.7206\n",
      "Epoch: 9, training loss: 0.6925\n",
      "Epoch 9, Val Loss: 0.7170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-07 19:18:20,658] Trial 6 finished with value: 0.3047660563977198 and parameters: {'hidden_dim': 32, 'dropout': 0.42398275351152914, 'initial_lr': 0.0005626062589890148, 'max_lr': 0.02955034559460314, 'criterion': 'BCEWithLogitsLoss', 'pos_weight': 4}. Best is trial 6 with value: 0.3047660563977198.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, training loss: 0.7003\n",
      "Epoch 10, Val Loss: 0.7155\n",
      "Using device: cuda\n",
      "Fold 1:\n",
      "Epoch: 1, training loss: 0.8661\n",
      "Epoch 1, Val Loss: 0.8284\n",
      "Epoch: 2, training loss: 0.8124\n",
      "Epoch 2, Val Loss: 0.7966\n",
      "Epoch: 3, training loss: 0.7943\n",
      "Epoch 3, Val Loss: 0.7827\n",
      "Epoch: 4, training loss: 0.7847\n",
      "Epoch 4, Val Loss: 0.7682\n",
      "Epoch: 5, training loss: 0.7689\n",
      "Epoch 5, Val Loss: 0.7532\n",
      "Epoch: 6, training loss: 0.7556\n",
      "Epoch 6, Val Loss: 0.7378\n",
      "Epoch: 7, training loss: 0.7409\n",
      "Epoch 7, Val Loss: 0.7234\n",
      "Epoch: 8, training loss: 0.7283\n",
      "Epoch 8, Val Loss: 0.7140\n",
      "Epoch: 9, training loss: 0.7195\n",
      "Epoch 9, Val Loss: 0.7006\n",
      "Epoch: 10, training loss: 0.7068\n",
      "Epoch 10, Val Loss: 0.6928\n",
      "Fold 2:\n",
      "Epoch: 1, training loss: 0.8337\n",
      "Epoch 1, Val Loss: 0.8127\n",
      "Epoch: 2, training loss: 0.7981\n",
      "Epoch 2, Val Loss: 0.7909\n",
      "Epoch: 3, training loss: 0.7804\n",
      "Epoch 3, Val Loss: 0.7778\n",
      "Epoch: 4, training loss: 0.7685\n",
      "Epoch 4, Val Loss: 0.7637\n",
      "Epoch: 5, training loss: 0.7520\n",
      "Epoch 5, Val Loss: 0.7514\n",
      "Epoch: 6, training loss: 0.7347\n",
      "Epoch 6, Val Loss: 0.7384\n",
      "Epoch: 7, training loss: 0.7243\n",
      "Epoch 7, Val Loss: 0.7270\n",
      "Epoch: 8, training loss: 0.7080\n",
      "Epoch 8, Val Loss: 0.7184\n",
      "Epoch: 9, training loss: 0.6999\n",
      "Epoch 9, Val Loss: 0.7100\n",
      "Epoch: 10, training loss: 0.6933\n",
      "Epoch 10, Val Loss: 0.7113\n",
      "Fold 3:\n",
      "Epoch: 1, training loss: 0.8599\n",
      "Epoch 1, Val Loss: 0.8359\n",
      "Epoch: 2, training loss: 0.8199\n",
      "Epoch 2, Val Loss: 0.8054\n",
      "Epoch: 3, training loss: 0.7965\n",
      "Epoch 3, Val Loss: 0.7893\n",
      "Epoch: 4, training loss: 0.7796\n",
      "Epoch 4, Val Loss: 0.7745\n",
      "Epoch: 5, training loss: 0.7625\n",
      "Epoch 5, Val Loss: 0.7594\n",
      "Epoch: 6, training loss: 0.7448\n",
      "Epoch 6, Val Loss: 0.7451\n",
      "Epoch: 7, training loss: 0.7250\n",
      "Epoch 7, Val Loss: 0.7340\n",
      "Epoch: 8, training loss: 0.7123\n",
      "Epoch 8, Val Loss: 0.7240\n",
      "Epoch: 9, training loss: 0.6999\n",
      "Epoch 9, Val Loss: 0.7183\n",
      "Epoch: 10, training loss: 0.6940\n",
      "Epoch 10, Val Loss: 0.7115\n",
      "Fold 4:\n",
      "Epoch: 1, training loss: 0.8468\n",
      "Epoch 1, Val Loss: 0.8214\n",
      "Epoch: 2, training loss: 0.8124\n",
      "Epoch 2, Val Loss: 0.7981\n",
      "Epoch: 3, training loss: 0.7936\n",
      "Epoch 3, Val Loss: 0.7833\n",
      "Epoch: 4, training loss: 0.7789\n",
      "Epoch 4, Val Loss: 0.7689\n",
      "Epoch: 5, training loss: 0.7638\n",
      "Epoch 5, Val Loss: 0.7529\n",
      "Epoch: 6, training loss: 0.7478\n",
      "Epoch 6, Val Loss: 0.7386\n",
      "Epoch: 7, training loss: 0.7294\n",
      "Epoch 7, Val Loss: 0.7263\n",
      "Epoch: 8, training loss: 0.7190\n",
      "Epoch 8, Val Loss: 0.7136\n",
      "Epoch: 9, training loss: 0.7051\n",
      "Epoch 9, Val Loss: 0.7053\n",
      "Epoch: 10, training loss: 0.7009\n",
      "Epoch 10, Val Loss: 0.6995\n",
      "Fold 5:\n",
      "Epoch: 1, training loss: 0.8737\n",
      "Epoch 1, Val Loss: 0.8403\n",
      "Epoch: 2, training loss: 0.8219\n",
      "Epoch 2, Val Loss: 0.8049\n",
      "Epoch: 3, training loss: 0.7943\n",
      "Epoch 3, Val Loss: 0.7917\n",
      "Epoch: 4, training loss: 0.7786\n",
      "Epoch 4, Val Loss: 0.7813\n",
      "Epoch: 5, training loss: 0.7671\n",
      "Epoch 5, Val Loss: 0.7710\n",
      "Epoch: 6, training loss: 0.7523\n",
      "Epoch 6, Val Loss: 0.7616\n",
      "Epoch: 7, training loss: 0.7399\n",
      "Epoch 7, Val Loss: 0.7497\n",
      "Epoch: 8, training loss: 0.7234\n",
      "Epoch 8, Val Loss: 0.7396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-07 19:18:26,054] Trial 7 finished with value: 0.31873696097212745 and parameters: {'hidden_dim': 90, 'dropout': 0.17464675495011817, 'initial_lr': 0.0004484680873713837, 'max_lr': 0.002640413992649716, 'criterion': 'BCEWithLogitsLoss', 'pos_weight': 4}. Best is trial 7 with value: 0.31873696097212745.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, training loss: 0.7174\n",
      "Epoch 9, Val Loss: 0.7312\n",
      "Epoch: 10, training loss: 0.7047\n",
      "Epoch 10, Val Loss: 0.7242\n",
      "Using device: cuda\n",
      "Fold 1:\n",
      "Epoch: 1, training loss: 1.2954\n",
      "Epoch 1, Val Loss: 1.2428\n",
      "Epoch: 2, training loss: 1.2297\n",
      "Epoch 2, Val Loss: 1.1655\n",
      "Epoch: 3, training loss: 1.1850\n",
      "Epoch 3, Val Loss: 1.1253\n",
      "Epoch: 4, training loss: 1.1680\n",
      "Epoch 4, Val Loss: 1.1445\n",
      "Epoch: 5, training loss: 1.1481\n",
      "Epoch 5, Val Loss: 1.0946\n",
      "Epoch: 6, training loss: 1.1538\n",
      "Epoch 6, Val Loss: 1.1048\n",
      "Epoch: 7, training loss: 1.1531\n",
      "Epoch 7, Val Loss: 1.1050\n",
      "Epoch: 8, training loss: 1.1250\n",
      "Epoch 8, Val Loss: 1.0878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [07/Apr/2025 19:18:27] \"GET /api/studies/0?after=6 HTTP/1.1\" 200 9068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, training loss: 1.1387\n",
      "Epoch 9, Val Loss: 1.0968\n",
      "Epoch: 10, training loss: 1.1244\n",
      "Epoch 10, Val Loss: 1.0913\n",
      "Fold 2:\n",
      "Epoch: 1, training loss: 1.3008\n",
      "Epoch 1, Val Loss: 1.2599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\optuna_dashboard\\_importance.py:96: ExperimentalWarning: PedAnovaImportanceEvaluator is experimental (supported from v3.6.0). The interface can change in the future.\n",
      "  study, target=target, evaluator=PedAnovaImportanceEvaluator()\n",
      "d:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\optuna\\importance\\_ped_anova\\evaluator.py:150: UserWarning: PedAnovaImportanceEvaluator computes the importances of params to achieve low `target` values. If this is not what you want, please modify target, e.g., by multiplying the output by -1.\n",
      "  warnings.warn(\n",
      "127.0.0.1 - - [07/Apr/2025 19:18:27] \"GET /api/studies/0/param_importances?evaluator=ped_anova HTTP/1.1\" 200 581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, training loss: 1.2281\n",
      "Epoch 2, Val Loss: 1.2027\n",
      "Epoch: 3, training loss: 1.1697\n",
      "Epoch 3, Val Loss: 1.1672\n",
      "Epoch: 4, training loss: 1.1529\n",
      "Epoch 4, Val Loss: 1.1389\n",
      "Epoch: 5, training loss: 1.1305\n",
      "Epoch 5, Val Loss: 1.1289\n",
      "Epoch: 6, training loss: 1.1284\n",
      "Epoch 6, Val Loss: 1.1369\n",
      "Epoch: 7, training loss: 1.1166\n",
      "Epoch 7, Val Loss: 1.1247\n",
      "Epoch: 8, training loss: 1.1185\n",
      "Epoch 8, Val Loss: 1.1275\n",
      "Epoch: 9, training loss: 1.1169\n",
      "Epoch 9, Val Loss: 1.1247\n",
      "Epoch: 10, training loss: 1.1127\n",
      "Epoch 10, Val Loss: 1.1282\n",
      "Fold 3:\n",
      "Epoch: 1, training loss: 1.3054\n",
      "Epoch 1, Val Loss: 1.2734\n",
      "Epoch: 2, training loss: 1.2293\n",
      "Epoch 2, Val Loss: 1.2182\n",
      "Epoch: 3, training loss: 1.1818\n",
      "Epoch 3, Val Loss: 1.2086\n",
      "Epoch: 4, training loss: 1.1636\n",
      "Epoch 4, Val Loss: 1.1734\n",
      "Epoch: 5, training loss: 1.1383\n",
      "Epoch 5, Val Loss: 1.1637\n",
      "Epoch: 6, training loss: 1.1171\n",
      "Epoch 6, Val Loss: 1.1691\n",
      "Epoch: 7, training loss: 1.1239\n",
      "Epoch 7, Val Loss: 1.1771\n",
      "Epoch: 8, training loss: 1.1190\n",
      "Epoch 8, Val Loss: 1.1595\n",
      "Epoch: 9, training loss: 1.1044\n",
      "Epoch 9, Val Loss: 1.1652\n",
      "Epoch: 10, training loss: 1.0999\n",
      "Epoch 10, Val Loss: 1.1877\n",
      "Fold 4:\n",
      "Epoch: 1, training loss: 1.3185\n",
      "Epoch 1, Val Loss: 1.2861\n",
      "Epoch: 2, training loss: 1.2534\n",
      "Epoch 2, Val Loss: 1.2185\n",
      "Epoch: 3, training loss: 1.1991\n",
      "Epoch 3, Val Loss: 1.1656\n",
      "Epoch: 4, training loss: 1.1551\n",
      "Epoch 4, Val Loss: 1.1451\n",
      "Epoch: 5, training loss: 1.1519\n",
      "Epoch 5, Val Loss: 1.1330\n",
      "Epoch: 6, training loss: 1.1374\n",
      "Epoch 6, Val Loss: 1.1389\n",
      "Epoch: 7, training loss: 1.1319\n",
      "Epoch 7, Val Loss: 1.1372\n",
      "Epoch: 8, training loss: 1.1223\n",
      "Epoch 8, Val Loss: 1.1385\n",
      "Epoch: 9, training loss: 1.1234\n",
      "Epoch 9, Val Loss: 1.1471\n",
      "Epoch: 10, training loss: 1.1258\n",
      "Epoch 10, Val Loss: 1.1493\n",
      "Fold 5:\n",
      "Epoch: 1, training loss: 1.3179\n",
      "Epoch 1, Val Loss: 1.2865\n",
      "Epoch: 2, training loss: 1.2522\n",
      "Epoch 2, Val Loss: 1.2371\n",
      "Epoch: 3, training loss: 1.2030\n",
      "Epoch 3, Val Loss: 1.1945\n",
      "Epoch: 4, training loss: 1.1703\n",
      "Epoch 4, Val Loss: 1.1801\n",
      "Epoch: 5, training loss: 1.1412\n",
      "Epoch 5, Val Loss: 1.1486\n",
      "Epoch: 6, training loss: 1.1303\n",
      "Epoch 6, Val Loss: 1.1766\n",
      "Epoch: 7, training loss: 1.1261\n",
      "Epoch 7, Val Loss: 1.1562\n",
      "Epoch: 8, training loss: 1.1214\n",
      "Epoch 8, Val Loss: 1.1583\n",
      "Epoch: 9, training loss: 1.1346\n",
      "Epoch 9, Val Loss: 1.1578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-07 19:18:32,821] Trial 8 finished with value: 0.22194384551959612 and parameters: {'hidden_dim': 69, 'dropout': 0.35478846240186046, 'initial_lr': 0.00029189192061565843, 'max_lr': 0.01981549218714277, 'criterion': 'BCEWithLogitsLoss', 'pos_weight': 10}. Best is trial 7 with value: 0.31873696097212745.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, training loss: 1.1267\n",
      "Epoch 10, Val Loss: 1.1608\n",
      "Using device: cuda\n",
      "Fold 1:\n",
      "Epoch: 1, training loss: 0.8466\n",
      "Epoch 1, Val Loss: 0.8081\n",
      "Epoch: 2, training loss: 0.8052\n",
      "Epoch 2, Val Loss: 0.7772\n",
      "Epoch: 3, training loss: 0.7769\n",
      "Epoch 3, Val Loss: 0.7457\n",
      "Epoch: 4, training loss: 0.7476\n",
      "Epoch 4, Val Loss: 0.7201\n",
      "Epoch: 5, training loss: 0.7232\n",
      "Epoch 5, Val Loss: 0.6974\n",
      "Epoch: 6, training loss: 0.7094\n",
      "Epoch 6, Val Loss: 0.6841\n",
      "Epoch: 7, training loss: 0.7119\n",
      "Epoch 7, Val Loss: 0.6853\n",
      "Epoch: 8, training loss: 0.6961\n",
      "Epoch 8, Val Loss: 0.6876\n",
      "Epoch: 9, training loss: 0.6979\n",
      "Epoch 9, Val Loss: 0.6805\n",
      "Epoch: 10, training loss: 0.6946\n",
      "Epoch 10, Val Loss: 0.6746\n",
      "Fold 2:\n",
      "Epoch: 1, training loss: 0.8541\n",
      "Epoch 1, Val Loss: 0.7947\n",
      "Epoch: 2, training loss: 0.7892\n",
      "Epoch 2, Val Loss: 0.7683\n",
      "Epoch: 3, training loss: 0.7572\n",
      "Epoch 3, Val Loss: 0.7431\n",
      "Epoch: 4, training loss: 0.7429\n",
      "Epoch 4, Val Loss: 0.7260\n",
      "Epoch: 5, training loss: 0.7207\n",
      "Epoch 5, Val Loss: 0.7130\n",
      "Epoch: 6, training loss: 0.7067\n",
      "Epoch 6, Val Loss: 0.7017\n",
      "Epoch: 7, training loss: 0.6982\n",
      "Epoch 7, Val Loss: 0.7005\n",
      "Epoch: 8, training loss: 0.7014\n",
      "Epoch 8, Val Loss: 0.6986\n",
      "Epoch: 9, training loss: 0.6917\n",
      "Epoch 9, Val Loss: 0.6998\n",
      "Epoch: 10, training loss: 0.6930\n",
      "Epoch 10, Val Loss: 0.6973\n",
      "Fold 3:\n",
      "Epoch: 1, training loss: 0.8429\n",
      "Epoch 1, Val Loss: 0.7974\n",
      "Epoch: 2, training loss: 0.7844\n",
      "Epoch 2, Val Loss: 0.7708\n",
      "Epoch: 3, training loss: 0.7644\n",
      "Epoch 3, Val Loss: 0.7509\n",
      "Epoch: 4, training loss: 0.7336\n",
      "Epoch 4, Val Loss: 0.7337\n",
      "Epoch: 5, training loss: 0.7125\n",
      "Epoch 5, Val Loss: 0.7193\n",
      "Epoch: 6, training loss: 0.7066\n",
      "Epoch 6, Val Loss: 0.7176\n",
      "Epoch: 7, training loss: 0.7027\n",
      "Epoch 7, Val Loss: 0.7168\n",
      "Epoch: 8, training loss: 0.6838\n",
      "Epoch 8, Val Loss: 0.7146\n",
      "Epoch: 9, training loss: 0.6870\n",
      "Epoch 9, Val Loss: 0.7113\n",
      "Epoch: 10, training loss: 0.6828\n",
      "Epoch 10, Val Loss: 0.7080\n",
      "Fold 4:\n",
      "Epoch: 1, training loss: 0.8209\n",
      "Epoch 1, Val Loss: 0.7891\n",
      "Epoch: 2, training loss: 0.7885\n",
      "Epoch 2, Val Loss: 0.7649\n",
      "Epoch: 3, training loss: 0.7677\n",
      "Epoch 3, Val Loss: 0.7462\n",
      "Epoch: 4, training loss: 0.7370\n",
      "Epoch 4, Val Loss: 0.7216\n",
      "Epoch: 5, training loss: 0.7193\n",
      "Epoch 5, Val Loss: 0.7075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [07/Apr/2025 19:18:37] \"GET /api/studies/0?after=8 HTTP/1.1\" 200 7249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, training loss: 0.7168\n",
      "Epoch 6, Val Loss: 0.6990\n",
      "Epoch: 7, training loss: 0.7103\n",
      "Epoch 7, Val Loss: 0.6936\n",
      "Epoch: 8, training loss: 0.7074\n",
      "Epoch 8, Val Loss: 0.6951\n",
      "Epoch: 9, training loss: 0.6991\n",
      "Epoch 9, Val Loss: 0.6935\n",
      "Epoch: 10, training loss: 0.6978\n",
      "Epoch 10, Val Loss: 0.6965\n",
      "Fold 5:\n",
      "Epoch: 1, training loss: 0.8127\n",
      "Epoch 1, Val Loss: 0.7808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\optuna_dashboard\\_importance.py:96: ExperimentalWarning: PedAnovaImportanceEvaluator is experimental (supported from v3.6.0). The interface can change in the future.\n",
      "  study, target=target, evaluator=PedAnovaImportanceEvaluator()\n",
      "d:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\optuna\\importance\\_ped_anova\\evaluator.py:150: UserWarning: PedAnovaImportanceEvaluator computes the importances of params to achieve low `target` values. If this is not what you want, please modify target, e.g., by multiplying the output by -1.\n",
      "  warnings.warn(\n",
      "127.0.0.1 - - [07/Apr/2025 19:18:37] \"GET /api/studies/0/param_importances?evaluator=ped_anova HTTP/1.1\" 200 578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, training loss: 0.7840\n",
      "Epoch 2, Val Loss: 0.7626\n",
      "Epoch: 3, training loss: 0.7560\n",
      "Epoch 3, Val Loss: 0.7464\n",
      "Epoch: 4, training loss: 0.7382\n",
      "Epoch 4, Val Loss: 0.7286\n",
      "Epoch: 5, training loss: 0.7236\n",
      "Epoch 5, Val Loss: 0.7176\n",
      "Epoch: 6, training loss: 0.7107\n",
      "Epoch 6, Val Loss: 0.7164\n",
      "Epoch: 7, training loss: 0.7034\n",
      "Epoch 7, Val Loss: 0.7061\n",
      "Epoch: 8, training loss: 0.6935\n",
      "Epoch 8, Val Loss: 0.7045\n",
      "Epoch: 9, training loss: 0.6949\n",
      "Epoch 9, Val Loss: 0.7072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-07 19:18:38,666] Trial 9 finished with value: 0.2833931964497351 and parameters: {'hidden_dim': 95, 'dropout': 0.4314866232179114, 'initial_lr': 0.00029122986278302807, 'max_lr': 0.008831488363882773, 'criterion': 'BCEWithLogitsLoss', 'pos_weight': 4}. Best is trial 7 with value: 0.31873696097212745.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, training loss: 0.6948\n",
      "Epoch 10, Val Loss: 0.7025\n",
      "Using device: cuda\n",
      "Fold 1:\n",
      "Epoch: 1, training loss: 0.6064\n",
      "Epoch 1, Val Loss: 0.5750\n",
      "Epoch: 2, training loss: 0.5429\n",
      "Epoch 2, Val Loss: 0.5054\n",
      "Epoch: 3, training loss: 0.4709\n",
      "Epoch 3, Val Loss: 0.4364\n",
      "Epoch: 4, training loss: 0.4076\n",
      "Epoch 4, Val Loss: 0.3814\n",
      "Epoch: 5, training loss: 0.3633\n",
      "Epoch 5, Val Loss: 0.3486\n",
      "Epoch: 6, training loss: 0.3413\n",
      "Epoch 6, Val Loss: 0.3352\n",
      "Epoch: 7, training loss: 0.3317\n",
      "Epoch 7, Val Loss: 0.3299\n",
      "Epoch: 8, training loss: 0.3302\n",
      "Epoch 8, Val Loss: 0.3266\n",
      "Epoch: 9, training loss: 0.3259\n",
      "Epoch 9, Val Loss: 0.3232\n",
      "Epoch: 10, training loss: 0.3217\n",
      "Epoch 10, Val Loss: 0.3197\n",
      "Fold 2:\n",
      "Epoch: 1, training loss: 0.6064\n",
      "Epoch 1, Val Loss: 0.5699\n",
      "Epoch: 2, training loss: 0.5401\n",
      "Epoch 2, Val Loss: 0.4982\n",
      "Epoch: 3, training loss: 0.4669\n",
      "Epoch 3, Val Loss: 0.4244\n",
      "Epoch: 4, training loss: 0.3986\n",
      "Epoch 4, Val Loss: 0.3676\n",
      "Epoch: 5, training loss: 0.3540\n",
      "Epoch 5, Val Loss: 0.3399\n",
      "Epoch: 6, training loss: 0.3385\n",
      "Epoch 6, Val Loss: 0.3311\n",
      "Epoch: 7, training loss: 0.3289\n",
      "Epoch 7, Val Loss: 0.3277\n",
      "Epoch: 8, training loss: 0.3282\n",
      "Epoch 8, Val Loss: 0.3254\n",
      "Epoch: 9, training loss: 0.3237\n",
      "Epoch 9, Val Loss: 0.3232\n",
      "Epoch: 10, training loss: 0.3222\n",
      "Epoch 10, Val Loss: 0.3209\n",
      "Fold 3:\n",
      "Epoch: 1, training loss: 0.7155\n",
      "Epoch 1, Val Loss: 0.6833\n",
      "Epoch: 2, training loss: 0.6526\n",
      "Epoch 2, Val Loss: 0.6107\n",
      "Epoch: 3, training loss: 0.5751\n",
      "Epoch 3, Val Loss: 0.5290\n",
      "Epoch: 4, training loss: 0.4923\n",
      "Epoch 4, Val Loss: 0.4442\n",
      "Epoch: 5, training loss: 0.4151\n",
      "Epoch 5, Val Loss: 0.3762\n",
      "Epoch: 6, training loss: 0.3636\n",
      "Epoch 6, Val Loss: 0.3398\n",
      "Epoch: 7, training loss: 0.3400\n",
      "Epoch 7, Val Loss: 0.3279\n",
      "Epoch: 8, training loss: 0.3335\n",
      "Epoch 8, Val Loss: 0.3238\n",
      "Epoch: 9, training loss: 0.3287\n",
      "Epoch 9, Val Loss: 0.3216\n",
      "Epoch: 10, training loss: 0.3279\n",
      "Epoch 10, Val Loss: 0.3197\n",
      "Fold 4:\n",
      "Epoch: 1, training loss: 0.5968\n",
      "Epoch 1, Val Loss: 0.5645\n",
      "Epoch: 2, training loss: 0.5309\n",
      "Epoch 2, Val Loss: 0.4915\n",
      "Epoch: 3, training loss: 0.4581\n",
      "Epoch 3, Val Loss: 0.4212\n",
      "Epoch: 4, training loss: 0.3956\n",
      "Epoch 4, Val Loss: 0.3683\n",
      "Epoch: 5, training loss: 0.3534\n",
      "Epoch 5, Val Loss: 0.3396\n",
      "Epoch: 6, training loss: 0.3352\n",
      "Epoch 6, Val Loss: 0.3288\n",
      "Epoch: 7, training loss: 0.3276\n",
      "Epoch 7, Val Loss: 0.3252\n",
      "Epoch: 8, training loss: 0.3232\n",
      "Epoch 8, Val Loss: 0.3229\n",
      "Epoch: 9, training loss: 0.3237\n",
      "Epoch 9, Val Loss: 0.3205\n",
      "Epoch: 10, training loss: 0.3224\n",
      "Epoch 10, Val Loss: 0.3180\n",
      "Fold 5:\n",
      "Epoch: 1, training loss: 0.6975\n",
      "Epoch 1, Val Loss: 0.6654\n",
      "Epoch: 2, training loss: 0.6236\n",
      "Epoch 2, Val Loss: 0.5807\n",
      "Epoch: 3, training loss: 0.5347\n",
      "Epoch 3, Val Loss: 0.4877\n",
      "Epoch: 4, training loss: 0.4438\n",
      "Epoch 4, Val Loss: 0.4052\n",
      "Epoch: 5, training loss: 0.3775\n",
      "Epoch 5, Val Loss: 0.3556\n",
      "Epoch: 6, training loss: 0.3442\n",
      "Epoch 6, Val Loss: 0.3375\n",
      "Epoch: 7, training loss: 0.3334\n",
      "Epoch 7, Val Loss: 0.3315\n",
      "Epoch: 8, training loss: 0.3275\n",
      "Epoch 8, Val Loss: 0.3280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-07 19:18:45,182] Trial 10 finished with value: 0.37710130097009875 and parameters: {'hidden_dim': 62, 'dropout': 0.12002536318431695, 'initial_lr': 8.045048412301608e-05, 'max_lr': 0.001099307168188595, 'criterion': 'BCEWithLogitsLoss', 'pos_weight': 1}. Best is trial 10 with value: 0.37710130097009875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, training loss: 0.3239\n",
      "Epoch 9, Val Loss: 0.3243\n",
      "Epoch: 10, training loss: 0.3204\n",
      "Epoch 10, Val Loss: 0.3213\n",
      "Using device: cuda\n",
      "Fold 1:\n",
      "Epoch: 1, training loss: 0.8501\n",
      "Epoch 1, Val Loss: 0.8154\n",
      "Epoch: 2, training loss: 0.7741\n",
      "Epoch 2, Val Loss: 0.7280\n",
      "Epoch: 3, training loss: 0.6792\n",
      "Epoch 3, Val Loss: 0.6261\n",
      "Epoch: 4, training loss: 0.5705\n",
      "Epoch 4, Val Loss: 0.5117\n",
      "Epoch: 5, training loss: 0.4599\n",
      "Epoch 5, Val Loss: 0.4091\n",
      "Epoch: 6, training loss: 0.3795\n",
      "Epoch 6, Val Loss: 0.3522\n",
      "Epoch: 7, training loss: 0.3437\n",
      "Epoch 7, Val Loss: 0.3329\n",
      "Epoch: 8, training loss: 0.3318\n",
      "Epoch 8, Val Loss: 0.3280\n",
      "Epoch: 9, training loss: 0.3299\n",
      "Epoch 9, Val Loss: 0.3256\n",
      "Epoch: 10, training loss: 0.3272\n",
      "Epoch 10, Val Loss: 0.3232\n",
      "Fold 2:\n",
      "Epoch: 1, training loss: 0.7005\n",
      "Epoch 1, Val Loss: 0.6693\n",
      "Epoch: 2, training loss: 0.6332\n",
      "Epoch 2, Val Loss: 0.5943\n",
      "Epoch: 3, training loss: 0.5523\n",
      "Epoch 3, Val Loss: 0.5105\n",
      "Epoch: 4, training loss: 0.4688\n",
      "Epoch 4, Val Loss: 0.4327\n",
      "Epoch: 5, training loss: 0.3980\n",
      "Epoch 5, Val Loss: 0.3737\n",
      "Epoch: 6, training loss: 0.3528\n",
      "Epoch 6, Val Loss: 0.3440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [07/Apr/2025 19:18:47] \"GET /api/studies/0?after=9 HTTP/1.1\" 200 9076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, training loss: 0.3376\n",
      "Epoch 7, Val Loss: 0.3339\n",
      "Epoch: 8, training loss: 0.3302\n",
      "Epoch 8, Val Loss: 0.3294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\optuna_dashboard\\_importance.py:96: ExperimentalWarning: PedAnovaImportanceEvaluator is experimental (supported from v3.6.0). The interface can change in the future.\n",
      "  study, target=target, evaluator=PedAnovaImportanceEvaluator()\n",
      "d:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\optuna\\importance\\_ped_anova\\evaluator.py:150: UserWarning: PedAnovaImportanceEvaluator computes the importances of params to achieve low `target` values. If this is not what you want, please modify target, e.g., by multiplying the output by -1.\n",
      "  warnings.warn(\n",
      "127.0.0.1 - - [07/Apr/2025 19:18:47] \"GET /api/studies/0/param_importances?evaluator=ped_anova HTTP/1.1\" 200 580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, training loss: 0.3253\n",
      "Epoch 9, Val Loss: 0.3260\n",
      "Epoch: 10, training loss: 0.3229\n",
      "Epoch 10, Val Loss: 0.3227\n",
      "Fold 3:\n",
      "Epoch: 1, training loss: 0.7063\n",
      "Epoch 1, Val Loss: 0.6731\n",
      "Epoch: 2, training loss: 0.6363\n",
      "Epoch 2, Val Loss: 0.5938\n",
      "Epoch: 3, training loss: 0.5540\n",
      "Epoch 3, Val Loss: 0.5080\n",
      "Epoch: 4, training loss: 0.4707\n",
      "Epoch 4, Val Loss: 0.4307\n",
      "Epoch: 5, training loss: 0.4023\n",
      "Epoch 5, Val Loss: 0.3741\n",
      "Epoch: 6, training loss: 0.3604\n",
      "Epoch 6, Val Loss: 0.3445\n",
      "Epoch: 7, training loss: 0.3394\n",
      "Epoch 7, Val Loss: 0.3334\n",
      "Epoch: 8, training loss: 0.3328\n",
      "Epoch 8, Val Loss: 0.3295\n",
      "Epoch: 9, training loss: 0.3311\n",
      "Epoch 9, Val Loss: 0.3274\n",
      "Epoch: 10, training loss: 0.3278\n",
      "Epoch 10, Val Loss: 0.3254\n",
      "Fold 4:\n",
      "Epoch: 1, training loss: 0.7350\n",
      "Epoch 1, Val Loss: 0.7018\n",
      "Epoch: 2, training loss: 0.6618\n",
      "Epoch 2, Val Loss: 0.6165\n",
      "Epoch: 3, training loss: 0.5709\n",
      "Epoch 3, Val Loss: 0.5227\n",
      "Epoch: 4, training loss: 0.4798\n",
      "Epoch 4, Val Loss: 0.4353\n",
      "Epoch: 5, training loss: 0.4050\n",
      "Epoch 5, Val Loss: 0.3708\n",
      "Epoch: 6, training loss: 0.3578\n",
      "Epoch 6, Val Loss: 0.3400\n",
      "Epoch: 7, training loss: 0.3398\n",
      "Epoch 7, Val Loss: 0.3311\n",
      "Epoch: 8, training loss: 0.3326\n",
      "Epoch 8, Val Loss: 0.3272\n",
      "Epoch: 9, training loss: 0.3278\n",
      "Epoch 9, Val Loss: 0.3240\n",
      "Epoch: 10, training loss: 0.3250\n",
      "Epoch 10, Val Loss: 0.3208\n",
      "Fold 5:\n",
      "Epoch: 1, training loss: 0.5091\n",
      "Epoch 1, Val Loss: 0.4858\n",
      "Epoch: 2, training loss: 0.4655\n",
      "Epoch 2, Val Loss: 0.4389\n",
      "Epoch: 3, training loss: 0.4193\n",
      "Epoch 3, Val Loss: 0.3943\n",
      "Epoch: 4, training loss: 0.3792\n",
      "Epoch 4, Val Loss: 0.3616\n",
      "Epoch: 5, training loss: 0.3517\n",
      "Epoch 5, Val Loss: 0.3427\n",
      "Epoch: 6, training loss: 0.3365\n",
      "Epoch 6, Val Loss: 0.3350\n",
      "Epoch: 7, training loss: 0.3309\n",
      "Epoch 7, Val Loss: 0.3318\n",
      "Epoch: 8, training loss: 0.3270\n",
      "Epoch 8, Val Loss: 0.3293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-07 19:18:51,747] Trial 11 finished with value: 0.37710130097009875 and parameters: {'hidden_dim': 60, 'dropout': 0.11555981142503965, 'initial_lr': 7.800798928696113e-05, 'max_lr': 0.0010514297753455371, 'criterion': 'BCEWithLogitsLoss', 'pos_weight': 1}. Best is trial 10 with value: 0.37710130097009875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, training loss: 0.3220\n",
      "Epoch 9, Val Loss: 0.3269\n",
      "Epoch: 10, training loss: 0.3191\n",
      "Epoch 10, Val Loss: 0.3244\n",
      "Using device: cuda\n",
      "Fold 1:\n",
      "Epoch: 1, training loss: 0.6986\n",
      "Epoch 1, Val Loss: 0.6548\n",
      "Epoch: 2, training loss: 0.6125\n",
      "Epoch 2, Val Loss: 0.5606\n",
      "Epoch: 3, training loss: 0.5147\n",
      "Epoch 3, Val Loss: 0.4675\n",
      "Epoch: 4, training loss: 0.4313\n",
      "Epoch 4, Val Loss: 0.3946\n",
      "Epoch: 5, training loss: 0.3698\n",
      "Epoch 5, Val Loss: 0.3514\n",
      "Epoch: 6, training loss: 0.3428\n",
      "Epoch 6, Val Loss: 0.3342\n",
      "Epoch: 7, training loss: 0.3326\n",
      "Epoch 7, Val Loss: 0.3288\n",
      "Epoch: 8, training loss: 0.3308\n",
      "Epoch 8, Val Loss: 0.3260\n",
      "Epoch: 9, training loss: 0.3264\n",
      "Epoch 9, Val Loss: 0.3230\n",
      "Epoch: 10, training loss: 0.3255\n",
      "Epoch 10, Val Loss: 0.3202\n",
      "Fold 2:\n",
      "Epoch: 1, training loss: 0.6789\n",
      "Epoch 1, Val Loss: 0.6440\n",
      "Epoch: 2, training loss: 0.6081\n",
      "Epoch 2, Val Loss: 0.5639\n",
      "Epoch: 3, training loss: 0.5213\n",
      "Epoch 3, Val Loss: 0.4756\n",
      "Epoch: 4, training loss: 0.4378\n",
      "Epoch 4, Val Loss: 0.3970\n",
      "Epoch: 5, training loss: 0.3721\n",
      "Epoch 5, Val Loss: 0.3494\n",
      "Epoch: 6, training loss: 0.3436\n",
      "Epoch 6, Val Loss: 0.3326\n",
      "Epoch: 7, training loss: 0.3313\n",
      "Epoch 7, Val Loss: 0.3280\n",
      "Epoch: 8, training loss: 0.3278\n",
      "Epoch 8, Val Loss: 0.3254\n",
      "Epoch: 9, training loss: 0.3245\n",
      "Epoch 9, Val Loss: 0.3228\n",
      "Epoch: 10, training loss: 0.3225\n",
      "Epoch 10, Val Loss: 0.3203\n",
      "Fold 3:\n",
      "Epoch: 1, training loss: 0.5150\n",
      "Epoch 1, Val Loss: 0.4897\n",
      "Epoch: 2, training loss: 0.4685\n",
      "Epoch 2, Val Loss: 0.4388\n",
      "Epoch: 3, training loss: 0.4178\n",
      "Epoch 3, Val Loss: 0.3888\n",
      "Epoch: 4, training loss: 0.3744\n",
      "Epoch 4, Val Loss: 0.3508\n",
      "Epoch: 5, training loss: 0.3424\n",
      "Epoch 5, Val Loss: 0.3319\n",
      "Epoch: 6, training loss: 0.3309\n",
      "Epoch 6, Val Loss: 0.3254\n",
      "Epoch: 7, training loss: 0.3269\n",
      "Epoch 7, Val Loss: 0.3224\n",
      "Epoch: 8, training loss: 0.3233\n",
      "Epoch 8, Val Loss: 0.3201\n",
      "Epoch: 9, training loss: 0.3200\n",
      "Epoch 9, Val Loss: 0.3178\n",
      "Epoch: 10, training loss: 0.3171\n",
      "Epoch 10, Val Loss: 0.3153\n",
      "Fold 4:\n",
      "Epoch: 1, training loss: 0.8271\n",
      "Epoch 1, Val Loss: 0.7825\n",
      "Epoch: 2, training loss: 0.7379\n",
      "Epoch 2, Val Loss: 0.6805\n",
      "Epoch: 3, training loss: 0.6281\n",
      "Epoch 3, Val Loss: 0.5660\n",
      "Epoch: 4, training loss: 0.5113\n",
      "Epoch 4, Val Loss: 0.4510\n",
      "Epoch: 5, training loss: 0.4085\n",
      "Epoch 5, Val Loss: 0.3693\n",
      "Epoch: 6, training loss: 0.3521\n",
      "Epoch 6, Val Loss: 0.3402\n",
      "Epoch: 7, training loss: 0.3362\n",
      "Epoch 7, Val Loss: 0.3334\n",
      "Epoch: 8, training loss: 0.3310\n",
      "Epoch 8, Val Loss: 0.3299\n",
      "Epoch: 9, training loss: 0.3284\n",
      "Epoch 9, Val Loss: 0.3264\n",
      "Epoch: 10, training loss: 0.3268\n",
      "Epoch 10, Val Loss: 0.3231\n",
      "Fold 5:\n",
      "Epoch: 1, training loss: 0.6510\n",
      "Epoch 1, Val Loss: 0.6094\n",
      "Epoch: 2, training loss: 0.5692\n",
      "Epoch 2, Val Loss: 0.5217\n",
      "Epoch: 3, training loss: 0.4823\n",
      "Epoch 3, Val Loss: 0.4390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [07/Apr/2025 19:18:57] \"GET /api/studies/0?after=11 HTTP/1.1\" 200 7257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, training loss: 0.4093\n",
      "Epoch 4, Val Loss: 0.3781\n",
      "Epoch: 5, training loss: 0.3641\n",
      "Epoch 5, Val Loss: 0.3471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\optuna_dashboard\\_importance.py:96: ExperimentalWarning: PedAnovaImportanceEvaluator is experimental (supported from v3.6.0). The interface can change in the future.\n",
      "  study, target=target, evaluator=PedAnovaImportanceEvaluator()\n",
      "d:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\optuna\\importance\\_ped_anova\\evaluator.py:150: UserWarning: PedAnovaImportanceEvaluator computes the importances of params to achieve low `target` values. If this is not what you want, please modify target, e.g., by multiplying the output by -1.\n",
      "  warnings.warn(\n",
      "127.0.0.1 - - [07/Apr/2025 19:18:57] \"GET /api/studies/0/param_importances?evaluator=ped_anova HTTP/1.1\" 200 583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, training loss: 0.3422\n",
      "Epoch 6, Val Loss: 0.3372\n",
      "Epoch: 7, training loss: 0.3352\n",
      "Epoch 7, Val Loss: 0.3337\n",
      "Epoch: 8, training loss: 0.3324\n",
      "Epoch 8, Val Loss: 0.3312\n",
      "Epoch: 9, training loss: 0.3289\n",
      "Epoch 9, Val Loss: 0.3290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-07 19:18:58,441] Trial 12 finished with value: 0.37710130097009875 and parameters: {'hidden_dim': 59, 'dropout': 0.12419320319507288, 'initial_lr': 7.460583464646687e-05, 'max_lr': 0.0011946895719926942, 'criterion': 'BCEWithLogitsLoss', 'pos_weight': 1}. Best is trial 10 with value: 0.37710130097009875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, training loss: 0.3229\n",
      "Epoch 10, Val Loss: 0.3268\n",
      "Using device: cuda\n",
      "Fold 1:\n",
      "Epoch: 1, training loss: 0.3786\n",
      "Epoch 1, Val Loss: 0.3205\n",
      "Epoch: 2, training loss: 0.3136\n",
      "Epoch 2, Val Loss: 0.2988\n",
      "Epoch: 3, training loss: 0.2979\n",
      "Epoch 3, Val Loss: 0.2928\n",
      "Epoch: 4, training loss: 0.2950\n",
      "Epoch 4, Val Loss: 0.2885\n",
      "Epoch: 5, training loss: 0.2925\n",
      "Epoch 5, Val Loss: 0.2852\n",
      "Epoch: 6, training loss: 0.2940\n",
      "Epoch 6, Val Loss: 0.2869\n",
      "Epoch: 7, training loss: 0.2977\n",
      "Epoch 7, Val Loss: 0.2972\n",
      "Epoch: 8, training loss: 0.2976\n",
      "Epoch 8, Val Loss: 0.2878\n",
      "Epoch: 9, training loss: 0.2945\n",
      "Epoch 9, Val Loss: 0.2846\n",
      "Epoch: 10, training loss: 0.2936\n",
      "Epoch 10, Val Loss: 0.2850\n",
      "Fold 2:\n",
      "Epoch: 1, training loss: 0.3689\n",
      "Epoch 1, Val Loss: 0.3222\n",
      "Epoch: 2, training loss: 0.3131\n",
      "Epoch 2, Val Loss: 0.3028\n",
      "Epoch: 3, training loss: 0.2973\n",
      "Epoch 3, Val Loss: 0.2924\n",
      "Epoch: 4, training loss: 0.2933\n",
      "Epoch 4, Val Loss: 0.3028\n",
      "Epoch: 5, training loss: 0.2898\n",
      "Epoch 5, Val Loss: 0.2934\n",
      "Epoch: 6, training loss: 0.2941\n",
      "Epoch 6, Val Loss: 0.2916\n",
      "Epoch: 7, training loss: 0.2923\n",
      "Epoch 7, Val Loss: 0.2989\n",
      "Epoch: 8, training loss: 0.2952\n",
      "Epoch 8, Val Loss: 0.2955\n",
      "Epoch: 9, training loss: 0.2937\n",
      "Epoch 9, Val Loss: 0.2967\n",
      "Epoch: 10, training loss: 0.2905\n",
      "Epoch 10, Val Loss: 0.2992\n",
      "Fold 3:\n",
      "Epoch: 1, training loss: 0.3667\n",
      "Epoch 1, Val Loss: 0.3178\n",
      "Epoch: 2, training loss: 0.3105\n",
      "Epoch 2, Val Loss: 0.3030\n",
      "Epoch: 3, training loss: 0.2965\n",
      "Epoch 3, Val Loss: 0.2985\n",
      "Epoch: 4, training loss: 0.2920\n",
      "Epoch 4, Val Loss: 0.2966\n",
      "Epoch: 5, training loss: 0.2916\n",
      "Epoch 5, Val Loss: 0.2966\n",
      "Epoch: 6, training loss: 0.2936\n",
      "Epoch 6, Val Loss: 0.3034\n",
      "Epoch: 7, training loss: 0.2882\n",
      "Epoch 7, Val Loss: 0.2981\n",
      "Epoch: 8, training loss: 0.2932\n",
      "Epoch 8, Val Loss: 0.2981\n",
      "Epoch: 9, training loss: 0.2948\n",
      "Epoch 9, Val Loss: 0.3006\n",
      "Epoch: 10, training loss: 0.2922\n",
      "Epoch 10, Val Loss: 0.2934\n",
      "Fold 4:\n",
      "Epoch: 1, training loss: 0.3606\n",
      "Epoch 1, Val Loss: 0.3138\n",
      "Epoch: 2, training loss: 0.3116\n",
      "Epoch 2, Val Loss: 0.2931\n",
      "Epoch: 3, training loss: 0.2952\n",
      "Epoch 3, Val Loss: 0.2918\n",
      "Epoch: 4, training loss: 0.2957\n",
      "Epoch 4, Val Loss: 0.2949\n",
      "Epoch: 5, training loss: 0.2957\n",
      "Epoch 5, Val Loss: 0.3052\n",
      "Epoch: 6, training loss: 0.2956\n",
      "Epoch 6, Val Loss: 0.2904\n",
      "Epoch: 7, training loss: 0.2889\n",
      "Epoch 7, Val Loss: 0.2985\n",
      "Epoch: 8, training loss: 0.2980\n",
      "Epoch 8, Val Loss: 0.2928\n",
      "Epoch: 9, training loss: 0.2923\n",
      "Epoch 9, Val Loss: 0.2929\n",
      "Epoch: 10, training loss: 0.2917\n",
      "Epoch 10, Val Loss: 0.2929\n",
      "Fold 5:\n",
      "Epoch: 1, training loss: 0.3577\n",
      "Epoch 1, Val Loss: 0.3163\n",
      "Epoch: 2, training loss: 0.3057\n",
      "Epoch 2, Val Loss: 0.2978\n",
      "Epoch: 3, training loss: 0.2917\n",
      "Epoch 3, Val Loss: 0.2929\n",
      "Epoch: 4, training loss: 0.2949\n",
      "Epoch 4, Val Loss: 0.2970\n",
      "Epoch: 5, training loss: 0.2925\n",
      "Epoch 5, Val Loss: 0.2962\n",
      "Epoch: 6, training loss: 0.2921\n",
      "Epoch 6, Val Loss: 0.2951\n",
      "Epoch: 7, training loss: 0.2937\n",
      "Epoch 7, Val Loss: 0.2953\n",
      "Epoch: 8, training loss: 0.2941\n",
      "Epoch 8, Val Loss: 0.2954\n",
      "Epoch: 9, training loss: 0.2954\n",
      "Epoch 9, Val Loss: 0.2992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-07 19:19:04,799] Trial 13 finished with value: 0.37735558639428074 and parameters: {'hidden_dim': 53, 'dropout': 0.22349186907065982, 'initial_lr': 6.995890036673634e-05, 'max_lr': 0.08938960521453812, 'criterion': 'BCEWithLogitsLoss', 'pos_weight': 1}. Best is trial 13 with value: 0.37735558639428074.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, training loss: 0.2924\n",
      "Epoch 10, Val Loss: 0.3068\n",
      "Using device: cuda\n",
      "Fold 1:\n",
      "Epoch: 1, training loss: 0.5446\n",
      "Epoch 1, Val Loss: 0.4911\n",
      "Epoch: 2, training loss: 0.4842\n",
      "Epoch 2, Val Loss: 0.4585\n",
      "Epoch: 3, training loss: 0.4763\n",
      "Epoch 3, Val Loss: 0.4472\n",
      "Epoch: 4, training loss: 0.4687\n",
      "Epoch 4, Val Loss: 0.4567\n",
      "Epoch: 5, training loss: 0.4613\n",
      "Epoch 5, Val Loss: 0.4497\n",
      "Epoch: 6, training loss: 0.4671\n",
      "Epoch 6, Val Loss: 0.4507\n",
      "Epoch: 7, training loss: 0.4612\n",
      "Epoch 7, Val Loss: 0.4575\n",
      "Epoch: 8, training loss: 0.4629\n",
      "Epoch 8, Val Loss: 0.4486\n",
      "Epoch: 9, training loss: 0.4617\n",
      "Epoch 9, Val Loss: 0.4615\n",
      "Epoch: 10, training loss: 0.4789\n",
      "Epoch 10, Val Loss: 0.4445\n",
      "Fold 2:\n",
      "Epoch: 1, training loss: 0.5475\n",
      "Epoch 1, Val Loss: 0.5028\n",
      "Epoch: 2, training loss: 0.4850\n",
      "Epoch 2, Val Loss: 0.4761\n",
      "Epoch: 3, training loss: 0.4694\n",
      "Epoch 3, Val Loss: 0.4694\n",
      "Epoch: 4, training loss: 0.4707\n",
      "Epoch 4, Val Loss: 0.4791\n",
      "Epoch: 5, training loss: 0.4651\n",
      "Epoch 5, Val Loss: 0.4657\n",
      "Epoch: 6, training loss: 0.4587\n",
      "Epoch 6, Val Loss: 0.4797\n",
      "Epoch: 7, training loss: 0.4616\n",
      "Epoch 7, Val Loss: 0.4704\n",
      "Epoch: 8, training loss: 0.4613\n",
      "Epoch 8, Val Loss: 0.4734\n",
      "Epoch: 9, training loss: 0.4612\n",
      "Epoch 9, Val Loss: 0.4598\n",
      "Epoch: 10, training loss: 0.4668\n",
      "Epoch 10, Val Loss: 0.4639\n",
      "Fold 3:\n",
      "Epoch: 1, training loss: 0.5367\n",
      "Epoch 1, Val Loss: 0.4912\n",
      "Epoch: 2, training loss: 0.4794\n",
      "Epoch 2, Val Loss: 0.4725\n",
      "Epoch: 3, training loss: 0.4685\n",
      "Epoch 3, Val Loss: 0.4697\n",
      "Epoch: 4, training loss: 0.4597\n",
      "Epoch 4, Val Loss: 0.4880\n",
      "Epoch: 5, training loss: 0.4571\n",
      "Epoch 5, Val Loss: 0.4682\n",
      "Epoch: 6, training loss: 0.4599\n",
      "Epoch 6, Val Loss: 0.4911\n",
      "Epoch: 7, training loss: 0.4590\n",
      "Epoch 7, Val Loss: 0.4733\n",
      "Epoch: 8, training loss: 0.4589\n",
      "Epoch 8, Val Loss: 0.4744\n",
      "Epoch: 9, training loss: 0.4659\n",
      "Epoch 9, Val Loss: 0.4764\n",
      "Epoch: 10, training loss: 0.4587\n",
      "Epoch 10, Val Loss: 0.4851\n",
      "Fold 4:\n",
      "Epoch: 1, training loss: 0.5449\n",
      "Epoch 1, Val Loss: 0.4923\n",
      "Epoch: 2, training loss: 0.4897\n",
      "Epoch 2, Val Loss: 0.4644\n",
      "Epoch: 3, training loss: 0.4713\n",
      "Epoch 3, Val Loss: 0.4656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [07/Apr/2025 19:19:08] \"GET /api/studies/0?after=12 HTTP/1.1\" 200 9071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, training loss: 0.4661\n",
      "Epoch 4, Val Loss: 0.4625\n",
      "Epoch: 5, training loss: 0.4601\n",
      "Epoch 5, Val Loss: 0.4575\n",
      "Epoch: 6, training loss: 0.4610\n",
      "Epoch 6, Val Loss: 0.4719\n",
      "Epoch: 7, training loss: 0.4572\n",
      "Epoch 7, Val Loss: 0.4619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\optuna_dashboard\\_importance.py:96: ExperimentalWarning: PedAnovaImportanceEvaluator is experimental (supported from v3.6.0). The interface can change in the future.\n",
      "  study, target=target, evaluator=PedAnovaImportanceEvaluator()\n",
      "d:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\optuna\\importance\\_ped_anova\\evaluator.py:150: UserWarning: PedAnovaImportanceEvaluator computes the importances of params to achieve low `target` values. If this is not what you want, please modify target, e.g., by multiplying the output by -1.\n",
      "  warnings.warn(\n",
      "127.0.0.1 - - [07/Apr/2025 19:19:09] \"GET /api/studies/0/param_importances?evaluator=ped_anova HTTP/1.1\" 200 582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, training loss: 0.4545\n",
      "Epoch 8, Val Loss: 0.4745\n",
      "Epoch: 9, training loss: 0.4653\n",
      "Epoch 9, Val Loss: 0.4614\n",
      "Epoch: 10, training loss: 0.4658\n",
      "Epoch 10, Val Loss: 0.4586\n",
      "Fold 5:\n",
      "Epoch: 1, training loss: 0.5350\n",
      "Epoch 1, Val Loss: 0.4980\n",
      "Epoch: 2, training loss: 0.4813\n",
      "Epoch 2, Val Loss: 0.4715\n",
      "Epoch: 3, training loss: 0.4687\n",
      "Epoch 3, Val Loss: 0.4635\n",
      "Epoch: 4, training loss: 0.4631\n",
      "Epoch 4, Val Loss: 0.4605\n",
      "Epoch: 5, training loss: 0.4697\n",
      "Epoch 5, Val Loss: 0.4734\n",
      "Epoch: 6, training loss: 0.4639\n",
      "Epoch 6, Val Loss: 0.4689\n",
      "Epoch: 7, training loss: 0.4696\n",
      "Epoch 7, Val Loss: 0.4610\n",
      "Epoch: 8, training loss: 0.4652\n",
      "Epoch 8, Val Loss: 0.5043\n",
      "Epoch: 9, training loss: 0.4657\n",
      "Epoch 9, Val Loss: 0.4710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-07 19:19:10,882] Trial 14 finished with value: 0.36146610438540316 and parameters: {'hidden_dim': 49, 'dropout': 0.2261716128281292, 'initial_lr': 3.431856524962002e-05, 'max_lr': 0.09832709235379451, 'criterion': 'BCEWithLogitsLoss', 'pos_weight': 2}. Best is trial 13 with value: 0.37735558639428074.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, training loss: 0.4721\n",
      "Epoch 10, Val Loss: 0.4680\n",
      "Using device: cuda\n",
      "Fold 1:\n",
      "Epoch: 1, training loss: 1.0654\n",
      "Epoch 1, Val Loss: 0.9837\n",
      "Epoch: 2, training loss: 0.9827\n",
      "Epoch 2, Val Loss: 0.9327\n",
      "Epoch: 3, training loss: 0.9650\n",
      "Epoch 3, Val Loss: 0.9111\n",
      "Epoch: 4, training loss: 0.9517\n",
      "Epoch 4, Val Loss: 0.9175\n",
      "Epoch: 5, training loss: 0.9511\n",
      "Epoch 5, Val Loss: 0.9362\n",
      "Epoch: 6, training loss: 0.9491\n",
      "Epoch 6, Val Loss: 0.9152\n",
      "Epoch: 7, training loss: 0.9514\n",
      "Epoch 7, Val Loss: 0.9609\n",
      "Epoch: 8, training loss: 0.9403\n",
      "Epoch 8, Val Loss: 0.9282\n",
      "Epoch: 9, training loss: 0.9563\n",
      "Epoch 9, Val Loss: 0.9052\n",
      "Epoch: 10, training loss: 0.9485\n",
      "Epoch 10, Val Loss: 0.9622\n",
      "Fold 2:\n",
      "Epoch: 1, training loss: 1.0465\n",
      "Epoch 1, Val Loss: 0.9926\n",
      "Epoch: 2, training loss: 0.9792\n",
      "Epoch 2, Val Loss: 0.9510\n",
      "Epoch: 3, training loss: 0.9435\n",
      "Epoch 3, Val Loss: 0.9404\n",
      "Epoch: 4, training loss: 0.9550\n",
      "Epoch 4, Val Loss: 0.9615\n",
      "Epoch: 5, training loss: 0.9525\n",
      "Epoch 5, Val Loss: 1.0113\n",
      "Epoch: 6, training loss: 0.9336\n",
      "Epoch 6, Val Loss: 0.9747\n",
      "Epoch: 7, training loss: 0.9441\n",
      "Epoch 7, Val Loss: 0.9672\n",
      "Epoch: 8, training loss: 0.9525\n",
      "Epoch 8, Val Loss: 0.9599\n",
      "Epoch: 9, training loss: 0.9569\n",
      "Epoch 9, Val Loss: 0.9707\n",
      "Epoch: 10, training loss: 0.9456\n",
      "Epoch 10, Val Loss: 0.9471\n",
      "Fold 3:\n",
      "Epoch: 1, training loss: 1.0538\n",
      "Epoch 1, Val Loss: 0.9902\n",
      "Epoch: 2, training loss: 0.9717\n",
      "Epoch 2, Val Loss: 0.9695\n",
      "Epoch: 3, training loss: 0.9580\n",
      "Epoch 3, Val Loss: 0.9788\n",
      "Epoch: 4, training loss: 0.9426\n",
      "Epoch 4, Val Loss: 0.9664\n",
      "Epoch: 5, training loss: 0.9471\n",
      "Epoch 5, Val Loss: 0.9680\n",
      "Epoch: 6, training loss: 0.9443\n",
      "Epoch 6, Val Loss: 0.9792\n",
      "Epoch: 7, training loss: 0.9477\n",
      "Epoch 7, Val Loss: 0.9728\n",
      "Epoch: 8, training loss: 0.9533\n",
      "Epoch 8, Val Loss: 1.0328\n",
      "Epoch: 9, training loss: 0.9374\n",
      "Epoch 9, Val Loss: 0.9896\n",
      "Epoch: 10, training loss: 0.9343\n",
      "Epoch 10, Val Loss: 1.0464\n",
      "Fold 4:\n",
      "Epoch: 1, training loss: 1.0642\n",
      "Epoch 1, Val Loss: 1.0013\n",
      "Epoch: 2, training loss: 0.9788\n",
      "Epoch 2, Val Loss: 0.9645\n",
      "Epoch: 3, training loss: 0.9572\n",
      "Epoch 3, Val Loss: 0.9553\n",
      "Epoch: 4, training loss: 0.9442\n",
      "Epoch 4, Val Loss: 0.9574\n",
      "Epoch: 5, training loss: 0.9558\n",
      "Epoch 5, Val Loss: 0.9696\n",
      "Epoch: 6, training loss: 0.9497\n",
      "Epoch 6, Val Loss: 1.0136\n",
      "Epoch: 7, training loss: 0.9507\n",
      "Epoch 7, Val Loss: 0.9566\n",
      "Epoch: 8, training loss: 0.9527\n",
      "Epoch 8, Val Loss: 0.9503\n",
      "Epoch: 9, training loss: 0.9440\n",
      "Epoch 9, Val Loss: 0.9668\n",
      "Epoch: 10, training loss: 0.9560\n",
      "Epoch 10, Val Loss: 0.9817\n",
      "Fold 5:\n",
      "Epoch: 1, training loss: 1.0504\n",
      "Epoch 1, Val Loss: 1.0030\n",
      "Epoch: 2, training loss: 0.9751\n",
      "Epoch 2, Val Loss: 0.9720\n",
      "Epoch: 3, training loss: 0.9459\n",
      "Epoch 3, Val Loss: 0.9650\n",
      "Epoch: 4, training loss: 0.9571\n",
      "Epoch 4, Val Loss: 0.9901\n",
      "Epoch: 5, training loss: 0.9579\n",
      "Epoch 5, Val Loss: 0.9841\n",
      "Epoch: 6, training loss: 0.9416\n",
      "Epoch 6, Val Loss: 0.9776\n",
      "Epoch: 7, training loss: 0.9417\n",
      "Epoch 7, Val Loss: 1.0035\n",
      "Epoch: 8, training loss: 0.9433\n",
      "Epoch 8, Val Loss: 1.0307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-07 19:19:16,778] Trial 15 finished with value: 0.26701508233701265 and parameters: {'hidden_dim': 50, 'dropout': 0.21846591764693718, 'initial_lr': 0.0001276149632402392, 'max_lr': 0.08504637527983548, 'criterion': 'BCEWithLogitsLoss', 'pos_weight': 7}. Best is trial 13 with value: 0.37735558639428074.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, training loss: 0.9631\n",
      "Epoch 9, Val Loss: 0.9990\n",
      "Epoch: 10, training loss: 0.9541\n",
      "Epoch 10, Val Loss: 1.0268\n",
      "Using device: cuda\n",
      "Fold 1:\n",
      "Epoch: 1, training loss: 0.5665\n",
      "Epoch 1, Val Loss: 0.5105\n",
      "Epoch: 2, training loss: 0.4992\n",
      "Epoch 2, Val Loss: 0.4700\n",
      "Epoch: 3, training loss: 0.4714\n",
      "Epoch 3, Val Loss: 0.4631\n",
      "Epoch: 4, training loss: 0.4686\n",
      "Epoch 4, Val Loss: 0.4664\n",
      "Epoch: 5, training loss: 0.4662\n",
      "Epoch 5, Val Loss: 0.4554\n",
      "Epoch: 6, training loss: 0.4619\n",
      "Epoch 6, Val Loss: 0.4496\n",
      "Epoch: 7, training loss: 0.4621\n",
      "Epoch 7, Val Loss: 0.4553\n",
      "Epoch: 8, training loss: 0.4572\n",
      "Epoch 8, Val Loss: 0.4486\n",
      "Epoch: 9, training loss: 0.4617\n",
      "Epoch 9, Val Loss: 0.4462\n",
      "Epoch: 10, training loss: 0.4604\n",
      "Epoch 10, Val Loss: 0.4627\n",
      "Fold 2:\n",
      "Epoch: 1, training loss: 0.5864\n",
      "Epoch 1, Val Loss: 0.5085\n",
      "Epoch: 2, training loss: 0.4958\n",
      "Epoch 2, Val Loss: 0.4764\n",
      "Epoch: 3, training loss: 0.4726\n",
      "Epoch 3, Val Loss: 0.4614\n",
      "Epoch: 4, training loss: 0.4614\n",
      "Epoch 4, Val Loss: 0.4571\n",
      "Epoch: 5, training loss: 0.4664\n",
      "Epoch 5, Val Loss: 0.4538\n",
      "Epoch: 6, training loss: 0.4554\n",
      "Epoch 6, Val Loss: 0.4657\n",
      "Epoch: 7, training loss: 0.4781\n",
      "Epoch 7, Val Loss: 0.4551\n",
      "Epoch: 8, training loss: 0.4583\n",
      "Epoch 8, Val Loss: 0.4685\n",
      "Epoch: 9, training loss: 0.4629\n",
      "Epoch 9, Val Loss: 0.4561\n",
      "Epoch: 10, training loss: 0.4589\n",
      "Epoch 10, Val Loss: 0.4573\n",
      "Fold 3:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [07/Apr/2025 19:19:19] \"GET /api/studies/0?after=14 HTTP/1.1\" 200 9067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, training loss: 0.5780\n",
      "Epoch 1, Val Loss: 0.5067\n",
      "Epoch: 2, training loss: 0.4890\n",
      "Epoch 2, Val Loss: 0.4824\n",
      "Epoch: 3, training loss: 0.4659\n",
      "Epoch 3, Val Loss: 0.4708\n",
      "Epoch: 4, training loss: 0.4610\n",
      "Epoch 4, Val Loss: 0.4800\n",
      "Epoch: 5, training loss: 0.4584\n",
      "Epoch 5, Val Loss: 0.4724\n",
      "Epoch: 6, training loss: 0.4535\n",
      "Epoch 6, Val Loss: 0.4728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\optuna_dashboard\\_importance.py:96: ExperimentalWarning: PedAnovaImportanceEvaluator is experimental (supported from v3.6.0). The interface can change in the future.\n",
      "  study, target=target, evaluator=PedAnovaImportanceEvaluator()\n",
      "d:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\optuna\\importance\\_ped_anova\\evaluator.py:150: UserWarning: PedAnovaImportanceEvaluator computes the importances of params to achieve low `target` values. If this is not what you want, please modify target, e.g., by multiplying the output by -1.\n",
      "  warnings.warn(\n",
      "127.0.0.1 - - [07/Apr/2025 19:19:20] \"GET /api/studies/0/param_importances?evaluator=ped_anova HTTP/1.1\" 200 579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, training loss: 0.4599\n",
      "Epoch 7, Val Loss: 0.4895\n",
      "Epoch: 8, training loss: 0.4600\n",
      "Epoch 8, Val Loss: 0.4861\n",
      "Epoch: 9, training loss: 0.4577\n",
      "Epoch 9, Val Loss: 0.4700\n",
      "Epoch: 10, training loss: 0.4567\n",
      "Epoch 10, Val Loss: 0.4834\n",
      "Fold 4:\n",
      "Epoch: 1, training loss: 0.5420\n",
      "Epoch 1, Val Loss: 0.5042\n",
      "Epoch: 2, training loss: 0.4864\n",
      "Epoch 2, Val Loss: 0.4692\n",
      "Epoch: 3, training loss: 0.4644\n",
      "Epoch 3, Val Loss: 0.4619\n",
      "Epoch: 4, training loss: 0.4612\n",
      "Epoch 4, Val Loss: 0.4578\n",
      "Epoch: 5, training loss: 0.4611\n",
      "Epoch 5, Val Loss: 0.4590\n",
      "Epoch: 6, training loss: 0.4551\n",
      "Epoch 6, Val Loss: 0.4655\n",
      "Epoch: 7, training loss: 0.4551\n",
      "Epoch 7, Val Loss: 0.4630\n",
      "Epoch: 8, training loss: 0.4552\n",
      "Epoch 8, Val Loss: 0.4837\n",
      "Epoch: 9, training loss: 0.4630\n",
      "Epoch 9, Val Loss: 0.4719\n",
      "Epoch: 10, training loss: 0.4647\n",
      "Epoch 10, Val Loss: 0.4633\n",
      "Fold 5:\n",
      "Epoch: 1, training loss: 0.5460\n",
      "Epoch 1, Val Loss: 0.5200\n",
      "Epoch: 2, training loss: 0.4905\n",
      "Epoch 2, Val Loss: 0.4872\n",
      "Epoch: 3, training loss: 0.4716\n",
      "Epoch 3, Val Loss: 0.4715\n",
      "Epoch: 4, training loss: 0.4619\n",
      "Epoch 4, Val Loss: 0.4733\n",
      "Epoch: 5, training loss: 0.4593\n",
      "Epoch 5, Val Loss: 0.4841\n",
      "Epoch: 6, training loss: 0.4601\n",
      "Epoch 6, Val Loss: 0.4847\n",
      "Epoch: 7, training loss: 0.4606\n",
      "Epoch 7, Val Loss: 0.4773\n",
      "Epoch: 8, training loss: 0.4514\n",
      "Epoch 8, Val Loss: 0.4749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-07 19:19:23,034] Trial 16 finished with value: 0.3546722113797059 and parameters: {'hidden_dim': 70, 'dropout': 0.2064515236372696, 'initial_lr': 4.031311659833696e-05, 'max_lr': 0.050540352876080885, 'criterion': 'BCEWithLogitsLoss', 'pos_weight': 2}. Best is trial 13 with value: 0.37735558639428074.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, training loss: 0.4542\n",
      "Epoch 9, Val Loss: 0.4760\n",
      "Epoch: 10, training loss: 0.4559\n",
      "Epoch 10, Val Loss: 0.4772\n",
      "Using device: cuda\n",
      "Fold 1:\n",
      "Epoch: 1, training loss: 0.6840\n",
      "Epoch 1, Val Loss: 0.6275\n",
      "Epoch: 2, training loss: 0.5868\n",
      "Epoch 2, Val Loss: 0.5519\n",
      "Epoch: 3, training loss: 0.5367\n",
      "Epoch 3, Val Loss: 0.5266\n",
      "Epoch: 4, training loss: 0.5204\n",
      "Epoch 4, Val Loss: 0.5148\n",
      "Epoch: 5, training loss: 0.5117\n",
      "Epoch 5, Val Loss: 0.5037\n",
      "Epoch: 6, training loss: 0.5018\n",
      "Epoch 6, Val Loss: 0.4930\n",
      "Epoch: 7, training loss: 0.4923\n",
      "Epoch 7, Val Loss: 0.4832\n",
      "Epoch: 8, training loss: 0.4834\n",
      "Epoch 8, Val Loss: 0.4739\n",
      "Epoch: 9, training loss: 0.4790\n",
      "Epoch 9, Val Loss: 0.4661\n",
      "Epoch: 10, training loss: 0.4700\n",
      "Epoch 10, Val Loss: 0.4598\n",
      "Fold 2:\n",
      "Epoch: 1, training loss: 0.7436\n",
      "Epoch 1, Val Loss: 0.6835\n",
      "Epoch: 2, training loss: 0.6180\n",
      "Epoch 2, Val Loss: 0.5752\n",
      "Epoch: 3, training loss: 0.5470\n",
      "Epoch 3, Val Loss: 0.5337\n",
      "Epoch: 4, training loss: 0.5223\n",
      "Epoch 4, Val Loss: 0.5227\n",
      "Epoch: 5, training loss: 0.5127\n",
      "Epoch 5, Val Loss: 0.5134\n",
      "Epoch: 6, training loss: 0.5004\n",
      "Epoch 6, Val Loss: 0.5035\n",
      "Epoch: 7, training loss: 0.4941\n",
      "Epoch 7, Val Loss: 0.4942\n",
      "Epoch: 8, training loss: 0.4848\n",
      "Epoch 8, Val Loss: 0.4858\n",
      "Epoch: 9, training loss: 0.4821\n",
      "Epoch 9, Val Loss: 0.4794\n",
      "Epoch: 10, training loss: 0.4736\n",
      "Epoch 10, Val Loss: 0.4731\n",
      "Fold 3:\n",
      "Epoch: 1, training loss: 0.7828\n",
      "Epoch 1, Val Loss: 0.7159\n",
      "Epoch: 2, training loss: 0.6545\n",
      "Epoch 2, Val Loss: 0.5952\n",
      "Epoch: 3, training loss: 0.5580\n",
      "Epoch 3, Val Loss: 0.5304\n",
      "Epoch: 4, training loss: 0.5241\n",
      "Epoch 4, Val Loss: 0.5166\n",
      "Epoch: 5, training loss: 0.5139\n",
      "Epoch 5, Val Loss: 0.5091\n",
      "Epoch: 6, training loss: 0.5082\n",
      "Epoch 6, Val Loss: 0.5017\n",
      "Epoch: 7, training loss: 0.4968\n",
      "Epoch 7, Val Loss: 0.4963\n",
      "Epoch: 8, training loss: 0.4887\n",
      "Epoch 8, Val Loss: 0.4906\n",
      "Epoch: 9, training loss: 0.4796\n",
      "Epoch 9, Val Loss: 0.4857\n",
      "Epoch: 10, training loss: 0.4764\n",
      "Epoch 10, Val Loss: 0.4794\n",
      "Fold 4:\n",
      "Epoch: 1, training loss: 0.7135\n",
      "Epoch 1, Val Loss: 0.6487\n",
      "Epoch: 2, training loss: 0.6132\n",
      "Epoch 2, Val Loss: 0.5655\n",
      "Epoch: 3, training loss: 0.5567\n",
      "Epoch 3, Val Loss: 0.5372\n",
      "Epoch: 4, training loss: 0.5413\n",
      "Epoch 4, Val Loss: 0.5265\n",
      "Epoch: 5, training loss: 0.5266\n",
      "Epoch 5, Val Loss: 0.5157\n",
      "Epoch: 6, training loss: 0.5174\n",
      "Epoch 6, Val Loss: 0.5052\n",
      "Epoch: 7, training loss: 0.5071\n",
      "Epoch 7, Val Loss: 0.4957\n",
      "Epoch: 8, training loss: 0.4984\n",
      "Epoch 8, Val Loss: 0.4874\n",
      "Epoch: 9, training loss: 0.4885\n",
      "Epoch 9, Val Loss: 0.4799\n",
      "Epoch: 10, training loss: 0.4836\n",
      "Epoch 10, Val Loss: 0.4725\n",
      "Fold 5:\n",
      "Epoch: 1, training loss: 0.6696\n",
      "Epoch 1, Val Loss: 0.6226\n",
      "Epoch: 2, training loss: 0.5816\n",
      "Epoch 2, Val Loss: 0.5521\n",
      "Epoch: 3, training loss: 0.5355\n",
      "Epoch 3, Val Loss: 0.5282\n",
      "Epoch: 4, training loss: 0.5144\n",
      "Epoch 4, Val Loss: 0.5210\n",
      "Epoch: 5, training loss: 0.5133\n",
      "Epoch 5, Val Loss: 0.5145\n",
      "Epoch: 6, training loss: 0.5010\n",
      "Epoch 6, Val Loss: 0.5085\n",
      "Epoch: 7, training loss: 0.4951\n",
      "Epoch 7, Val Loss: 0.5025\n",
      "Epoch: 8, training loss: 0.4889\n",
      "Epoch 8, Val Loss: 0.4959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-07 19:19:28,678] Trial 17 finished with value: 0.3749179741678658 and parameters: {'hidden_dim': 91, 'dropout': 0.25971546264340206, 'initial_lr': 0.00014747074459018367, 'max_lr': 0.0022612831192323005, 'criterion': 'BCEWithLogitsLoss', 'pos_weight': 2}. Best is trial 13 with value: 0.37735558639428074.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, training loss: 0.4774\n",
      "Epoch 9, Val Loss: 0.4906\n",
      "Epoch: 10, training loss: 0.4751\n",
      "Epoch 10, Val Loss: 0.4862\n",
      "Using device: cuda\n",
      "Fold 1:\n",
      "Epoch: 1, training loss: 1.0962\n",
      "Epoch 1, Val Loss: 1.0340\n",
      "Epoch: 2, training loss: 1.0084\n",
      "Epoch 2, Val Loss: 0.9508\n",
      "Epoch: 3, training loss: 0.9628\n",
      "Epoch 3, Val Loss: 0.9378\n",
      "Epoch: 4, training loss: 0.9521\n",
      "Epoch 4, Val Loss: 0.9273\n",
      "Epoch: 5, training loss: 0.9419\n",
      "Epoch 5, Val Loss: 0.9801\n",
      "Epoch: 6, training loss: 0.9656\n",
      "Epoch 6, Val Loss: 0.9385\n",
      "Epoch: 7, training loss: 0.9307\n",
      "Epoch 7, Val Loss: 0.9414\n",
      "Epoch: 8, training loss: 0.9643\n",
      "Epoch 8, Val Loss: 0.9399\n",
      "Epoch: 9, training loss: 0.9618\n",
      "Epoch 9, Val Loss: 0.9129\n",
      "Epoch: 10, training loss: 0.9337\n",
      "Epoch 10, Val Loss: 0.9268\n",
      "Fold 2:\n",
      "Epoch: 1, training loss: 1.0758\n",
      "Epoch 1, Val Loss: 1.0308\n",
      "Epoch: 2, training loss: 0.9983\n",
      "Epoch 2, Val Loss: 0.9696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [07/Apr/2025 19:19:30] \"GET /api/studies/0?after=16 HTTP/1.1\" 200 9075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, training loss: 0.9500\n",
      "Epoch 3, Val Loss: 0.9578\n",
      "Epoch: 4, training loss: 0.9434\n",
      "Epoch 4, Val Loss: 0.9664\n",
      "Epoch: 5, training loss: 0.9454\n",
      "Epoch 5, Val Loss: 0.9813\n",
      "Epoch: 6, training loss: 0.9367\n",
      "Epoch 6, Val Loss: 0.9538\n",
      "Epoch: 7, training loss: 0.9363\n",
      "Epoch 7, Val Loss: 0.9517\n",
      "Epoch: 8, training loss: 0.9327\n",
      "Epoch 8, Val Loss: 0.9466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\optuna_dashboard\\_importance.py:96: ExperimentalWarning: PedAnovaImportanceEvaluator is experimental (supported from v3.6.0). The interface can change in the future.\n",
      "  study, target=target, evaluator=PedAnovaImportanceEvaluator()\n",
      "d:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\optuna\\importance\\_ped_anova\\evaluator.py:150: UserWarning: PedAnovaImportanceEvaluator computes the importances of params to achieve low `target` values. If this is not what you want, please modify target, e.g., by multiplying the output by -1.\n",
      "  warnings.warn(\n",
      "127.0.0.1 - - [07/Apr/2025 19:19:31] \"GET /api/studies/0/param_importances?evaluator=ped_anova HTTP/1.1\" 200 578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, training loss: 0.9460\n",
      "Epoch 9, Val Loss: 0.9636\n",
      "Epoch: 10, training loss: 0.9281\n",
      "Epoch 10, Val Loss: 0.9998\n",
      "Fold 3:\n",
      "Epoch: 1, training loss: 1.0764\n",
      "Epoch 1, Val Loss: 1.0459\n",
      "Epoch: 2, training loss: 0.9877\n",
      "Epoch 2, Val Loss: 0.9895\n",
      "Epoch: 3, training loss: 0.9584\n",
      "Epoch 3, Val Loss: 1.0234\n",
      "Epoch: 4, training loss: 0.9538\n",
      "Epoch 4, Val Loss: 0.9654\n",
      "Epoch: 5, training loss: 0.9254\n",
      "Epoch 5, Val Loss: 0.9666\n",
      "Epoch: 6, training loss: 0.9306\n",
      "Epoch 6, Val Loss: 0.9838\n",
      "Epoch: 7, training loss: 0.9260\n",
      "Epoch 7, Val Loss: 0.9653\n",
      "Epoch: 8, training loss: 0.9251\n",
      "Epoch 8, Val Loss: 0.9972\n",
      "Epoch: 9, training loss: 0.9344\n",
      "Epoch 9, Val Loss: 0.9692\n",
      "Epoch: 10, training loss: 0.9198\n",
      "Epoch 10, Val Loss: 0.9745\n",
      "Fold 4:\n",
      "Epoch: 1, training loss: 1.0841\n",
      "Epoch 1, Val Loss: 1.0275\n",
      "Epoch: 2, training loss: 0.9975\n",
      "Epoch 2, Val Loss: 0.9658\n",
      "Epoch: 3, training loss: 0.9564\n",
      "Epoch 3, Val Loss: 0.9384\n",
      "Epoch: 4, training loss: 0.9446\n",
      "Epoch 4, Val Loss: 0.9452\n",
      "Epoch: 5, training loss: 0.9406\n",
      "Epoch 5, Val Loss: 0.9347\n",
      "Epoch: 6, training loss: 0.9292\n",
      "Epoch 6, Val Loss: 0.9996\n",
      "Epoch: 7, training loss: 0.9491\n",
      "Epoch 7, Val Loss: 0.9849\n",
      "Epoch: 8, training loss: 0.9363\n",
      "Epoch 8, Val Loss: 0.9636\n",
      "Epoch: 9, training loss: 0.9288\n",
      "Epoch 9, Val Loss: 0.9681\n",
      "Epoch: 10, training loss: 0.9284\n",
      "Epoch 10, Val Loss: 1.0001\n",
      "Fold 5:\n",
      "Epoch: 1, training loss: 1.0655\n",
      "Epoch 1, Val Loss: 1.0348\n",
      "Epoch: 2, training loss: 0.9929\n",
      "Epoch 2, Val Loss: 0.9760\n",
      "Epoch: 3, training loss: 0.9576\n",
      "Epoch 3, Val Loss: 0.9661\n",
      "Epoch: 4, training loss: 0.9482\n",
      "Epoch 4, Val Loss: 0.9690\n",
      "Epoch: 5, training loss: 0.9424\n",
      "Epoch 5, Val Loss: 1.0203\n",
      "Epoch: 6, training loss: 0.9381\n",
      "Epoch 6, Val Loss: 0.9942\n",
      "Epoch: 7, training loss: 0.9455\n",
      "Epoch 7, Val Loss: 0.9929\n",
      "Epoch: 8, training loss: 0.9373\n",
      "Epoch 8, Val Loss: 0.9803\n",
      "Epoch: 9, training loss: 0.9282\n",
      "Epoch 9, Val Loss: 0.9893\n",
      "Epoch: 10, training loss: 0.9402\n",
      "Epoch 10, Val Loss: 0.9810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-07 19:19:34,622] Trial 18 finished with value: 0.23987722691832616 and parameters: {'hidden_dim': 42, 'dropout': 0.10434576686033112, 'initial_lr': 3.506617452546226e-05, 'max_lr': 0.053062878488186985, 'criterion': 'BCEWithLogitsLoss', 'pos_weight': 7}. Best is trial 13 with value: 0.37735558639428074.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Fold 1:\n",
      "Epoch: 1, training loss: 0.5439\n",
      "Epoch 1, Val Loss: 0.4618\n",
      "Epoch: 2, training loss: 0.4046\n",
      "Epoch 2, Val Loss: 0.3581\n",
      "Epoch: 3, training loss: 0.3448\n",
      "Epoch 3, Val Loss: 0.3318\n",
      "Epoch: 4, training loss: 0.3298\n",
      "Epoch 4, Val Loss: 0.3255\n",
      "Epoch: 5, training loss: 0.3245\n",
      "Epoch 5, Val Loss: 0.3194\n",
      "Epoch: 6, training loss: 0.3200\n",
      "Epoch 6, Val Loss: 0.3130\n",
      "Epoch: 7, training loss: 0.3141\n",
      "Epoch 7, Val Loss: 0.3068\n",
      "Epoch: 8, training loss: 0.3070\n",
      "Epoch 8, Val Loss: 0.3011\n",
      "Epoch: 9, training loss: 0.3033\n",
      "Epoch 9, Val Loss: 0.2961\n",
      "Epoch: 10, training loss: 0.2971\n",
      "Epoch 10, Val Loss: 0.2909\n",
      "Fold 2:\n",
      "Epoch: 1, training loss: 0.6727\n",
      "Epoch 1, Val Loss: 0.5877\n",
      "Epoch: 2, training loss: 0.5087\n",
      "Epoch 2, Val Loss: 0.4230\n",
      "Epoch: 3, training loss: 0.3722\n",
      "Epoch 3, Val Loss: 0.3321\n",
      "Epoch: 4, training loss: 0.3285\n",
      "Epoch 4, Val Loss: 0.3228\n",
      "Epoch: 5, training loss: 0.3215\n",
      "Epoch 5, Val Loss: 0.3184\n",
      "Epoch: 6, training loss: 0.3158\n",
      "Epoch 6, Val Loss: 0.3134\n",
      "Epoch: 7, training loss: 0.3111\n",
      "Epoch 7, Val Loss: 0.3088\n",
      "Epoch: 8, training loss: 0.3061\n",
      "Epoch 8, Val Loss: 0.3045\n",
      "Epoch: 9, training loss: 0.3013\n",
      "Epoch 9, Val Loss: 0.2987\n",
      "Epoch: 10, training loss: 0.2957\n",
      "Epoch 10, Val Loss: 0.2956\n",
      "Fold 3:\n",
      "Epoch: 1, training loss: 0.6256\n",
      "Epoch 1, Val Loss: 0.5408\n",
      "Epoch: 2, training loss: 0.4681\n",
      "Epoch 2, Val Loss: 0.3983\n",
      "Epoch: 3, training loss: 0.3657\n",
      "Epoch 3, Val Loss: 0.3388\n",
      "Epoch: 4, training loss: 0.3371\n",
      "Epoch 4, Val Loss: 0.3298\n",
      "Epoch: 5, training loss: 0.3308\n",
      "Epoch 5, Val Loss: 0.3246\n",
      "Epoch: 6, training loss: 0.3249\n",
      "Epoch 6, Val Loss: 0.3196\n",
      "Epoch: 7, training loss: 0.3157\n",
      "Epoch 7, Val Loss: 0.3149\n",
      "Epoch: 8, training loss: 0.3092\n",
      "Epoch 8, Val Loss: 0.3117\n",
      "Epoch: 9, training loss: 0.3049\n",
      "Epoch 9, Val Loss: 0.3072\n",
      "Epoch: 10, training loss: 0.3006\n",
      "Epoch 10, Val Loss: 0.3040\n",
      "Fold 4:\n",
      "Epoch: 1, training loss: 0.5750\n",
      "Epoch 1, Val Loss: 0.4968\n",
      "Epoch: 2, training loss: 0.4358\n",
      "Epoch 2, Val Loss: 0.3781\n",
      "Epoch: 3, training loss: 0.3553\n",
      "Epoch 3, Val Loss: 0.3369\n",
      "Epoch: 4, training loss: 0.3349\n",
      "Epoch 4, Val Loss: 0.3301\n",
      "Epoch: 5, training loss: 0.3294\n",
      "Epoch 5, Val Loss: 0.3241\n",
      "Epoch: 6, training loss: 0.3233\n",
      "Epoch 6, Val Loss: 0.3176\n",
      "Epoch: 7, training loss: 0.3153\n",
      "Epoch 7, Val Loss: 0.3109\n",
      "Epoch: 8, training loss: 0.3103\n",
      "Epoch 8, Val Loss: 0.3046\n",
      "Epoch: 9, training loss: 0.3033\n",
      "Epoch 9, Val Loss: 0.3013\n",
      "Epoch: 10, training loss: 0.2968\n",
      "Epoch 10, Val Loss: 0.2946\n",
      "Fold 5:\n",
      "Epoch: 1, training loss: 0.6145\n",
      "Epoch 1, Val Loss: 0.5180\n",
      "Epoch: 2, training loss: 0.4446\n",
      "Epoch 2, Val Loss: 0.3785\n",
      "Epoch: 3, training loss: 0.3527\n",
      "Epoch 3, Val Loss: 0.3353\n",
      "Epoch: 4, training loss: 0.3308\n",
      "Epoch 4, Val Loss: 0.3298\n",
      "Epoch: 5, training loss: 0.3255\n",
      "Epoch 5, Val Loss: 0.3253\n",
      "Epoch: 6, training loss: 0.3206\n",
      "Epoch 6, Val Loss: 0.3204\n",
      "Epoch: 7, training loss: 0.3153\n",
      "Epoch 7, Val Loss: 0.3163\n",
      "Epoch: 8, training loss: 0.3082\n",
      "Epoch 8, Val Loss: 0.3117\n",
      "Epoch: 9, training loss: 0.3049\n",
      "Epoch 9, Val Loss: 0.3075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-07 19:19:39,715] Trial 19 finished with value: 0.37716486079509115 and parameters: {'hidden_dim': 80, 'dropout': 0.16732434250872613, 'initial_lr': 5.2939976032630886e-05, 'max_lr': 0.0031377631006227398, 'criterion': 'BCEWithLogitsLoss', 'pos_weight': 1}. Best is trial 13 with value: 0.37735558639428074.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, training loss: 0.2988\n",
      "Epoch 10, Val Loss: 0.3053\n",
      "Using device: cuda\n",
      "Fold 1:\n",
      "Epoch: 1, training loss: 0.8165\n",
      "Epoch 1, Val Loss: 0.7365\n",
      "Epoch: 2, training loss: 0.6968\n",
      "Epoch 2, Val Loss: 0.6754\n",
      "Epoch: 3, training loss: 0.6684\n",
      "Epoch 3, Val Loss: 0.6587\n",
      "Epoch: 4, training loss: 0.6559\n",
      "Epoch 4, Val Loss: 0.6431\n",
      "Epoch: 5, training loss: 0.6390\n",
      "Epoch 5, Val Loss: 0.6256\n",
      "Epoch: 6, training loss: 0.6244\n",
      "Epoch 6, Val Loss: 0.6096\n",
      "Epoch: 7, training loss: 0.6124\n",
      "Epoch 7, Val Loss: 0.5983\n",
      "Epoch: 8, training loss: 0.6052\n",
      "Epoch 8, Val Loss: 0.5889\n",
      "Epoch: 9, training loss: 0.5926\n",
      "Epoch 9, Val Loss: 0.5825\n",
      "Epoch: 10, training loss: 0.5942\n",
      "Epoch 10, Val Loss: 0.5770\n",
      "Fold 2:\n",
      "Epoch: 1, training loss: 0.7207\n",
      "Epoch 1, Val Loss: 0.6780\n",
      "Epoch: 2, training loss: 0.6794\n",
      "Epoch 2, Val Loss: 0.6572\n",
      "Epoch: 3, training loss: 0.6597\n",
      "Epoch 3, Val Loss: 0.6441\n",
      "Epoch: 4, training loss: 0.6457\n",
      "Epoch 4, Val Loss: 0.6304\n",
      "Epoch: 5, training loss: 0.6287\n",
      "Epoch 5, Val Loss: 0.6174\n",
      "Epoch: 6, training loss: 0.6145\n",
      "Epoch 6, Val Loss: 0.6060\n",
      "Epoch: 7, training loss: 0.5999\n",
      "Epoch 7, Val Loss: 0.6010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [07/Apr/2025 19:19:41] \"GET /api/studies/0?after=18 HTTP/1.1\" 200 9077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, training loss: 0.5933\n",
      "Epoch 8, Val Loss: 0.5944\n",
      "Epoch: 9, training loss: 0.5855\n",
      "Epoch 9, Val Loss: 0.5943\n",
      "Epoch: 10, training loss: 0.5804\n",
      "Epoch 10, Val Loss: 0.5953\n",
      "Fold 3:\n",
      "Epoch: 1, training loss: 0.7400\n",
      "Epoch 1, Val Loss: 0.6956\n",
      "Epoch: 2, training loss: 0.6832\n",
      "Epoch 2, Val Loss: 0.6682\n",
      "Epoch: 3, training loss: 0.6650\n",
      "Epoch 3, Val Loss: 0.6544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\optuna_dashboard\\_importance.py:96: ExperimentalWarning: PedAnovaImportanceEvaluator is experimental (supported from v3.6.0). The interface can change in the future.\n",
      "  study, target=target, evaluator=PedAnovaImportanceEvaluator()\n",
      "d:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\optuna\\importance\\_ped_anova\\evaluator.py:150: UserWarning: PedAnovaImportanceEvaluator computes the importances of params to achieve low `target` values. If this is not what you want, please modify target, e.g., by multiplying the output by -1.\n",
      "  warnings.warn(\n",
      "127.0.0.1 - - [07/Apr/2025 19:19:42] \"GET /api/studies/0/param_importances?evaluator=ped_anova HTTP/1.1\" 200 579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, training loss: 0.6484\n",
      "Epoch 4, Val Loss: 0.6394\n",
      "Epoch: 5, training loss: 0.6370\n",
      "Epoch 5, Val Loss: 0.6278\n",
      "Epoch: 6, training loss: 0.6150\n",
      "Epoch 6, Val Loss: 0.6166\n",
      "Epoch: 7, training loss: 0.5993\n",
      "Epoch 7, Val Loss: 0.6092\n",
      "Epoch: 8, training loss: 0.5940\n",
      "Epoch 8, Val Loss: 0.6044\n",
      "Epoch: 9, training loss: 0.5877\n",
      "Epoch 9, Val Loss: 0.6016\n",
      "Epoch: 10, training loss: 0.5835\n",
      "Epoch 10, Val Loss: 0.5988\n",
      "Fold 4:\n",
      "Epoch: 1, training loss: 0.8410\n",
      "Epoch 1, Val Loss: 0.7618\n",
      "Epoch: 2, training loss: 0.7150\n",
      "Epoch 2, Val Loss: 0.6818\n",
      "Epoch: 3, training loss: 0.6687\n",
      "Epoch 3, Val Loss: 0.6627\n",
      "Epoch: 4, training loss: 0.6525\n",
      "Epoch 4, Val Loss: 0.6455\n",
      "Epoch: 5, training loss: 0.6365\n",
      "Epoch 5, Val Loss: 0.6308\n",
      "Epoch: 6, training loss: 0.6208\n",
      "Epoch 6, Val Loss: 0.6133\n",
      "Epoch: 7, training loss: 0.6078\n",
      "Epoch 7, Val Loss: 0.6012\n",
      "Epoch: 8, training loss: 0.6018\n",
      "Epoch 8, Val Loss: 0.5911\n",
      "Epoch: 9, training loss: 0.5935\n",
      "Epoch 9, Val Loss: 0.5886\n",
      "Epoch: 10, training loss: 0.5857\n",
      "Epoch 10, Val Loss: 0.5863\n",
      "Fold 5:\n",
      "Epoch: 1, training loss: 0.7354\n",
      "Epoch 1, Val Loss: 0.6931\n",
      "Epoch: 2, training loss: 0.6730\n",
      "Epoch 2, Val Loss: 0.6708\n",
      "Epoch: 3, training loss: 0.6563\n",
      "Epoch 3, Val Loss: 0.6586\n",
      "Epoch: 4, training loss: 0.6462\n",
      "Epoch 4, Val Loss: 0.6466\n",
      "Epoch: 5, training loss: 0.6298\n",
      "Epoch 5, Val Loss: 0.6348\n",
      "Epoch: 6, training loss: 0.6146\n",
      "Epoch 6, Val Loss: 0.6262\n",
      "Epoch: 7, training loss: 0.6090\n",
      "Epoch 7, Val Loss: 0.6184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-07 19:19:45,066] Trial 20 finished with value: 0.322507835955913 and parameters: {'hidden_dim': 82, 'dropout': 0.1694563421994657, 'initial_lr': 4.585198147670415e-05, 'max_lr': 0.003782180862683939, 'criterion': 'BCEWithLogitsLoss', 'pos_weight': 3}. Best is trial 13 with value: 0.37735558639428074.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, training loss: 0.5990\n",
      "Epoch 8, Val Loss: 0.6106\n",
      "Epoch: 9, training loss: 0.5869\n",
      "Epoch 9, Val Loss: 0.6062\n",
      "Epoch: 10, training loss: 0.5879\n",
      "Epoch 10, Val Loss: 0.6051\n",
      "Using device: cuda\n",
      "Fold 1:\n",
      "Epoch: 1, training loss: 0.6729\n",
      "Epoch 1, Val Loss: 0.6243\n",
      "Epoch: 2, training loss: 0.5710\n",
      "Epoch 2, Val Loss: 0.5155\n",
      "Epoch: 3, training loss: 0.4644\n",
      "Epoch 3, Val Loss: 0.4158\n",
      "Epoch: 4, training loss: 0.3813\n",
      "Epoch 4, Val Loss: 0.3527\n",
      "Epoch: 5, training loss: 0.3421\n",
      "Epoch 5, Val Loss: 0.3345\n",
      "Epoch: 6, training loss: 0.3306\n",
      "Epoch 6, Val Loss: 0.3295\n",
      "Epoch: 7, training loss: 0.3290\n",
      "Epoch 7, Val Loss: 0.3251\n",
      "Epoch: 8, training loss: 0.3241\n",
      "Epoch 8, Val Loss: 0.3206\n",
      "Epoch: 9, training loss: 0.3203\n",
      "Epoch 9, Val Loss: 0.3169\n",
      "Epoch: 10, training loss: 0.3165\n",
      "Epoch 10, Val Loss: 0.3129\n",
      "Fold 2:\n",
      "Epoch: 1, training loss: 0.7975\n",
      "Epoch 1, Val Loss: 0.7430\n",
      "Epoch: 2, training loss: 0.6807\n",
      "Epoch 2, Val Loss: 0.6175\n",
      "Epoch: 3, training loss: 0.5506\n",
      "Epoch 3, Val Loss: 0.4854\n",
      "Epoch: 4, training loss: 0.4297\n",
      "Epoch 4, Val Loss: 0.3851\n",
      "Epoch: 5, training loss: 0.3595\n",
      "Epoch 5, Val Loss: 0.3448\n",
      "Epoch: 6, training loss: 0.3407\n",
      "Epoch 6, Val Loss: 0.3358\n",
      "Epoch: 7, training loss: 0.3340\n",
      "Epoch 7, Val Loss: 0.3320\n",
      "Epoch: 8, training loss: 0.3325\n",
      "Epoch 8, Val Loss: 0.3284\n",
      "Epoch: 9, training loss: 0.3258\n",
      "Epoch 9, Val Loss: 0.3246\n",
      "Epoch: 10, training loss: 0.3225\n",
      "Epoch 10, Val Loss: 0.3209\n",
      "Fold 3:\n",
      "Epoch: 1, training loss: 0.6247\n",
      "Epoch 1, Val Loss: 0.5786\n",
      "Epoch: 2, training loss: 0.5290\n",
      "Epoch 2, Val Loss: 0.4803\n",
      "Epoch: 3, training loss: 0.4392\n",
      "Epoch 3, Val Loss: 0.3960\n",
      "Epoch: 4, training loss: 0.3727\n",
      "Epoch 4, Val Loss: 0.3496\n",
      "Epoch: 5, training loss: 0.3440\n",
      "Epoch 5, Val Loss: 0.3344\n",
      "Epoch: 6, training loss: 0.3342\n",
      "Epoch 6, Val Loss: 0.3298\n",
      "Epoch: 7, training loss: 0.3311\n",
      "Epoch 7, Val Loss: 0.3265\n",
      "Epoch: 8, training loss: 0.3248\n",
      "Epoch 8, Val Loss: 0.3233\n",
      "Epoch: 9, training loss: 0.3226\n",
      "Epoch 9, Val Loss: 0.3200\n",
      "Epoch: 10, training loss: 0.3172\n",
      "Epoch 10, Val Loss: 0.3170\n",
      "Fold 4:\n",
      "Epoch: 1, training loss: 0.7354\n",
      "Epoch 1, Val Loss: 0.6592\n",
      "Epoch: 2, training loss: 0.5894\n",
      "Epoch 2, Val Loss: 0.5080\n",
      "Epoch: 3, training loss: 0.4558\n",
      "Epoch 3, Val Loss: 0.4001\n",
      "Epoch: 4, training loss: 0.3752\n",
      "Epoch 4, Val Loss: 0.3476\n",
      "Epoch: 5, training loss: 0.3424\n",
      "Epoch 5, Val Loss: 0.3328\n",
      "Epoch: 6, training loss: 0.3325\n",
      "Epoch 6, Val Loss: 0.3287\n",
      "Epoch: 7, training loss: 0.3317\n",
      "Epoch 7, Val Loss: 0.3249\n",
      "Epoch: 8, training loss: 0.3257\n",
      "Epoch 8, Val Loss: 0.3213\n",
      "Epoch: 9, training loss: 0.3218\n",
      "Epoch 9, Val Loss: 0.3173\n",
      "Epoch: 10, training loss: 0.3177\n",
      "Epoch 10, Val Loss: 0.3132\n",
      "Fold 5:\n",
      "Epoch: 1, training loss: 0.6045\n",
      "Epoch 1, Val Loss: 0.5654\n",
      "Epoch: 2, training loss: 0.5180\n",
      "Epoch 2, Val Loss: 0.4710\n",
      "Epoch: 3, training loss: 0.4292\n",
      "Epoch 3, Val Loss: 0.3907\n",
      "Epoch: 4, training loss: 0.3661\n",
      "Epoch 4, Val Loss: 0.3469\n",
      "Epoch: 5, training loss: 0.3406\n",
      "Epoch 5, Val Loss: 0.3350\n",
      "Epoch: 6, training loss: 0.3320\n",
      "Epoch 6, Val Loss: 0.3316\n",
      "Epoch: 7, training loss: 0.3271\n",
      "Epoch 7, Val Loss: 0.3287\n",
      "Epoch: 8, training loss: 0.3263\n",
      "Epoch 8, Val Loss: 0.3264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-07 19:19:50,540] Trial 21 finished with value: 0.37716486079509115 and parameters: {'hidden_dim': 66, 'dropout': 0.14801192993860665, 'initial_lr': 6.404025489165365e-05, 'max_lr': 0.001734493323693035, 'criterion': 'BCEWithLogitsLoss', 'pos_weight': 1}. Best is trial 13 with value: 0.37735558639428074.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, training loss: 0.3221\n",
      "Epoch 9, Val Loss: 0.3240\n",
      "Epoch: 10, training loss: 0.3165\n",
      "Epoch 10, Val Loss: 0.3214\n",
      "Using device: cuda\n",
      "Fold 1:\n",
      "Epoch: 1, training loss: 0.6022\n",
      "Epoch 1, Val Loss: 0.5781\n",
      "Epoch: 2, training loss: 0.5592\n",
      "Epoch 2, Val Loss: 0.5404\n",
      "Epoch: 3, training loss: 0.5290\n",
      "Epoch 3, Val Loss: 0.5238\n",
      "Epoch: 4, training loss: 0.5208\n",
      "Epoch 4, Val Loss: 0.5166\n",
      "Epoch: 5, training loss: 0.5135\n",
      "Epoch 5, Val Loss: 0.5102\n",
      "Epoch: 6, training loss: 0.5088\n",
      "Epoch 6, Val Loss: 0.5026\n",
      "Epoch: 7, training loss: 0.5016\n",
      "Epoch 7, Val Loss: 0.4954\n",
      "Epoch: 8, training loss: 0.4945\n",
      "Epoch 8, Val Loss: 0.4880\n",
      "Epoch: 9, training loss: 0.4896\n",
      "Epoch 9, Val Loss: 0.4796\n",
      "Epoch: 10, training loss: 0.4832\n",
      "Epoch 10, Val Loss: 0.4724\n",
      "Fold 2:\n",
      "Epoch: 1, training loss: 0.6638\n",
      "Epoch 1, Val Loss: 0.6260\n",
      "Epoch: 2, training loss: 0.5969\n",
      "Epoch 2, Val Loss: 0.5638\n",
      "Epoch: 3, training loss: 0.5515\n",
      "Epoch 3, Val Loss: 0.5320\n",
      "Epoch: 4, training loss: 0.5274\n",
      "Epoch 4, Val Loss: 0.5201\n",
      "Epoch: 5, training loss: 0.5205\n",
      "Epoch 5, Val Loss: 0.5119\n",
      "Epoch: 6, training loss: 0.5096\n",
      "Epoch 6, Val Loss: 0.5032\n",
      "Epoch: 7, training loss: 0.5005\n",
      "Epoch 7, Val Loss: 0.4963\n",
      "Epoch: 8, training loss: 0.4938\n",
      "Epoch 8, Val Loss: 0.4881\n",
      "Epoch: 9, training loss: 0.4829\n",
      "Epoch 9, Val Loss: 0.4815\n",
      "Epoch: 10, training loss: 0.4765\n",
      "Epoch 10, Val Loss: 0.4762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [07/Apr/2025 19:19:52] \"GET /api/studies/0?after=20 HTTP/1.1\" 200 9073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3:\n",
      "Epoch: 1, training loss: 0.6830\n",
      "Epoch 1, Val Loss: 0.6379\n",
      "Epoch: 2, training loss: 0.6073\n",
      "Epoch 2, Val Loss: 0.5666\n",
      "Epoch: 3, training loss: 0.5527\n",
      "Epoch 3, Val Loss: 0.5304\n",
      "Epoch: 4, training loss: 0.5281\n",
      "Epoch 4, Val Loss: 0.5181\n",
      "Epoch: 5, training loss: 0.5204\n",
      "Epoch 5, Val Loss: 0.5120\n",
      "Epoch: 6, training loss: 0.5123\n",
      "Epoch 6, Val Loss: 0.5063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\optuna_dashboard\\_importance.py:96: ExperimentalWarning: PedAnovaImportanceEvaluator is experimental (supported from v3.6.0). The interface can change in the future.\n",
      "  study, target=target, evaluator=PedAnovaImportanceEvaluator()\n",
      "d:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\optuna\\importance\\_ped_anova\\evaluator.py:150: UserWarning: PedAnovaImportanceEvaluator computes the importances of params to achieve low `target` values. If this is not what you want, please modify target, e.g., by multiplying the output by -1.\n",
      "  warnings.warn(\n",
      "127.0.0.1 - - [07/Apr/2025 19:19:53] \"GET /api/studies/0/param_importances?evaluator=ped_anova HTTP/1.1\" 200 578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, training loss: 0.5070\n",
      "Epoch 7, Val Loss: 0.5007\n",
      "Epoch: 8, training loss: 0.4974\n",
      "Epoch 8, Val Loss: 0.4951\n",
      "Epoch: 9, training loss: 0.4865\n",
      "Epoch 9, Val Loss: 0.4896\n",
      "Epoch: 10, training loss: 0.4830\n",
      "Epoch 10, Val Loss: 0.4845\n",
      "Fold 4:\n",
      "Epoch: 1, training loss: 0.6263\n",
      "Epoch 1, Val Loss: 0.5932\n",
      "Epoch: 2, training loss: 0.5744\n",
      "Epoch 2, Val Loss: 0.5454\n",
      "Epoch: 3, training loss: 0.5359\n",
      "Epoch 3, Val Loss: 0.5241\n",
      "Epoch: 4, training loss: 0.5236\n",
      "Epoch 4, Val Loss: 0.5175\n",
      "Epoch: 5, training loss: 0.5191\n",
      "Epoch 5, Val Loss: 0.5120\n",
      "Epoch: 6, training loss: 0.5132\n",
      "Epoch 6, Val Loss: 0.5057\n",
      "Epoch: 7, training loss: 0.5048\n",
      "Epoch 7, Val Loss: 0.4992\n",
      "Epoch: 8, training loss: 0.4971\n",
      "Epoch 8, Val Loss: 0.4929\n",
      "Epoch: 9, training loss: 0.4909\n",
      "Epoch 9, Val Loss: 0.4863\n",
      "Epoch: 10, training loss: 0.4881\n",
      "Epoch 10, Val Loss: 0.4796\n",
      "Fold 5:\n",
      "Epoch: 1, training loss: 0.8068\n",
      "Epoch 1, Val Loss: 0.7544\n",
      "Epoch: 2, training loss: 0.6999\n",
      "Epoch 2, Val Loss: 0.6479\n",
      "Epoch: 3, training loss: 0.6056\n",
      "Epoch 3, Val Loss: 0.5708\n",
      "Epoch: 4, training loss: 0.5521\n",
      "Epoch 4, Val Loss: 0.5379\n",
      "Epoch: 5, training loss: 0.5281\n",
      "Epoch 5, Val Loss: 0.5279\n",
      "Epoch: 6, training loss: 0.5179\n",
      "Epoch 6, Val Loss: 0.5221\n",
      "Epoch: 7, training loss: 0.5133\n",
      "Epoch 7, Val Loss: 0.5167\n",
      "Epoch: 8, training loss: 0.5040\n",
      "Epoch 8, Val Loss: 0.5112\n",
      "Epoch: 9, training loss: 0.4973\n",
      "Epoch 9, Val Loss: 0.5060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-07 19:19:55,239] Trial 22 finished with value: 0.37716486079509115 and parameters: {'hidden_dim': 76, 'dropout': 0.18569777338876553, 'initial_lr': 0.00015455837266170388, 'max_lr': 0.001803201020836854, 'criterion': 'BCEWithLogitsLoss', 'pos_weight': 2}. Best is trial 13 with value: 0.37735558639428074.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, training loss: 0.4873\n",
      "Epoch 10, Val Loss: 0.5005\n",
      "Using device: cuda\n",
      "Fold 1:\n",
      "Epoch: 1, training loss: 0.6508\n",
      "Epoch 1, Val Loss: 0.4982\n",
      "Epoch: 2, training loss: 0.4041\n",
      "Epoch 2, Val Loss: 0.3417\n",
      "Epoch: 3, training loss: 0.3342\n",
      "Epoch 3, Val Loss: 0.3287\n",
      "Epoch: 4, training loss: 0.3281\n",
      "Epoch 4, Val Loss: 0.3212\n",
      "Epoch: 5, training loss: 0.3236\n",
      "Epoch 5, Val Loss: 0.3144\n",
      "Epoch: 6, training loss: 0.3148\n",
      "Epoch 6, Val Loss: 0.3078\n",
      "Epoch: 7, training loss: 0.3062\n",
      "Epoch 7, Val Loss: 0.3019\n",
      "Epoch: 8, training loss: 0.3020\n",
      "Epoch 8, Val Loss: 0.2966\n",
      "Epoch: 9, training loss: 0.2991\n",
      "Epoch 9, Val Loss: 0.2947\n",
      "Epoch: 10, training loss: 0.2966\n",
      "Epoch 10, Val Loss: 0.2893\n",
      "Fold 2:\n",
      "Epoch: 1, training loss: 0.4979\n",
      "Epoch 1, Val Loss: 0.4046\n",
      "Epoch: 2, training loss: 0.3631\n",
      "Epoch 2, Val Loss: 0.3355\n",
      "Epoch: 3, training loss: 0.3343\n",
      "Epoch 3, Val Loss: 0.3288\n",
      "Epoch: 4, training loss: 0.3273\n",
      "Epoch 4, Val Loss: 0.3226\n",
      "Epoch: 5, training loss: 0.3221\n",
      "Epoch 5, Val Loss: 0.3159\n",
      "Epoch: 6, training loss: 0.3129\n",
      "Epoch 6, Val Loss: 0.3097\n",
      "Epoch: 7, training loss: 0.3051\n",
      "Epoch 7, Val Loss: 0.3032\n",
      "Epoch: 8, training loss: 0.2990\n",
      "Epoch 8, Val Loss: 0.2979\n",
      "Epoch: 9, training loss: 0.2942\n",
      "Epoch 9, Val Loss: 0.2932\n",
      "Epoch: 10, training loss: 0.2901\n",
      "Epoch 10, Val Loss: 0.2911\n",
      "Fold 3:\n",
      "Epoch: 1, training loss: 0.6305\n",
      "Epoch 1, Val Loss: 0.5048\n",
      "Epoch: 2, training loss: 0.4179\n",
      "Epoch 2, Val Loss: 0.3453\n",
      "Epoch: 3, training loss: 0.3356\n",
      "Epoch 3, Val Loss: 0.3258\n",
      "Epoch: 4, training loss: 0.3280\n",
      "Epoch 4, Val Loss: 0.3209\n",
      "Epoch: 5, training loss: 0.3218\n",
      "Epoch 5, Val Loss: 0.3156\n",
      "Epoch: 6, training loss: 0.3132\n",
      "Epoch 6, Val Loss: 0.3102\n",
      "Epoch: 7, training loss: 0.3057\n",
      "Epoch 7, Val Loss: 0.3044\n",
      "Epoch: 8, training loss: 0.3021\n",
      "Epoch 8, Val Loss: 0.3008\n",
      "Epoch: 9, training loss: 0.2958\n",
      "Epoch 9, Val Loss: 0.3006\n",
      "Epoch: 10, training loss: 0.2941\n",
      "Epoch 10, Val Loss: 0.2969\n",
      "Fold 4:\n",
      "Epoch: 1, training loss: 0.5216\n",
      "Epoch 1, Val Loss: 0.4150\n",
      "Epoch: 2, training loss: 0.3614\n",
      "Epoch 2, Val Loss: 0.3340\n",
      "Epoch: 3, training loss: 0.3306\n",
      "Epoch 3, Val Loss: 0.3279\n",
      "Epoch: 4, training loss: 0.3253\n",
      "Epoch 4, Val Loss: 0.3213\n",
      "Epoch: 5, training loss: 0.3197\n",
      "Epoch 5, Val Loss: 0.3146\n",
      "Epoch: 6, training loss: 0.3128\n",
      "Epoch 6, Val Loss: 0.3092\n",
      "Epoch: 7, training loss: 0.3064\n",
      "Epoch 7, Val Loss: 0.3008\n",
      "Epoch: 8, training loss: 0.2981\n",
      "Epoch 8, Val Loss: 0.2939\n",
      "Epoch: 9, training loss: 0.2931\n",
      "Epoch 9, Val Loss: 0.2906\n",
      "Epoch: 10, training loss: 0.2892\n",
      "Epoch 10, Val Loss: 0.2886\n",
      "Fold 5:\n",
      "Epoch: 1, training loss: 0.5057\n",
      "Epoch 1, Val Loss: 0.4089\n",
      "Epoch: 2, training loss: 0.3609\n",
      "Epoch 2, Val Loss: 0.3333\n",
      "Epoch: 3, training loss: 0.3291\n",
      "Epoch 3, Val Loss: 0.3267\n",
      "Epoch: 4, training loss: 0.3220\n",
      "Epoch 4, Val Loss: 0.3212\n",
      "Epoch: 5, training loss: 0.3170\n",
      "Epoch 5, Val Loss: 0.3152\n",
      "Epoch: 6, training loss: 0.3113\n",
      "Epoch 6, Val Loss: 0.3089\n",
      "Epoch: 7, training loss: 0.3043\n",
      "Epoch 7, Val Loss: 0.3038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-07 19:20:00,039] Trial 23 finished with value: 0.37677795873322417 and parameters: {'hidden_dim': 100, 'dropout': 0.24985914115108893, 'initial_lr': 2.3346902046709897e-05, 'max_lr': 0.004621759587027566, 'criterion': 'BCEWithLogitsLoss', 'pos_weight': 1}. Best is trial 13 with value: 0.37735558639428074.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, training loss: 0.2989\n",
      "Epoch 8, Val Loss: 0.3011\n",
      "Epoch: 9, training loss: 0.2959\n",
      "Epoch 9, Val Loss: 0.2962\n",
      "Epoch: 10, training loss: 0.2934\n",
      "Epoch 10, Val Loss: 0.2972\n",
      "Using device: cuda\n",
      "Fold 1:\n",
      "Epoch: 1, training loss: 0.8382\n",
      "Epoch 1, Val Loss: 0.8074\n",
      "Epoch: 2, training loss: 0.7771\n",
      "Epoch 2, Val Loss: 0.7476\n",
      "Epoch: 3, training loss: 0.7246\n",
      "Epoch 3, Val Loss: 0.7040\n",
      "Epoch: 4, training loss: 0.6920\n",
      "Epoch 4, Val Loss: 0.6829\n",
      "Epoch: 5, training loss: 0.6759\n",
      "Epoch 5, Val Loss: 0.6731\n",
      "Epoch: 6, training loss: 0.6677\n",
      "Epoch 6, Val Loss: 0.6643\n",
      "Epoch: 7, training loss: 0.6605\n",
      "Epoch 7, Val Loss: 0.6545\n",
      "Epoch: 8, training loss: 0.6485\n",
      "Epoch 8, Val Loss: 0.6437\n",
      "Epoch: 9, training loss: 0.6419\n",
      "Epoch 9, Val Loss: 0.6321\n",
      "Epoch: 10, training loss: 0.6296\n",
      "Epoch 10, Val Loss: 0.6210\n",
      "Fold 2:\n",
      "Epoch: 1, training loss: 0.7931\n",
      "Epoch 1, Val Loss: 0.7701\n",
      "Epoch: 2, training loss: 0.7444\n",
      "Epoch 2, Val Loss: 0.7244\n",
      "Epoch: 3, training loss: 0.7087\n",
      "Epoch 3, Val Loss: 0.6970\n",
      "Epoch: 4, training loss: 0.6872\n",
      "Epoch 4, Val Loss: 0.6832\n",
      "Epoch: 5, training loss: 0.6799\n",
      "Epoch 5, Val Loss: 0.6735\n",
      "Epoch: 6, training loss: 0.6667\n",
      "Epoch 6, Val Loss: 0.6643\n",
      "Epoch: 7, training loss: 0.6580\n",
      "Epoch 7, Val Loss: 0.6558\n",
      "Epoch: 8, training loss: 0.6448\n",
      "Epoch 8, Val Loss: 0.6466\n",
      "Epoch: 9, training loss: 0.6377\n",
      "Epoch 9, Val Loss: 0.6381\n",
      "Epoch: 10, training loss: 0.6289\n",
      "Epoch 10, Val Loss: 0.6300\n",
      "Fold 3:\n",
      "Epoch: 1, training loss: 0.8077\n",
      "Epoch 1, Val Loss: 0.7812\n",
      "Epoch: 2, training loss: 0.7555\n",
      "Epoch 2, Val Loss: 0.7289\n",
      "Epoch: 3, training loss: 0.7080\n",
      "Epoch 3, Val Loss: 0.6926\n",
      "Epoch: 4, training loss: 0.6841\n",
      "Epoch 4, Val Loss: 0.6753\n",
      "Epoch: 5, training loss: 0.6712\n",
      "Epoch 5, Val Loss: 0.6663\n",
      "Epoch: 6, training loss: 0.6620\n",
      "Epoch 6, Val Loss: 0.6583\n",
      "Epoch: 7, training loss: 0.6519\n",
      "Epoch 7, Val Loss: 0.6504\n",
      "Epoch: 8, training loss: 0.6399\n",
      "Epoch 8, Val Loss: 0.6434\n",
      "Epoch: 9, training loss: 0.6337\n",
      "Epoch 9, Val Loss: 0.6349\n",
      "Epoch: 10, training loss: 0.6191\n",
      "Epoch 10, Val Loss: 0.6287\n",
      "Fold 4:\n",
      "Epoch: 1, training loss: 0.7691\n",
      "Epoch 1, Val Loss: 0.7367\n",
      "Epoch: 2, training loss: 0.7183\n",
      "Epoch 2, Val Loss: 0.6964\n",
      "Epoch: 3, training loss: 0.6887\n",
      "Epoch 3, Val Loss: 0.6770\n",
      "Epoch: 4, training loss: 0.6730\n",
      "Epoch 4, Val Loss: 0.6674\n",
      "Epoch: 5, training loss: 0.6664\n",
      "Epoch 5, Val Loss: 0.6582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [07/Apr/2025 19:20:03] \"GET /api/studies/0?after=22 HTTP/1.1\" 200 9079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, training loss: 0.6583\n",
      "Epoch 6, Val Loss: 0.6491\n",
      "Epoch: 7, training loss: 0.6473\n",
      "Epoch 7, Val Loss: 0.6386\n",
      "Epoch: 8, training loss: 0.6342\n",
      "Epoch 8, Val Loss: 0.6289\n",
      "Epoch: 9, training loss: 0.6230\n",
      "Epoch 9, Val Loss: 0.6187\n",
      "Epoch: 10, training loss: 0.6149\n",
      "Epoch 10, Val Loss: 0.6098\n",
      "Fold 5:\n",
      "Epoch: 1, training loss: 0.7828\n",
      "Epoch 1, Val Loss: 0.7542\n",
      "Epoch: 2, training loss: 0.7335\n",
      "Epoch 2, Val Loss: 0.7106\n",
      "Epoch: 3, training loss: 0.6953\n",
      "Epoch 3, Val Loss: 0.6872\n",
      "Epoch: 4, training loss: 0.6781\n",
      "Epoch 4, Val Loss: 0.6767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\optuna_dashboard\\_importance.py:96: ExperimentalWarning: PedAnovaImportanceEvaluator is experimental (supported from v3.6.0). The interface can change in the future.\n",
      "  study, target=target, evaluator=PedAnovaImportanceEvaluator()\n",
      "d:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\optuna\\importance\\_ped_anova\\evaluator.py:150: UserWarning: PedAnovaImportanceEvaluator computes the importances of params to achieve low `target` values. If this is not what you want, please modify target, e.g., by multiplying the output by -1.\n",
      "  warnings.warn(\n",
      "127.0.0.1 - - [07/Apr/2025 19:20:04] \"GET /api/studies/0/param_importances?evaluator=ped_anova HTTP/1.1\" 200 581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, training loss: 0.6675\n",
      "Epoch 5, Val Loss: 0.6686\n",
      "Epoch: 6, training loss: 0.6584\n",
      "Epoch 6, Val Loss: 0.6599\n",
      "Epoch: 7, training loss: 0.6461\n",
      "Epoch 7, Val Loss: 0.6516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-07 19:20:04,705] Trial 24 finished with value: 0.3731205186108101 and parameters: {'hidden_dim': 67, 'dropout': 0.143063972356117, 'initial_lr': 5.584034529966907e-05, 'max_lr': 0.0016654021679331815, 'criterion': 'BCEWithLogitsLoss', 'pos_weight': 3}. Best is trial 13 with value: 0.37735558639428074.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, training loss: 0.6383\n",
      "Epoch 8, Val Loss: 0.6432\n",
      "Epoch: 9, training loss: 0.6248\n",
      "Epoch 9, Val Loss: 0.6361\n",
      "Epoch: 10, training loss: 0.6140\n",
      "Epoch 10, Val Loss: 0.6292\n",
      "Using device: cuda\n",
      "Fold 1:\n",
      "Epoch: 1, training loss: 0.8391\n",
      "Epoch 1, Val Loss: 0.7791\n",
      "Epoch: 2, training loss: 0.7372\n",
      "Epoch 2, Val Loss: 0.7038\n",
      "Epoch: 3, training loss: 0.6871\n",
      "Epoch 3, Val Loss: 0.6797\n",
      "Epoch: 4, training loss: 0.6748\n",
      "Epoch 4, Val Loss: 0.6644\n",
      "Epoch: 5, training loss: 0.6597\n",
      "Epoch 5, Val Loss: 0.6497\n",
      "Epoch: 6, training loss: 0.6476\n",
      "Epoch 6, Val Loss: 0.6338\n",
      "Epoch: 7, training loss: 0.6365\n",
      "Epoch 7, Val Loss: 0.6193\n",
      "Epoch: 8, training loss: 0.6218\n",
      "Epoch 8, Val Loss: 0.6075\n",
      "Epoch: 9, training loss: 0.6131\n",
      "Epoch 9, Val Loss: 0.5977\n",
      "Epoch: 10, training loss: 0.6079\n",
      "Epoch 10, Val Loss: 0.5910\n",
      "Fold 2:\n",
      "Epoch: 1, training loss: 0.8503\n",
      "Epoch 1, Val Loss: 0.7989\n",
      "Epoch: 2, training loss: 0.7630\n",
      "Epoch 2, Val Loss: 0.7238\n",
      "Epoch: 3, training loss: 0.7073\n",
      "Epoch 3, Val Loss: 0.6890\n",
      "Epoch: 4, training loss: 0.6849\n",
      "Epoch 4, Val Loss: 0.6745\n",
      "Epoch: 5, training loss: 0.6731\n",
      "Epoch 5, Val Loss: 0.6630\n",
      "Epoch: 6, training loss: 0.6589\n",
      "Epoch 6, Val Loss: 0.6520\n",
      "Epoch: 7, training loss: 0.6451\n",
      "Epoch 7, Val Loss: 0.6409\n",
      "Epoch: 8, training loss: 0.6314\n",
      "Epoch 8, Val Loss: 0.6263\n",
      "Epoch: 9, training loss: 0.6155\n",
      "Epoch 9, Val Loss: 0.6141\n",
      "Epoch: 10, training loss: 0.6095\n",
      "Epoch 10, Val Loss: 0.6036\n",
      "Fold 3:\n",
      "Epoch: 1, training loss: 0.7553\n",
      "Epoch 1, Val Loss: 0.7197\n",
      "Epoch: 2, training loss: 0.6988\n",
      "Epoch 2, Val Loss: 0.6792\n",
      "Epoch: 3, training loss: 0.6774\n",
      "Epoch 3, Val Loss: 0.6668\n",
      "Epoch: 4, training loss: 0.6611\n",
      "Epoch 4, Val Loss: 0.6578\n",
      "Epoch: 5, training loss: 0.6553\n",
      "Epoch 5, Val Loss: 0.6483\n",
      "Epoch: 6, training loss: 0.6403\n",
      "Epoch 6, Val Loss: 0.6385\n",
      "Epoch: 7, training loss: 0.6272\n",
      "Epoch 7, Val Loss: 0.6302\n",
      "Epoch: 8, training loss: 0.6189\n",
      "Epoch 8, Val Loss: 0.6225\n",
      "Epoch: 9, training loss: 0.6081\n",
      "Epoch 9, Val Loss: 0.6158\n",
      "Epoch: 10, training loss: 0.5987\n",
      "Epoch 10, Val Loss: 0.6102\n",
      "Fold 4:\n",
      "Epoch: 1, training loss: 0.8627\n",
      "Epoch 1, Val Loss: 0.8187\n",
      "Epoch: 2, training loss: 0.7691\n",
      "Epoch 2, Val Loss: 0.7316\n",
      "Epoch: 3, training loss: 0.6997\n",
      "Epoch 3, Val Loss: 0.6895\n",
      "Epoch: 4, training loss: 0.6752\n",
      "Epoch 4, Val Loss: 0.6735\n",
      "Epoch: 5, training loss: 0.6602\n",
      "Epoch 5, Val Loss: 0.6602\n",
      "Epoch: 6, training loss: 0.6519\n",
      "Epoch 6, Val Loss: 0.6487\n",
      "Epoch: 7, training loss: 0.6376\n",
      "Epoch 7, Val Loss: 0.6350\n",
      "Epoch: 8, training loss: 0.6279\n",
      "Epoch 8, Val Loss: 0.6243\n",
      "Epoch: 9, training loss: 0.6158\n",
      "Epoch 9, Val Loss: 0.6140\n",
      "Epoch: 10, training loss: 0.6104\n",
      "Epoch 10, Val Loss: 0.6048\n",
      "Fold 5:\n",
      "Epoch: 1, training loss: 0.8521\n",
      "Epoch 1, Val Loss: 0.8040\n",
      "Epoch: 2, training loss: 0.7680\n",
      "Epoch 2, Val Loss: 0.7294\n",
      "Epoch: 3, training loss: 0.7074\n",
      "Epoch 3, Val Loss: 0.6951\n",
      "Epoch: 4, training loss: 0.6783\n",
      "Epoch 4, Val Loss: 0.6829\n",
      "Epoch: 5, training loss: 0.6670\n",
      "Epoch 5, Val Loss: 0.6720\n",
      "Epoch: 6, training loss: 0.6550\n",
      "Epoch 6, Val Loss: 0.6610\n",
      "Epoch: 7, training loss: 0.6386\n",
      "Epoch 7, Val Loss: 0.6523\n",
      "Epoch: 8, training loss: 0.6295\n",
      "Epoch 8, Val Loss: 0.6442\n",
      "Epoch: 9, training loss: 0.6190\n",
      "Epoch 9, Val Loss: 0.6353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-07 19:20:10,229] Trial 25 finished with value: 0.3612850838551268 and parameters: {'hidden_dim': 54, 'dropout': 0.19986451737472227, 'initial_lr': 0.00010413779226844778, 'max_lr': 0.002930919341631058, 'criterion': 'BCEWithLogitsLoss', 'pos_weight': 3}. Best is trial 13 with value: 0.37735558639428074.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, training loss: 0.6093\n",
      "Epoch 10, Val Loss: 0.6271\n",
      "Using device: cuda\n",
      "Fold 1:\n",
      "Epoch: 1, training loss: 1.0581\n",
      "Epoch 1, Val Loss: 1.0336\n",
      "Epoch: 2, training loss: 1.0225\n",
      "Epoch 2, Val Loss: 0.9994\n",
      "Epoch: 3, training loss: 0.9971\n",
      "Epoch 3, Val Loss: 0.9815\n",
      "Epoch: 4, training loss: 0.9802\n",
      "Epoch 4, Val Loss: 0.9651\n",
      "Epoch: 5, training loss: 0.9648\n",
      "Epoch 5, Val Loss: 0.9499\n",
      "Epoch: 6, training loss: 0.9543\n",
      "Epoch 6, Val Loss: 0.9343\n",
      "Epoch: 7, training loss: 0.9360\n",
      "Epoch 7, Val Loss: 0.9169\n",
      "Epoch: 8, training loss: 0.9235\n",
      "Epoch 8, Val Loss: 0.9022\n",
      "Epoch: 9, training loss: 0.9121\n",
      "Epoch 9, Val Loss: 0.8895\n",
      "Epoch: 10, training loss: 0.8995\n",
      "Epoch 10, Val Loss: 0.8794\n",
      "Fold 2:\n",
      "Epoch: 1, training loss: 1.0197\n",
      "Epoch 1, Val Loss: 1.0065\n",
      "Epoch: 2, training loss: 1.0050\n",
      "Epoch 2, Val Loss: 0.9974\n",
      "Epoch: 3, training loss: 0.9901\n",
      "Epoch 3, Val Loss: 0.9868\n",
      "Epoch: 4, training loss: 0.9809\n",
      "Epoch 4, Val Loss: 0.9744\n",
      "Epoch: 5, training loss: 0.9706\n",
      "Epoch 5, Val Loss: 0.9619\n",
      "Epoch: 6, training loss: 0.9536\n",
      "Epoch 6, Val Loss: 0.9487\n",
      "Epoch: 7, training loss: 0.9408\n",
      "Epoch 7, Val Loss: 0.9337\n",
      "Epoch: 8, training loss: 0.9217\n",
      "Epoch 8, Val Loss: 0.9211\n",
      "Epoch: 9, training loss: 0.9088\n",
      "Epoch 9, Val Loss: 0.9089\n",
      "Epoch: 10, training loss: 0.8951\n",
      "Epoch 10, Val Loss: 0.8996\n",
      "Fold 3:\n",
      "Epoch: 1, training loss: 1.0409\n",
      "Epoch 1, Val Loss: 1.0239\n",
      "Epoch: 2, training loss: 1.0180\n",
      "Epoch 2, Val Loss: 1.0033\n",
      "Epoch: 3, training loss: 0.9981\n",
      "Epoch 3, Val Loss: 0.9911\n",
      "Epoch: 4, training loss: 0.9945\n",
      "Epoch 4, Val Loss: 0.9791\n",
      "Epoch: 5, training loss: 0.9750\n",
      "Epoch 5, Val Loss: 0.9667\n",
      "Epoch: 6, training loss: 0.9625\n",
      "Epoch 6, Val Loss: 0.9541\n",
      "Epoch: 7, training loss: 0.9430\n",
      "Epoch 7, Val Loss: 0.9422\n",
      "Epoch: 8, training loss: 0.9299\n",
      "Epoch 8, Val Loss: 0.9309\n",
      "Epoch: 9, training loss: 0.9119\n",
      "Epoch 9, Val Loss: 0.9200\n",
      "Epoch: 10, training loss: 0.8992\n",
      "Epoch 10, Val Loss: 0.9107\n",
      "Fold 4:\n",
      "Epoch: 1, training loss: 1.0152\n",
      "Epoch 1, Val Loss: 1.0093\n",
      "Epoch: 2, training loss: 1.0067\n",
      "Epoch 2, Val Loss: 0.9992\n",
      "Epoch: 3, training loss: 0.9929\n",
      "Epoch 3, Val Loss: 0.9876\n",
      "Epoch: 4, training loss: 0.9849\n",
      "Epoch 4, Val Loss: 0.9738\n",
      "Epoch: 5, training loss: 0.9619\n",
      "Epoch 5, Val Loss: 0.9597\n",
      "Epoch: 6, training loss: 0.9544\n",
      "Epoch 6, Val Loss: 0.9450\n",
      "Epoch: 7, training loss: 0.9355\n",
      "Epoch 7, Val Loss: 0.9298\n",
      "Epoch: 8, training loss: 0.9220\n",
      "Epoch 8, Val Loss: 0.9168\n",
      "Epoch: 9, training loss: 0.9076\n",
      "Epoch 9, Val Loss: 0.9049\n",
      "Epoch: 10, training loss: 0.9013\n",
      "Epoch 10, Val Loss: 0.8937\n",
      "Fold 5:\n",
      "Epoch: 1, training loss: 1.0244\n",
      "Epoch 1, Val Loss: 1.0230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [07/Apr/2025 19:20:14] \"GET /api/studies/0?after=24 HTTP/1.1\" 200 9075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, training loss: 1.0180\n",
      "Epoch 2, Val Loss: 1.0133\n",
      "Epoch: 3, training loss: 1.0032\n",
      "Epoch 3, Val Loss: 1.0025\n",
      "Epoch: 4, training loss: 0.9907\n",
      "Epoch 4, Val Loss: 0.9908\n",
      "Epoch: 5, training loss: 0.9788\n",
      "Epoch 5, Val Loss: 0.9777\n",
      "Epoch: 6, training loss: 0.9622\n",
      "Epoch 6, Val Loss: 0.9640\n",
      "Epoch: 7, training loss: 0.9424\n",
      "Epoch 7, Val Loss: 0.9500\n",
      "Epoch: 8, training loss: 0.9269\n",
      "Epoch 8, Val Loss: 0.9369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\optuna_dashboard\\_importance.py:96: ExperimentalWarning: PedAnovaImportanceEvaluator is experimental (supported from v3.6.0). The interface can change in the future.\n",
      "  study, target=target, evaluator=PedAnovaImportanceEvaluator()\n",
      "d:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\optuna\\importance\\_ped_anova\\evaluator.py:150: UserWarning: PedAnovaImportanceEvaluator computes the importances of params to achieve low `target` values. If this is not what you want, please modify target, e.g., by multiplying the output by -1.\n",
      "  warnings.warn(\n",
      "127.0.0.1 - - [07/Apr/2025 19:20:15] \"GET /api/studies/0/param_importances?evaluator=ped_anova HTTP/1.1\" 200 579\n",
      "[I 2025-04-07 19:20:15,353] Trial 26 finished with value: 0.27118560702479083 and parameters: {'hidden_dim': 84, 'dropout': 0.2369128285633994, 'initial_lr': 2.7245530928052767e-05, 'max_lr': 0.0017165721287832837, 'criterion': 'BCEWithLogitsLoss', 'pos_weight': 6}. Best is trial 13 with value: 0.37735558639428074.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, training loss: 0.9131\n",
      "Epoch 9, Val Loss: 0.9273\n",
      "Epoch: 10, training loss: 0.9070\n",
      "Epoch 10, Val Loss: 0.9141\n",
      "Using device: cuda\n",
      "Fold 1:\n",
      "Epoch: 1, training loss: 0.5047\n",
      "Epoch 1, Val Loss: 0.3615\n",
      "Epoch: 2, training loss: 0.3408\n",
      "Epoch 2, Val Loss: 0.3299\n",
      "Epoch: 3, training loss: 0.3256\n",
      "Epoch 3, Val Loss: 0.3175\n",
      "Epoch: 4, training loss: 0.3156\n",
      "Epoch 4, Val Loss: 0.3082\n",
      "Epoch: 5, training loss: 0.3081\n",
      "Epoch 5, Val Loss: 0.2985\n",
      "Epoch: 6, training loss: 0.3000\n",
      "Epoch 6, Val Loss: 0.2926\n",
      "Epoch: 7, training loss: 0.2966\n",
      "Epoch 7, Val Loss: 0.2894\n",
      "Epoch: 8, training loss: 0.2949\n",
      "Epoch 8, Val Loss: 0.2890\n",
      "Epoch: 9, training loss: 0.2916\n",
      "Epoch 9, Val Loss: 0.2860\n",
      "Epoch: 10, training loss: 0.2909\n",
      "Epoch 10, Val Loss: 0.2882\n",
      "Fold 2:\n",
      "Epoch: 1, training loss: 0.4764\n",
      "Epoch 1, Val Loss: 0.3559\n",
      "Epoch: 2, training loss: 0.3398\n",
      "Epoch 2, Val Loss: 0.3276\n",
      "Epoch: 3, training loss: 0.3255\n",
      "Epoch 3, Val Loss: 0.3184\n",
      "Epoch: 4, training loss: 0.3160\n",
      "Epoch 4, Val Loss: 0.3102\n",
      "Epoch: 5, training loss: 0.3051\n",
      "Epoch 5, Val Loss: 0.3020\n",
      "Epoch: 6, training loss: 0.2996\n",
      "Epoch 6, Val Loss: 0.2946\n",
      "Epoch: 7, training loss: 0.2949\n",
      "Epoch 7, Val Loss: 0.2921\n",
      "Epoch: 8, training loss: 0.2898\n",
      "Epoch 8, Val Loss: 0.2901\n",
      "Epoch: 9, training loss: 0.2897\n",
      "Epoch 9, Val Loss: 0.2899\n",
      "Epoch: 10, training loss: 0.2881\n",
      "Epoch 10, Val Loss: 0.2901\n",
      "Fold 3:\n",
      "Epoch: 1, training loss: 0.4682\n",
      "Epoch 1, Val Loss: 0.3460\n",
      "Epoch: 2, training loss: 0.3361\n",
      "Epoch 2, Val Loss: 0.3239\n",
      "Epoch: 3, training loss: 0.3232\n",
      "Epoch 3, Val Loss: 0.3135\n",
      "Epoch: 4, training loss: 0.3120\n",
      "Epoch 4, Val Loss: 0.3067\n",
      "Epoch: 5, training loss: 0.3029\n",
      "Epoch 5, Val Loss: 0.3013\n",
      "Epoch: 6, training loss: 0.2937\n",
      "Epoch 6, Val Loss: 0.2985\n",
      "Epoch: 7, training loss: 0.2924\n",
      "Epoch 7, Val Loss: 0.2961\n",
      "Epoch: 8, training loss: 0.2889\n",
      "Epoch 8, Val Loss: 0.2946\n",
      "Epoch: 9, training loss: 0.2878\n",
      "Epoch 9, Val Loss: 0.2937\n",
      "Epoch: 10, training loss: 0.2878\n",
      "Epoch 10, Val Loss: 0.2967\n",
      "Fold 4:\n",
      "Epoch: 1, training loss: 0.4888\n",
      "Epoch 1, Val Loss: 0.3481\n",
      "Epoch: 2, training loss: 0.3354\n",
      "Epoch 2, Val Loss: 0.3264\n",
      "Epoch: 3, training loss: 0.3262\n",
      "Epoch 3, Val Loss: 0.3181\n",
      "Epoch: 4, training loss: 0.3169\n",
      "Epoch 4, Val Loss: 0.3094\n",
      "Epoch: 5, training loss: 0.3092\n",
      "Epoch 5, Val Loss: 0.3006\n",
      "Epoch: 6, training loss: 0.3014\n",
      "Epoch 6, Val Loss: 0.2957\n",
      "Epoch: 7, training loss: 0.2962\n",
      "Epoch 7, Val Loss: 0.2919\n",
      "Epoch: 8, training loss: 0.2899\n",
      "Epoch 8, Val Loss: 0.2873\n",
      "Epoch: 9, training loss: 0.2907\n",
      "Epoch 9, Val Loss: 0.2980\n",
      "Epoch: 10, training loss: 0.2893\n",
      "Epoch 10, Val Loss: 0.2882\n",
      "Fold 5:\n",
      "Epoch: 1, training loss: 0.5801\n",
      "Epoch 1, Val Loss: 0.3979\n",
      "Epoch: 2, training loss: 0.3467\n",
      "Epoch 2, Val Loss: 0.3273\n",
      "Epoch: 3, training loss: 0.3278\n",
      "Epoch 3, Val Loss: 0.3200\n",
      "Epoch: 4, training loss: 0.3158\n",
      "Epoch 4, Val Loss: 0.3144\n",
      "Epoch: 5, training loss: 0.3082\n",
      "Epoch 5, Val Loss: 0.3104\n",
      "Epoch: 6, training loss: 0.2998\n",
      "Epoch 6, Val Loss: 0.3080\n",
      "Epoch: 7, training loss: 0.2958\n",
      "Epoch 7, Val Loss: 0.3050\n",
      "Epoch: 8, training loss: 0.2932\n",
      "Epoch 8, Val Loss: 0.3029\n",
      "Epoch: 9, training loss: 0.2910\n",
      "Epoch 9, Val Loss: 0.3003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-07 19:20:21,977] Trial 27 finished with value: 0.37343916032255653 and parameters: {'hidden_dim': 40, 'dropout': 0.14115786933994334, 'initial_lr': 5.238370487516618e-05, 'max_lr': 0.012585754613958786, 'criterion': 'BCEWithLogitsLoss', 'pos_weight': 1}. Best is trial 13 with value: 0.37735558639428074.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, training loss: 0.2921\n",
      "Epoch 10, Val Loss: 0.3016\n",
      "Using device: cuda\n",
      "Fold 1:\n",
      "Epoch: 1, training loss: 0.6483\n",
      "Epoch 1, Val Loss: 0.5820\n",
      "Epoch: 2, training loss: 0.5516\n",
      "Epoch 2, Val Loss: 0.5280\n",
      "Epoch: 3, training loss: 0.5286\n",
      "Epoch 3, Val Loss: 0.5170\n",
      "Epoch: 4, training loss: 0.5186\n",
      "Epoch 4, Val Loss: 0.5055\n",
      "Epoch: 5, training loss: 0.5060\n",
      "Epoch 5, Val Loss: 0.4943\n",
      "Epoch: 6, training loss: 0.4970\n",
      "Epoch 6, Val Loss: 0.4851\n",
      "Epoch: 7, training loss: 0.4894\n",
      "Epoch 7, Val Loss: 0.4773\n",
      "Epoch: 8, training loss: 0.4843\n",
      "Epoch 8, Val Loss: 0.4697\n",
      "Epoch: 9, training loss: 0.4812\n",
      "Epoch 9, Val Loss: 0.4624\n",
      "Epoch: 10, training loss: 0.4706\n",
      "Epoch 10, Val Loss: 0.4568\n",
      "Fold 2:\n",
      "Epoch: 1, training loss: 0.6895\n",
      "Epoch 1, Val Loss: 0.6182\n",
      "Epoch: 2, training loss: 0.5719\n",
      "Epoch 2, Val Loss: 0.5409\n",
      "Epoch: 3, training loss: 0.5310\n",
      "Epoch 3, Val Loss: 0.5241\n",
      "Epoch: 4, training loss: 0.5224\n",
      "Epoch 4, Val Loss: 0.5123\n",
      "Epoch: 5, training loss: 0.5134\n",
      "Epoch 5, Val Loss: 0.5016\n",
      "Epoch: 6, training loss: 0.4972\n",
      "Epoch 6, Val Loss: 0.4923\n",
      "Epoch: 7, training loss: 0.4910\n",
      "Epoch 7, Val Loss: 0.4827\n",
      "Epoch: 8, training loss: 0.4821\n",
      "Epoch 8, Val Loss: 0.4801\n",
      "Epoch: 9, training loss: 0.4732\n",
      "Epoch 9, Val Loss: 0.4709\n",
      "Epoch: 10, training loss: 0.4693\n",
      "Epoch 10, Val Loss: 0.4665\n",
      "Fold 3:\n",
      "Epoch: 1, training loss: 0.7484\n",
      "Epoch 1, Val Loss: 0.6667\n",
      "Epoch: 2, training loss: 0.6074\n",
      "Epoch 2, Val Loss: 0.5535\n",
      "Epoch: 3, training loss: 0.5406\n",
      "Epoch 3, Val Loss: 0.5235\n",
      "Epoch: 4, training loss: 0.5226\n",
      "Epoch 4, Val Loss: 0.5143\n",
      "Epoch: 5, training loss: 0.5140\n",
      "Epoch 5, Val Loss: 0.5057\n",
      "Epoch: 6, training loss: 0.5060\n",
      "Epoch 6, Val Loss: 0.4982\n",
      "Epoch: 7, training loss: 0.4933\n",
      "Epoch 7, Val Loss: 0.4909\n",
      "Epoch: 8, training loss: 0.4865\n",
      "Epoch 8, Val Loss: 0.4837\n",
      "Epoch: 9, training loss: 0.4748\n",
      "Epoch 9, Val Loss: 0.4822\n",
      "Epoch: 10, training loss: 0.4710\n",
      "Epoch 10, Val Loss: 0.4750\n",
      "Fold 4:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [07/Apr/2025 19:20:25] \"GET /api/studies/0?after=26 HTTP/1.1\" 200 9079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, training loss: 0.6829\n",
      "Epoch 1, Val Loss: 0.6135\n",
      "Epoch: 2, training loss: 0.5686\n",
      "Epoch 2, Val Loss: 0.5385\n",
      "Epoch: 3, training loss: 0.5297\n",
      "Epoch 3, Val Loss: 0.5230\n",
      "Epoch: 4, training loss: 0.5207\n",
      "Epoch 4, Val Loss: 0.5117\n",
      "Epoch: 5, training loss: 0.5059\n",
      "Epoch 5, Val Loss: 0.5008\n",
      "Epoch: 6, training loss: 0.4984\n",
      "Epoch 6, Val Loss: 0.4891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\optuna_dashboard\\_importance.py:96: ExperimentalWarning: PedAnovaImportanceEvaluator is experimental (supported from v3.6.0). The interface can change in the future.\n",
      "  study, target=target, evaluator=PedAnovaImportanceEvaluator()\n",
      "d:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\optuna\\importance\\_ped_anova\\evaluator.py:150: UserWarning: PedAnovaImportanceEvaluator computes the importances of params to achieve low `target` values. If this is not what you want, please modify target, e.g., by multiplying the output by -1.\n",
      "  warnings.warn(\n",
      "127.0.0.1 - - [07/Apr/2025 19:20:26] \"GET /api/studies/0/param_importances?evaluator=ped_anova HTTP/1.1\" 200 579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, training loss: 0.4893\n",
      "Epoch 7, Val Loss: 0.4797\n",
      "Epoch: 8, training loss: 0.4777\n",
      "Epoch 8, Val Loss: 0.4710\n",
      "Epoch: 9, training loss: 0.4703\n",
      "Epoch 9, Val Loss: 0.4678\n",
      "Epoch: 10, training loss: 0.4694\n",
      "Epoch 10, Val Loss: 0.4594\n",
      "Fold 5:\n",
      "Epoch: 1, training loss: 0.7207\n",
      "Epoch 1, Val Loss: 0.6289\n",
      "Epoch: 2, training loss: 0.5825\n",
      "Epoch 2, Val Loss: 0.5469\n",
      "Epoch: 3, training loss: 0.5402\n",
      "Epoch 3, Val Loss: 0.5327\n",
      "Epoch: 4, training loss: 0.5255\n",
      "Epoch 4, Val Loss: 0.5222\n",
      "Epoch: 5, training loss: 0.5143\n",
      "Epoch 5, Val Loss: 0.5133\n",
      "Epoch: 6, training loss: 0.5050\n",
      "Epoch 6, Val Loss: 0.5058\n",
      "Epoch: 7, training loss: 0.4890\n",
      "Epoch 7, Val Loss: 0.4972\n",
      "Epoch: 8, training loss: 0.4822\n",
      "Epoch 8, Val Loss: 0.4904\n",
      "Epoch: 9, training loss: 0.4809\n",
      "Epoch 9, Val Loss: 0.4830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-07 19:20:27,740] Trial 28 finished with value: 0.3677908916429346 and parameters: {'hidden_dim': 73, 'dropout': 0.35024667740703774, 'initial_lr': 0.00019647227200486015, 'max_lr': 0.003804973918051341, 'criterion': 'BCEWithLogitsLoss', 'pos_weight': 2}. Best is trial 13 with value: 0.37735558639428074.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, training loss: 0.4727\n",
      "Epoch 10, Val Loss: 0.4774\n",
      "Using device: cuda\n",
      "Fold 1:\n",
      "Epoch: 1, training loss: 0.7661\n",
      "Epoch 1, Val Loss: 0.7047\n",
      "Epoch: 2, training loss: 0.6888\n",
      "Epoch 2, Val Loss: 0.6687\n",
      "Epoch: 3, training loss: 0.6710\n",
      "Epoch 3, Val Loss: 0.6534\n",
      "Epoch: 4, training loss: 0.6504\n",
      "Epoch 4, Val Loss: 0.6369\n",
      "Epoch: 5, training loss: 0.6494\n",
      "Epoch 5, Val Loss: 0.6220\n",
      "Epoch: 6, training loss: 0.6264\n",
      "Epoch 6, Val Loss: 0.6083\n",
      "Epoch: 7, training loss: 0.6155\n",
      "Epoch 7, Val Loss: 0.5953\n",
      "Epoch: 8, training loss: 0.6118\n",
      "Epoch 8, Val Loss: 0.5892\n",
      "Epoch: 9, training loss: 0.5990\n",
      "Epoch 9, Val Loss: 0.5858\n",
      "Epoch: 10, training loss: 0.5940\n",
      "Epoch 10, Val Loss: 0.5831\n",
      "Fold 2:\n",
      "Epoch: 1, training loss: 0.7530\n",
      "Epoch 1, Val Loss: 0.7106\n",
      "Epoch: 2, training loss: 0.6947\n",
      "Epoch 2, Val Loss: 0.6749\n",
      "Epoch: 3, training loss: 0.6674\n",
      "Epoch 3, Val Loss: 0.6550\n",
      "Epoch: 4, training loss: 0.6497\n",
      "Epoch 4, Val Loss: 0.6359\n",
      "Epoch: 5, training loss: 0.6350\n",
      "Epoch 5, Val Loss: 0.6224\n",
      "Epoch: 6, training loss: 0.6172\n",
      "Epoch 6, Val Loss: 0.6088\n",
      "Epoch: 7, training loss: 0.6098\n",
      "Epoch 7, Val Loss: 0.6037\n",
      "Epoch: 8, training loss: 0.5958\n",
      "Epoch 8, Val Loss: 0.5998\n",
      "Epoch: 9, training loss: 0.5904\n",
      "Epoch 9, Val Loss: 0.5915\n",
      "Epoch: 10, training loss: 0.5861\n",
      "Epoch 10, Val Loss: 0.5893\n",
      "Fold 3:\n",
      "Epoch: 1, training loss: 0.8695\n",
      "Epoch 1, Val Loss: 0.7671\n",
      "Epoch: 2, training loss: 0.7180\n",
      "Epoch 2, Val Loss: 0.6721\n",
      "Epoch: 3, training loss: 0.6657\n",
      "Epoch 3, Val Loss: 0.6579\n",
      "Epoch: 4, training loss: 0.6569\n",
      "Epoch 4, Val Loss: 0.6459\n",
      "Epoch: 5, training loss: 0.6405\n",
      "Epoch 5, Val Loss: 0.6359\n",
      "Epoch: 6, training loss: 0.6229\n",
      "Epoch 6, Val Loss: 0.6250\n",
      "Epoch: 7, training loss: 0.6150\n",
      "Epoch 7, Val Loss: 0.6179\n",
      "Epoch: 8, training loss: 0.6019\n",
      "Epoch 8, Val Loss: 0.6099\n",
      "Epoch: 9, training loss: 0.5984\n",
      "Epoch 9, Val Loss: 0.6048\n",
      "Epoch: 10, training loss: 0.5833\n",
      "Epoch 10, Val Loss: 0.6031\n",
      "Fold 4:\n",
      "Epoch: 1, training loss: 0.8062\n",
      "Epoch 1, Val Loss: 0.7310\n",
      "Epoch: 2, training loss: 0.7082\n",
      "Epoch 2, Val Loss: 0.6829\n",
      "Epoch: 3, training loss: 0.6833\n",
      "Epoch 3, Val Loss: 0.6651\n",
      "Epoch: 4, training loss: 0.6647\n",
      "Epoch 4, Val Loss: 0.6479\n",
      "Epoch: 5, training loss: 0.6480\n",
      "Epoch 5, Val Loss: 0.6316\n",
      "Epoch: 6, training loss: 0.6325\n",
      "Epoch 6, Val Loss: 0.6170\n",
      "Epoch: 7, training loss: 0.6240\n",
      "Epoch 7, Val Loss: 0.6059\n",
      "Epoch: 8, training loss: 0.6133\n",
      "Epoch 8, Val Loss: 0.5956\n",
      "Epoch: 9, training loss: 0.5978\n",
      "Epoch 9, Val Loss: 0.5924\n",
      "Epoch: 10, training loss: 0.5979\n",
      "Epoch 10, Val Loss: 0.5846\n",
      "Fold 5:\n",
      "Epoch: 1, training loss: 0.8198\n",
      "Epoch 1, Val Loss: 0.7449\n",
      "Epoch: 2, training loss: 0.7157\n",
      "Epoch 2, Val Loss: 0.6861\n",
      "Epoch: 3, training loss: 0.6837\n",
      "Epoch 3, Val Loss: 0.6754\n",
      "Epoch: 4, training loss: 0.6657\n",
      "Epoch 4, Val Loss: 0.6660\n",
      "Epoch: 5, training loss: 0.6540\n",
      "Epoch 5, Val Loss: 0.6548\n",
      "Epoch: 6, training loss: 0.6416\n",
      "Epoch 6, Val Loss: 0.6452\n",
      "Epoch: 7, training loss: 0.6267\n",
      "Epoch 7, Val Loss: 0.6355\n",
      "Epoch: 8, training loss: 0.6151\n",
      "Epoch 8, Val Loss: 0.6251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-07 19:20:33,583] Trial 29 finished with value: 0.3298145897323156 and parameters: {'hidden_dim': 43, 'dropout': 0.2916714612161655, 'initial_lr': 6.0470067299959614e-05, 'max_lr': 0.006214577365959155, 'criterion': 'BCEWithLogitsLoss', 'pos_weight': 3}. Best is trial 13 with value: 0.37735558639428074.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, training loss: 0.6082\n",
      "Epoch 9, Val Loss: 0.6158\n",
      "Epoch: 10, training loss: 0.5984\n",
      "Epoch 10, Val Loss: 0.6120\n",
      "Best trial:\n",
      "  Combined score: 0.37735558639428074\n",
      "  Best hyperparameters:\n",
      "    hidden_dim: 53\n",
      "    dropout: 0.22349186907065982\n",
      "    initial_lr: 6.995890036673634e-05\n",
      "    max_lr: 0.08938960521453812\n",
      "    criterion: BCEWithLogitsLoss\n",
      "    pos_weight: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [07/Apr/2025 19:20:36] \"GET /api/studies/0?after=28 HTTP/1.1\" 200 7336\n",
      "d:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\optuna_dashboard\\_importance.py:96: ExperimentalWarning: PedAnovaImportanceEvaluator is experimental (supported from v3.6.0). The interface can change in the future.\n",
      "  study, target=target, evaluator=PedAnovaImportanceEvaluator()\n",
      "d:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\optuna\\importance\\_ped_anova\\evaluator.py:150: UserWarning: PedAnovaImportanceEvaluator computes the importances of params to achieve low `target` values. If this is not what you want, please modify target, e.g., by multiplying the output by -1.\n",
      "  warnings.warn(\n",
      "127.0.0.1 - - [07/Apr/2025 19:20:37] \"GET /api/studies/0/param_importances?evaluator=ped_anova HTTP/1.1\" 200 580\n",
      "127.0.0.1 - - [07/Apr/2025 19:20:47] \"GET /api/studies/0?after=30 HTTP/1.1\" 200 3690\n",
      "127.0.0.1 - - [07/Apr/2025 19:20:58] \"GET /api/studies/0?after=30 HTTP/1.1\" 200 3690\n",
      "127.0.0.1 - - [07/Apr/2025 19:21:09] \"GET /api/studies/0?after=30 HTTP/1.1\" 200 3690\n",
      "127.0.0.1 - - [07/Apr/2025 19:21:20] \"GET /api/studies/0?after=30 HTTP/1.1\" 200 3690\n",
      "127.0.0.1 - - [07/Apr/2025 19:21:31] \"GET /api/studies/0?after=30 HTTP/1.1\" 200 3690\n",
      "127.0.0.1 - - [07/Apr/2025 19:21:42] \"GET /api/studies/0?after=30 HTTP/1.1\" 200 3690\n",
      "127.0.0.1 - - [07/Apr/2025 19:21:53] \"GET /api/studies/0?after=30 HTTP/1.1\" 200 3690\n",
      "127.0.0.1 - - [07/Apr/2025 19:22:04] \"GET /api/studies/0?after=30 HTTP/1.1\" 200 3690\n",
      "127.0.0.1 - - [07/Apr/2025 19:22:15] \"GET /api/studies/0?after=30 HTTP/1.1\" 200 3690\n",
      "127.0.0.1 - - [07/Apr/2025 19:22:26] \"GET /api/studies/0?after=30 HTTP/1.1\" 200 3690\n",
      "127.0.0.1 - - [07/Apr/2025 19:22:37] \"GET /api/studies/0?after=30 HTTP/1.1\" 200 3690\n",
      "127.0.0.1 - - [07/Apr/2025 19:22:48] \"GET /api/studies/0?after=30 HTTP/1.1\" 200 3690\n",
      "127.0.0.1 - - [07/Apr/2025 19:22:59] \"GET /api/studies/0?after=30 HTTP/1.1\" 200 3690\n",
      "127.0.0.1 - - [07/Apr/2025 19:23:10] \"GET /api/studies/0?after=30 HTTP/1.1\" 200 3690\n",
      "127.0.0.1 - - [07/Apr/2025 19:23:21] \"GET /api/studies/0?after=30 HTTP/1.1\" 200 3690\n",
      "127.0.0.1 - - [07/Apr/2025 19:23:32] \"GET /api/studies/0?after=30 HTTP/1.1\" 200 3690\n",
      "127.0.0.1 - - [07/Apr/2025 19:23:43] \"GET /api/studies/0?after=30 HTTP/1.1\" 200 3690\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import optuna\n",
    "from optuna_dashboard import run_server\n",
    "\n",
    "def start_dashboard():\n",
    "    run_server(storage)\n",
    "\n",
    "storage = optuna.storages.InMemoryStorage()\n",
    "study = optuna.create_study(direction=\"maximize\", storage=storage, study_name=\"Basic\")\n",
    "\n",
    "# Start dashboard in a separate thread\n",
    "dashboard_thread = threading.Thread(target=start_dashboard, daemon=True)\n",
    "dashboard_thread.start()\n",
    "\n",
    "# Run optimization\n",
    "study.optimize(maximise_combined_score, n_trials=30)\n",
    "\n",
    "# After optimization, print results\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(f\"  Combined score: {trial.value}\")\n",
    "print(\"  Best hyperparameters:\")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfd0e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-07 19:15:44,416] A new study created in memory with name: Basic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1:\n",
      "Epoch: 1, training loss: 1.1752\n",
      "Epoch 1, Val Loss: 1.0746\n",
      "Epoch: 2, training loss: 1.1142\n",
      "Epoch 2, Val Loss: 1.0331\n",
      "Epoch: 3, training loss: 1.0963\n",
      "Epoch 3, Val Loss: 1.0771\n",
      "Epoch: 4, training loss: 1.0960\n",
      "Epoch 4, Val Loss: 1.0444\n",
      "Epoch: 5, training loss: 1.0885\n",
      "Epoch 5, Val Loss: 1.0640\n",
      "Epoch: 6, training loss: 1.0821\n",
      "Epoch 6, Val Loss: 1.0547\n",
      "Epoch: 7, training loss: 1.0911\n",
      "Epoch 7, Val Loss: 1.0512\n",
      "Epoch: 8, training loss: 1.0913\n",
      "Epoch 8, Val Loss: 1.0558\n",
      "Epoch: 9, training loss: 1.0997\n",
      "Epoch 9, Val Loss: 1.0401\n",
      "Epoch: 10, training loss: 1.0930\n",
      "Epoch 10, Val Loss: 1.0726\n",
      "Fold 2:\n",
      "Epoch: 1, training loss: 1.1849\n",
      "Epoch 1, Val Loss: 1.1467\n",
      "Epoch: 2, training loss: 1.1079\n",
      "Epoch 2, Val Loss: 1.0722\n",
      "Epoch: 3, training loss: 1.0763\n",
      "Epoch 3, Val Loss: 1.0743\n",
      "Epoch: 4, training loss: 1.0882\n",
      "Epoch 4, Val Loss: 1.1518\n",
      "Epoch: 5, training loss: 1.1009\n",
      "Epoch 5, Val Loss: 1.0854\n",
      "Epoch: 6, training loss: 1.0854\n",
      "Epoch 6, Val Loss: 1.3694\n",
      "Epoch: 7, training loss: 1.1273\n",
      "Epoch 7, Val Loss: 1.0768\n",
      "Epoch: 8, training loss: 1.0864\n",
      "Epoch 8, Val Loss: 1.0592\n",
      "Epoch: 9, training loss: 1.0963\n",
      "Epoch 9, Val Loss: 1.0851\n",
      "Epoch: 10, training loss: 1.0990\n",
      "Epoch 10, Val Loss: 1.0825\n",
      "Fold 3:\n",
      "Epoch: 1, training loss: 1.1636\n",
      "Epoch 1, Val Loss: 1.1131\n",
      "Epoch: 2, training loss: 1.0809\n",
      "Epoch 2, Val Loss: 1.0964\n",
      "Epoch: 3, training loss: 1.0658\n",
      "Epoch 3, Val Loss: 1.1066\n",
      "Epoch: 4, training loss: 1.0827\n",
      "Epoch 4, Val Loss: 1.1425\n",
      "Epoch: 5, training loss: 1.0850\n",
      "Epoch 5, Val Loss: 1.1082\n",
      "Epoch: 6, training loss: 1.0820\n",
      "Epoch 6, Val Loss: 1.1393\n",
      "Epoch: 7, training loss: 1.0775\n",
      "Epoch 7, Val Loss: 1.1075\n",
      "Epoch: 8, training loss: 1.0807\n",
      "Epoch 8, Val Loss: 1.1285\n",
      "Epoch: 9, training loss: 1.0777\n",
      "Epoch 9, Val Loss: 1.1843\n",
      "Epoch: 10, training loss: 1.1073\n",
      "Epoch 10, Val Loss: 1.1163\n",
      "Fold 4:\n",
      "Epoch: 1, training loss: 1.1891\n",
      "Epoch 1, Val Loss: 1.1150\n",
      "Epoch: 2, training loss: 1.1040\n",
      "Epoch 2, Val Loss: 1.0726\n",
      "Epoch: 3, training loss: 1.0931\n",
      "Epoch 3, Val Loss: 1.0873\n",
      "Epoch: 4, training loss: 1.0837\n",
      "Epoch 4, Val Loss: 1.1319\n",
      "Epoch: 5, training loss: 1.0935\n",
      "Epoch 5, Val Loss: 1.0933\n",
      "Epoch: 6, training loss: 1.1040\n",
      "Epoch 6, Val Loss: 1.0996\n",
      "Epoch: 7, training loss: 1.0810\n",
      "Epoch 7, Val Loss: 1.0967\n",
      "Epoch: 8, training loss: 1.0833\n",
      "Epoch 8, Val Loss: 1.1011\n",
      "Epoch: 9, training loss: 1.0915\n",
      "Epoch 9, Val Loss: 1.1026\n",
      "Epoch: 10, training loss: 1.0843\n",
      "Epoch 10, Val Loss: 1.2496\n",
      "Fold 5:\n",
      "Epoch: 1, training loss: 1.1939\n",
      "Epoch 1, Val Loss: 1.1528\n",
      "Epoch: 2, training loss: 1.1125\n",
      "Epoch 2, Val Loss: 1.0984\n",
      "Epoch: 3, training loss: 1.0878\n",
      "Epoch 3, Val Loss: 1.1287\n",
      "Epoch: 4, training loss: 1.0773\n",
      "Epoch 4, Val Loss: 1.0939\n",
      "Epoch: 5, training loss: 1.0940\n",
      "Epoch 5, Val Loss: 1.1167\n",
      "Epoch: 6, training loss: 1.0915\n",
      "Epoch 6, Val Loss: 1.2369\n",
      "Epoch: 7, training loss: 1.0971\n",
      "Epoch 7, Val Loss: 1.1081\n",
      "Epoch: 8, training loss: 1.0823\n",
      "Epoch 8, Val Loss: 1.1244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-07 19:15:50,856] Trial 0 finished with value: 0.24135359444644283 and parameters: {'hidden_dim': 102, 'dropout': 0.2865623578284173, 'initial_lr': 2.373445721330544e-05, 'max_lr': 0.09694282698403738, 'criterion': 'BCEWithLogitsLoss', 'pos_weight': 9}. Best is trial 0 with value: 0.24135359444644283.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, training loss: 1.0676\n",
      "Epoch 9, Val Loss: 1.1563\n",
      "Epoch: 10, training loss: 1.0983\n",
      "Epoch 10, Val Loss: 1.1275\n",
      "Fold 1:\n",
      "Epoch: 1, training loss: 1.2993\n",
      "Epoch 1, Val Loss: 1.2350\n",
      "Epoch: 2, training loss: 1.2369\n",
      "Epoch 2, Val Loss: 1.1836\n",
      "Epoch: 3, training loss: 1.2147\n",
      "Epoch 3, Val Loss: 1.1433\n",
      "Epoch: 4, training loss: 1.1701\n",
      "Epoch 4, Val Loss: 1.1109\n",
      "Epoch: 5, training loss: 1.1627\n",
      "Epoch 5, Val Loss: 1.1072\n",
      "Epoch: 6, training loss: 1.1452\n",
      "Epoch 6, Val Loss: 1.1065\n",
      "Epoch: 7, training loss: 1.1269\n",
      "Epoch 7, Val Loss: 1.0970\n",
      "Epoch: 8, training loss: 1.1314\n",
      "Epoch 8, Val Loss: 1.1167\n",
      "Epoch: 9, training loss: 1.1241\n",
      "Epoch 9, Val Loss: 1.0971\n",
      "Epoch: 10, training loss: 1.1209\n",
      "Epoch 10, Val Loss: 1.1141\n",
      "Fold 2:\n",
      "Epoch: 1, training loss: 1.3143\n",
      "Epoch 1, Val Loss: 1.2513\n",
      "Epoch: 2, training loss: 1.2325\n",
      "Epoch 2, Val Loss: 1.2010\n",
      "Epoch: 3, training loss: 1.1929\n",
      "Epoch 3, Val Loss: 1.1662\n",
      "Epoch: 4, training loss: 1.1564\n",
      "Epoch 4, Val Loss: 1.1411\n",
      "Epoch: 5, training loss: 1.1392\n",
      "Epoch 5, Val Loss: 1.1427\n",
      "Epoch: 6, training loss: 1.1490\n",
      "Epoch 6, Val Loss: 1.1599\n",
      "Epoch: 7, training loss: 1.1211\n",
      "Epoch 7, Val Loss: 1.1231\n",
      "Epoch: 8, training loss: 1.1381\n",
      "Epoch 8, Val Loss: 1.1317\n",
      "Epoch: 9, training loss: 1.1293\n",
      "Epoch 9, Val Loss: 1.1275\n",
      "Epoch: 10, training loss: 1.1211\n",
      "Epoch 10, Val Loss: 1.1405\n",
      "Fold 3:\n",
      "Epoch: 1, training loss: 1.2933\n",
      "Epoch 1, Val Loss: 1.2554\n",
      "Epoch: 2, training loss: 1.2425\n",
      "Epoch 2, Val Loss: 1.2145\n",
      "Epoch: 3, training loss: 1.1991\n",
      "Epoch 3, Val Loss: 1.1921\n",
      "Epoch: 4, training loss: 1.1635\n",
      "Epoch 4, Val Loss: 1.1644\n",
      "Epoch: 5, training loss: 1.1272\n",
      "Epoch 5, Val Loss: 1.1603\n",
      "Epoch: 6, training loss: 1.1310\n",
      "Epoch 6, Val Loss: 1.1539\n",
      "Epoch: 7, training loss: 1.1156\n",
      "Epoch 7, Val Loss: 1.1494\n",
      "Epoch: 8, training loss: 1.1141\n",
      "Epoch 8, Val Loss: 1.1637\n",
      "Epoch: 9, training loss: 1.1150\n",
      "Epoch 9, Val Loss: 1.1537\n",
      "Epoch: 10, training loss: 1.1093\n",
      "Epoch 10, Val Loss: 1.1876\n",
      "Fold 4:\n",
      "Epoch: 1, training loss: 1.3130\n",
      "Epoch 1, Val Loss: 1.2743\n",
      "Epoch: 2, training loss: 1.2598\n",
      "Epoch 2, Val Loss: 1.2261\n",
      "Epoch: 3, training loss: 1.2028\n",
      "Epoch 3, Val Loss: 1.1986\n",
      "Epoch: 4, training loss: 1.1725\n",
      "Epoch 4, Val Loss: 1.1612\n",
      "Epoch: 5, training loss: 1.1735\n",
      "Epoch 5, Val Loss: 1.1458\n",
      "Epoch: 6, training loss: 1.1418\n",
      "Epoch 6, Val Loss: 1.1421\n",
      "Epoch: 7, training loss: 1.1327\n",
      "Epoch 7, Val Loss: 1.1271\n",
      "Epoch: 8, training loss: 1.1231\n",
      "Epoch 8, Val Loss: 1.1366\n",
      "Epoch: 9, training loss: 1.1202\n",
      "Epoch 9, Val Loss: 1.1367\n",
      "Epoch: 10, training loss: 1.1207\n",
      "Epoch 10, Val Loss: 1.1210\n",
      "Fold 5:\n",
      "Epoch: 1, training loss: 1.3260\n",
      "Epoch 1, Val Loss: 1.3105\n",
      "Epoch: 2, training loss: 1.2610\n",
      "Epoch 2, Val Loss: 1.2608\n",
      "Epoch: 3, training loss: 1.2029\n",
      "Epoch 3, Val Loss: 1.2159\n",
      "Epoch: 4, training loss: 1.1678\n",
      "Epoch 4, Val Loss: 1.1798\n",
      "Epoch: 5, training loss: 1.1418\n",
      "Epoch 5, Val Loss: 1.1668\n",
      "Epoch: 6, training loss: 1.1319\n",
      "Epoch 6, Val Loss: 1.1542\n",
      "Epoch: 7, training loss: 1.1284\n",
      "Epoch 7, Val Loss: 1.1505\n",
      "Epoch: 8, training loss: 1.1443\n",
      "Epoch 8, Val Loss: 1.1709\n",
      "Epoch: 9, training loss: 1.1228\n",
      "Epoch 9, Val Loss: 1.1588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-07 19:15:55,996] Trial 1 finished with value: 0.223905539394924 and parameters: {'hidden_dim': 20, 'dropout': 0.23443747148705893, 'initial_lr': 0.0007176751647106228, 'max_lr': 0.03243199791432575, 'criterion': 'BCEWithLogitsLoss', 'pos_weight': 10}. Best is trial 0 with value: 0.24135359444644283.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, training loss: 1.1257\n",
      "Epoch 10, Val Loss: 1.1558\n",
      "Fold 1:\n",
      "Epoch: 1, training loss: 0.6456\n",
      "Epoch 1, Val Loss: 0.5331\n",
      "Epoch: 2, training loss: 0.4421\n",
      "Epoch 2, Val Loss: 0.3674\n",
      "Epoch: 3, training loss: 0.3521\n",
      "Epoch 3, Val Loss: 0.3404\n",
      "Epoch: 4, training loss: 0.3399\n",
      "Epoch 4, Val Loss: 0.3308\n",
      "Epoch: 5, training loss: 0.3315\n",
      "Epoch 5, Val Loss: 0.3218\n",
      "Epoch: 6, training loss: 0.3237\n",
      "Epoch 6, Val Loss: 0.3141\n",
      "Epoch: 7, training loss: 0.3180\n",
      "Epoch 7, Val Loss: 0.3069\n",
      "Epoch: 8, training loss: 0.3109\n",
      "Epoch 8, Val Loss: 0.3004\n",
      "Epoch: 9, training loss: 0.3032\n",
      "Epoch 9, Val Loss: 0.2973\n",
      "Epoch: 10, training loss: 0.2990\n",
      "Epoch 10, Val Loss: 0.2918\n",
      "Fold 2:\n",
      "Epoch: 1, training loss: 0.6180\n",
      "Epoch 1, Val Loss: 0.5130\n",
      "Epoch: 2, training loss: 0.4235\n",
      "Epoch 2, Val Loss: 0.3551\n",
      "Epoch: 3, training loss: 0.3442\n",
      "Epoch 3, Val Loss: 0.3346\n",
      "Epoch: 4, training loss: 0.3369\n",
      "Epoch 4, Val Loss: 0.3277\n",
      "Epoch: 5, training loss: 0.3264\n",
      "Epoch 5, Val Loss: 0.3214\n",
      "Epoch: 6, training loss: 0.3211\n",
      "Epoch 6, Val Loss: 0.3148\n",
      "Epoch: 7, training loss: 0.3118\n",
      "Epoch 7, Val Loss: 0.3087\n",
      "Epoch: 8, training loss: 0.3069\n",
      "Epoch 8, Val Loss: 0.3036\n",
      "Epoch: 9, training loss: 0.2990\n",
      "Epoch 9, Val Loss: 0.2983\n",
      "Epoch: 10, training loss: 0.2964\n",
      "Epoch 10, Val Loss: 0.2945\n",
      "Fold 3:\n",
      "Epoch: 1, training loss: 0.5228\n",
      "Epoch 1, Val Loss: 0.4139\n",
      "Epoch: 2, training loss: 0.3704\n",
      "Epoch 2, Val Loss: 0.3383\n",
      "Epoch: 3, training loss: 0.3403\n",
      "Epoch 3, Val Loss: 0.3301\n",
      "Epoch: 4, training loss: 0.3303\n",
      "Epoch 4, Val Loss: 0.3239\n",
      "Epoch: 5, training loss: 0.3254\n",
      "Epoch 5, Val Loss: 0.3177\n",
      "Epoch: 6, training loss: 0.3122\n",
      "Epoch 6, Val Loss: 0.3121\n",
      "Epoch: 7, training loss: 0.3050\n",
      "Epoch 7, Val Loss: 0.3088\n",
      "Epoch: 8, training loss: 0.3011\n",
      "Epoch 8, Val Loss: 0.3046\n",
      "Epoch: 9, training loss: 0.2950\n",
      "Epoch 9, Val Loss: 0.3011\n",
      "Epoch: 10, training loss: 0.2925\n",
      "Epoch 10, Val Loss: 0.2989\n",
      "Fold 4:\n",
      "Epoch: 1, training loss: 0.6738\n",
      "Epoch 1, Val Loss: 0.5564\n",
      "Epoch: 2, training loss: 0.4630\n",
      "Epoch 2, Val Loss: 0.3798\n",
      "Epoch: 3, training loss: 0.3500\n",
      "Epoch 3, Val Loss: 0.3327\n",
      "Epoch: 4, training loss: 0.3347\n",
      "Epoch 4, Val Loss: 0.3261\n",
      "Epoch: 5, training loss: 0.3259\n",
      "Epoch 5, Val Loss: 0.3193\n",
      "Epoch: 6, training loss: 0.3240\n",
      "Epoch 6, Val Loss: 0.3122\n",
      "Epoch: 7, training loss: 0.3133\n",
      "Epoch 7, Val Loss: 0.3050\n",
      "Epoch: 8, training loss: 0.3080\n",
      "Epoch 8, Val Loss: 0.2990\n",
      "Epoch: 9, training loss: 0.3018\n",
      "Epoch 9, Val Loss: 0.2952\n",
      "Epoch: 10, training loss: 0.3002\n",
      "Epoch 10, Val Loss: 0.2926\n",
      "Fold 5:\n",
      "Epoch: 1, training loss: 0.5610\n",
      "Epoch 1, Val Loss: 0.4729\n",
      "Epoch: 2, training loss: 0.4073\n",
      "Epoch 2, Val Loss: 0.3626\n",
      "Epoch: 3, training loss: 0.3444\n",
      "Epoch 3, Val Loss: 0.3387\n",
      "Epoch: 4, training loss: 0.3356\n",
      "Epoch 4, Val Loss: 0.3334\n",
      "Epoch: 5, training loss: 0.3309\n",
      "Epoch 5, Val Loss: 0.3282\n",
      "Epoch: 6, training loss: 0.3229\n",
      "Epoch 6, Val Loss: 0.3248\n",
      "Epoch: 7, training loss: 0.3132\n",
      "Epoch 7, Val Loss: 0.3214\n",
      "Epoch: 8, training loss: 0.3096\n",
      "Epoch 8, Val Loss: 0.3170\n",
      "Epoch: 9, training loss: 0.3040\n",
      "Epoch 9, Val Loss: 0.3132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-07 19:16:01,664] Trial 2 finished with value: 0.3762004050291111 and parameters: {'hidden_dim': 18, 'dropout': 0.10817184225489651, 'initial_lr': 3.512678150429276e-05, 'max_lr': 0.009076259460127942, 'criterion': 'BCEWithLogitsLoss', 'pos_weight': 1}. Best is trial 2 with value: 0.3762004050291111.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, training loss: 0.3007\n",
      "Epoch 10, Val Loss: 0.3107\n",
      "Fold 1:\n",
      "Epoch: 1, training loss: 1.0457\n",
      "Epoch 1, Val Loss: 0.9970\n",
      "Epoch: 2, training loss: 1.0053\n",
      "Epoch 2, Val Loss: 0.9695\n",
      "Epoch: 3, training loss: 0.9746\n",
      "Epoch 3, Val Loss: 0.9372\n",
      "Epoch: 4, training loss: 0.9490\n",
      "Epoch 4, Val Loss: 0.9119\n",
      "Epoch: 5, training loss: 0.9275\n",
      "Epoch 5, Val Loss: 0.8893\n",
      "Epoch: 6, training loss: 0.9044\n",
      "Epoch 6, Val Loss: 0.8747\n",
      "Epoch: 7, training loss: 0.8968\n",
      "Epoch 7, Val Loss: 0.8652\n",
      "Epoch: 8, training loss: 0.8813\n",
      "Epoch 8, Val Loss: 0.8619\n",
      "Epoch: 9, training loss: 0.8793\n",
      "Epoch 9, Val Loss: 0.8537\n",
      "Epoch: 10, training loss: 0.8806\n",
      "Epoch 10, Val Loss: 0.8579\n",
      "Fold 2:\n",
      "Epoch: 1, training loss: 1.0127\n",
      "Epoch 1, Val Loss: 0.9973\n",
      "Epoch: 2, training loss: 0.9773\n",
      "Epoch 2, Val Loss: 0.9622\n",
      "Epoch: 3, training loss: 0.9534\n",
      "Epoch 3, Val Loss: 0.9315\n",
      "Epoch: 4, training loss: 0.9318\n",
      "Epoch 4, Val Loss: 0.9122\n",
      "Epoch: 5, training loss: 0.9102\n",
      "Epoch 5, Val Loss: 0.8963\n",
      "Epoch: 6, training loss: 0.8878\n",
      "Epoch 6, Val Loss: 0.8882\n",
      "Epoch: 7, training loss: 0.8781\n",
      "Epoch 7, Val Loss: 0.8869\n",
      "Epoch: 8, training loss: 0.8676\n",
      "Epoch 8, Val Loss: 0.8807\n",
      "Epoch: 9, training loss: 0.8649\n",
      "Epoch 9, Val Loss: 0.8744\n",
      "Epoch: 10, training loss: 0.8713\n",
      "Epoch 10, Val Loss: 0.8708\n",
      "Fold 3:\n",
      "Epoch: 1, training loss: 1.0454\n",
      "Epoch 1, Val Loss: 1.0210\n",
      "Epoch: 2, training loss: 1.0104\n",
      "Epoch 2, Val Loss: 0.9908\n",
      "Epoch: 3, training loss: 0.9746\n",
      "Epoch 3, Val Loss: 0.9627\n",
      "Epoch: 4, training loss: 0.9458\n",
      "Epoch 4, Val Loss: 0.9415\n",
      "Epoch: 5, training loss: 0.9181\n",
      "Epoch 5, Val Loss: 0.9254\n",
      "Epoch: 6, training loss: 0.8995\n",
      "Epoch 6, Val Loss: 0.9134\n",
      "Epoch: 7, training loss: 0.8814\n",
      "Epoch 7, Val Loss: 0.9079\n",
      "Epoch: 8, training loss: 0.8732\n",
      "Epoch 8, Val Loss: 0.8981\n",
      "Epoch: 9, training loss: 0.8611\n",
      "Epoch 9, Val Loss: 0.9004\n",
      "Epoch: 10, training loss: 0.8571\n",
      "Epoch 10, Val Loss: 0.8937\n",
      "Fold 4:\n",
      "Epoch: 1, training loss: 1.0345\n",
      "Epoch 1, Val Loss: 1.0131\n",
      "Epoch: 2, training loss: 0.9953\n",
      "Epoch 2, Val Loss: 0.9891\n",
      "Epoch: 3, training loss: 0.9742\n",
      "Epoch 3, Val Loss: 0.9597\n",
      "Epoch: 4, training loss: 0.9453\n",
      "Epoch 4, Val Loss: 0.9286\n",
      "Epoch: 5, training loss: 0.9120\n",
      "Epoch 5, Val Loss: 0.8968\n",
      "Epoch: 6, training loss: 0.8948\n",
      "Epoch 6, Val Loss: 0.8781\n",
      "Epoch: 7, training loss: 0.8829\n",
      "Epoch 7, Val Loss: 0.8700\n",
      "Epoch: 8, training loss: 0.8684\n",
      "Epoch 8, Val Loss: 0.8731\n",
      "Epoch: 9, training loss: 0.8685\n",
      "Epoch 9, Val Loss: 0.8655\n",
      "Epoch: 10, training loss: 0.8584\n",
      "Epoch 10, Val Loss: 0.8690\n",
      "Fold 5:\n",
      "Epoch: 1, training loss: 1.0595\n",
      "Epoch 1, Val Loss: 1.0168\n",
      "Epoch: 2, training loss: 1.0155\n",
      "Epoch 2, Val Loss: 0.9911\n",
      "Epoch: 3, training loss: 0.9842\n",
      "Epoch 3, Val Loss: 0.9619\n",
      "Epoch: 4, training loss: 0.9427\n",
      "Epoch 4, Val Loss: 0.9290\n",
      "Epoch: 5, training loss: 0.9149\n",
      "Epoch 5, Val Loss: 0.9069\n",
      "Epoch: 6, training loss: 0.8969\n",
      "Epoch 6, Val Loss: 0.8908\n",
      "Epoch: 7, training loss: 0.8876\n",
      "Epoch 7, Val Loss: 0.8979\n",
      "Epoch: 8, training loss: 0.8743\n",
      "Epoch 8, Val Loss: 0.8885\n",
      "Epoch: 9, training loss: 0.8729\n",
      "Epoch 9, Val Loss: 0.8763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-07 19:16:06,674] Trial 3 finished with value: 0.24691630812333026 and parameters: {'hidden_dim': 16, 'dropout': 0.1930334665923532, 'initial_lr': 1.2456205586385888e-05, 'max_lr': 0.013412384557007835, 'criterion': 'BCEWithLogitsLoss', 'pos_weight': 6}. Best is trial 2 with value: 0.3762004050291111.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, training loss: 0.8762\n",
      "Epoch 10, Val Loss: 0.8812\n",
      "Fold 1:\n",
      "Epoch: 1, training loss: 0.9951\n",
      "Epoch 1, Val Loss: 0.9468\n",
      "Epoch: 2, training loss: 0.9346\n",
      "Epoch 2, Val Loss: 0.8824\n",
      "Epoch: 3, training loss: 0.8889\n",
      "Epoch 3, Val Loss: 0.8981\n",
      "Epoch: 4, training loss: 0.8858\n",
      "Epoch 4, Val Loss: 0.8701\n",
      "Epoch: 5, training loss: 0.8791\n",
      "Epoch 5, Val Loss: 0.8691\n",
      "Epoch: 6, training loss: 0.8644\n",
      "Epoch 6, Val Loss: 0.8430\n",
      "Epoch: 7, training loss: 0.8752\n",
      "Epoch 7, Val Loss: 0.8467\n",
      "Epoch: 8, training loss: 0.8560\n",
      "Epoch 8, Val Loss: 0.9047\n",
      "Epoch: 9, training loss: 0.8662\n",
      "Epoch 9, Val Loss: 0.8463\n",
      "Epoch: 10, training loss: 0.8628\n",
      "Epoch 10, Val Loss: 0.8521\n",
      "Fold 2:\n",
      "Epoch: 1, training loss: 1.0030\n",
      "Epoch 1, Val Loss: 0.9615\n",
      "Epoch: 2, training loss: 0.9356\n",
      "Epoch 2, Val Loss: 0.9212\n",
      "Epoch: 3, training loss: 0.8925\n",
      "Epoch 3, Val Loss: 0.8849\n",
      "Epoch: 4, training loss: 0.8746\n",
      "Epoch 4, Val Loss: 0.8725\n",
      "Epoch: 5, training loss: 0.8543\n",
      "Epoch 5, Val Loss: 0.8869\n",
      "Epoch: 6, training loss: 0.8683\n",
      "Epoch 6, Val Loss: 0.8885\n",
      "Epoch: 7, training loss: 0.8523\n",
      "Epoch 7, Val Loss: 0.8682\n",
      "Epoch: 8, training loss: 0.8510\n",
      "Epoch 8, Val Loss: 0.8614\n",
      "Epoch: 9, training loss: 0.8542\n",
      "Epoch 9, Val Loss: 0.8822\n",
      "Epoch: 10, training loss: 0.8502\n",
      "Epoch 10, Val Loss: 0.8767\n",
      "Fold 3:\n",
      "Epoch: 1, training loss: 0.9801\n",
      "Epoch 1, Val Loss: 0.9511\n",
      "Epoch: 2, training loss: 0.9138\n",
      "Epoch 2, Val Loss: 0.9162\n",
      "Epoch: 3, training loss: 0.8703\n",
      "Epoch 3, Val Loss: 0.8956\n",
      "Epoch: 4, training loss: 0.8646\n",
      "Epoch 4, Val Loss: 0.9065\n",
      "Epoch: 5, training loss: 0.8534\n",
      "Epoch 5, Val Loss: 0.9000\n",
      "Epoch: 6, training loss: 0.8543\n",
      "Epoch 6, Val Loss: 0.8893\n",
      "Epoch: 7, training loss: 0.8512\n",
      "Epoch 7, Val Loss: 0.8886\n",
      "Epoch: 8, training loss: 0.8459\n",
      "Epoch 8, Val Loss: 0.8938\n",
      "Epoch: 9, training loss: 0.8521\n",
      "Epoch 9, Val Loss: 0.9034\n",
      "Epoch: 10, training loss: 0.8838\n",
      "Epoch 10, Val Loss: 0.9023\n",
      "Fold 4:\n",
      "Epoch: 1, training loss: 0.9935\n",
      "Epoch 1, Val Loss: 0.9461\n",
      "Epoch: 2, training loss: 0.9261\n",
      "Epoch 2, Val Loss: 0.8932\n",
      "Epoch: 3, training loss: 0.8802\n",
      "Epoch 3, Val Loss: 0.8714\n",
      "Epoch: 4, training loss: 0.8647\n",
      "Epoch 4, Val Loss: 0.8796\n",
      "Epoch: 5, training loss: 0.8713\n",
      "Epoch 5, Val Loss: 0.8730\n",
      "Epoch: 6, training loss: 0.8635\n",
      "Epoch 6, Val Loss: 0.8853\n",
      "Epoch: 7, training loss: 0.8588\n",
      "Epoch 7, Val Loss: 0.8774\n",
      "Epoch: 8, training loss: 0.8586\n",
      "Epoch 8, Val Loss: 0.8957\n",
      "Epoch: 9, training loss: 0.8637\n",
      "Epoch 9, Val Loss: 0.8871\n",
      "Epoch: 10, training loss: 0.8519\n",
      "Epoch 10, Val Loss: 0.8781\n",
      "Fold 5:\n",
      "Epoch: 1, training loss: 1.0006\n",
      "Epoch 1, Val Loss: 0.9748\n",
      "Epoch: 2, training loss: 0.9261\n",
      "Epoch 2, Val Loss: 0.9200\n",
      "Epoch: 3, training loss: 0.8830\n",
      "Epoch 3, Val Loss: 0.8942\n",
      "Epoch: 4, training loss: 0.8762\n",
      "Epoch 4, Val Loss: 0.8916\n",
      "Epoch: 5, training loss: 0.8635\n",
      "Epoch 5, Val Loss: 0.8949\n",
      "Epoch: 6, training loss: 0.8590\n",
      "Epoch 6, Val Loss: 0.8990\n",
      "Epoch: 7, training loss: 0.8503\n",
      "Epoch 7, Val Loss: 0.9024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-07 19:16:12,009] Trial 4 finished with value: 0.24378711290808724 and parameters: {'hidden_dim': 127, 'dropout': 0.12949121225019986, 'initial_lr': 1.617435523721079e-05, 'max_lr': 0.020568075584426054, 'criterion': 'BCEWithLogitsLoss', 'pos_weight': 6}. Best is trial 2 with value: 0.3762004050291111.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, training loss: 0.8589\n",
      "Epoch 8, Val Loss: 0.9198\n",
      "Epoch: 9, training loss: 0.8553\n",
      "Epoch 9, Val Loss: 0.9100\n",
      "Epoch: 10, training loss: 0.8613\n",
      "Epoch 10, Val Loss: 0.8996\n",
      "Fold 1:\n",
      "Epoch: 1, training loss: 1.1025\n",
      "Epoch 1, Val Loss: 1.0524\n",
      "Epoch: 2, training loss: 1.0395\n",
      "Epoch 2, Val Loss: 0.9849\n",
      "Epoch: 3, training loss: 0.9877\n",
      "Epoch 3, Val Loss: 0.9494\n",
      "Epoch: 4, training loss: 0.9581\n",
      "Epoch 4, Val Loss: 0.9100\n",
      "Epoch: 5, training loss: 0.9484\n",
      "Epoch 5, Val Loss: 0.9175\n",
      "Epoch: 6, training loss: 0.9326\n",
      "Epoch 6, Val Loss: 0.9182\n",
      "Epoch: 7, training loss: 0.9369\n",
      "Epoch 7, Val Loss: 0.9120\n",
      "Epoch: 8, training loss: 0.9286\n",
      "Epoch 8, Val Loss: 0.9356\n",
      "Epoch: 9, training loss: 0.9317\n",
      "Epoch 9, Val Loss: 0.9269\n",
      "Epoch: 10, training loss: 0.9408\n",
      "Epoch 10, Val Loss: 0.9257\n",
      "Fold 2:\n",
      "Epoch: 1, training loss: 1.0725\n",
      "Epoch 1, Val Loss: 1.0361\n",
      "Epoch: 2, training loss: 1.0056\n",
      "Epoch 2, Val Loss: 0.9949\n",
      "Epoch: 3, training loss: 0.9730\n",
      "Epoch 3, Val Loss: 0.9587\n",
      "Epoch: 4, training loss: 0.9504\n",
      "Epoch 4, Val Loss: 0.9509\n",
      "Epoch: 5, training loss: 0.9395\n",
      "Epoch 5, Val Loss: 0.9466\n",
      "Epoch: 6, training loss: 0.9305\n",
      "Epoch 6, Val Loss: 0.9519\n",
      "Epoch: 7, training loss: 0.9373\n",
      "Epoch 7, Val Loss: 0.9674\n",
      "Epoch: 8, training loss: 0.9281\n",
      "Epoch 8, Val Loss: 0.9596\n",
      "Epoch: 9, training loss: 0.9275\n",
      "Epoch 9, Val Loss: 0.9859\n",
      "Epoch: 10, training loss: 0.9319\n",
      "Epoch 10, Val Loss: 0.9720\n",
      "Fold 3:\n",
      "Epoch: 1, training loss: 1.0765\n",
      "Epoch 1, Val Loss: 1.0466\n",
      "Epoch: 2, training loss: 1.0069\n",
      "Epoch 2, Val Loss: 0.9973\n",
      "Epoch: 3, training loss: 0.9628\n",
      "Epoch 3, Val Loss: 0.9804\n",
      "Epoch: 4, training loss: 0.9401\n",
      "Epoch 4, Val Loss: 0.9684\n",
      "Epoch: 5, training loss: 0.9350\n",
      "Epoch 5, Val Loss: 0.9663\n",
      "Epoch: 6, training loss: 0.9323\n",
      "Epoch 6, Val Loss: 0.9760\n",
      "Epoch: 7, training loss: 0.9239\n",
      "Epoch 7, Val Loss: 0.9814\n",
      "Epoch: 8, training loss: 0.9232\n",
      "Epoch 8, Val Loss: 0.9731\n",
      "Epoch: 9, training loss: 0.9194\n",
      "Epoch 9, Val Loss: 0.9707\n",
      "Epoch: 10, training loss: 0.9274\n",
      "Epoch 10, Val Loss: 0.9778\n",
      "Fold 4:\n",
      "Epoch: 1, training loss: 1.0911\n",
      "Epoch 1, Val Loss: 1.0563\n",
      "Epoch: 2, training loss: 1.0259\n",
      "Epoch 2, Val Loss: 1.0057\n",
      "Epoch: 3, training loss: 0.9786\n",
      "Epoch 3, Val Loss: 0.9594\n",
      "Epoch: 4, training loss: 0.9489\n",
      "Epoch 4, Val Loss: 0.9449\n",
      "Epoch: 5, training loss: 0.9399\n",
      "Epoch 5, Val Loss: 0.9805\n",
      "Epoch: 6, training loss: 0.9493\n",
      "Epoch 6, Val Loss: 0.9653\n",
      "Epoch: 7, training loss: 0.9280\n",
      "Epoch 7, Val Loss: 0.9434\n",
      "Epoch: 8, training loss: 0.9228\n",
      "Epoch 8, Val Loss: 0.9491\n",
      "Epoch: 9, training loss: 0.9283\n",
      "Epoch 9, Val Loss: 0.9523\n",
      "Epoch: 10, training loss: 0.9348\n",
      "Epoch 10, Val Loss: 0.9820\n",
      "Fold 5:\n",
      "Epoch: 1, training loss: 1.0796\n",
      "Epoch 1, Val Loss: 1.0613\n",
      "Epoch: 2, training loss: 1.0175\n",
      "Epoch 2, Val Loss: 1.0263\n",
      "Epoch: 3, training loss: 0.9763\n",
      "Epoch 3, Val Loss: 0.9896\n",
      "Epoch: 4, training loss: 0.9545\n",
      "Epoch 4, Val Loss: 0.9634\n",
      "Epoch: 5, training loss: 0.9452\n",
      "Epoch 5, Val Loss: 0.9592\n",
      "Epoch: 6, training loss: 0.9308\n",
      "Epoch 6, Val Loss: 0.9749\n",
      "Epoch: 7, training loss: 0.9317\n",
      "Epoch 7, Val Loss: 0.9732\n",
      "Epoch: 8, training loss: 0.9292\n",
      "Epoch 8, Val Loss: 0.9734\n",
      "Epoch: 9, training loss: 0.9291\n",
      "Epoch 9, Val Loss: 0.9706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-07 19:16:17,104] Trial 5 finished with value: 0.23368330443469798 and parameters: {'hidden_dim': 96, 'dropout': 0.10528798244406334, 'initial_lr': 7.749479553630793e-05, 'max_lr': 0.01673540104582186, 'criterion': 'BCEWithLogitsLoss', 'pos_weight': 7}. Best is trial 2 with value: 0.3762004050291111.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, training loss: 0.9211\n",
      "Epoch 10, Val Loss: 0.9666\n",
      "Fold 1:\n",
      "Epoch: 1, training loss: 0.3583\n",
      "Epoch 1, Val Loss: 0.3287\n",
      "Epoch: 2, training loss: 0.3235\n",
      "Epoch 2, Val Loss: 0.3139\n",
      "Epoch: 3, training loss: 0.3070\n",
      "Epoch 3, Val Loss: 0.3031\n",
      "Epoch: 4, training loss: 0.2969\n",
      "Epoch 4, Val Loss: 0.2853\n",
      "Epoch: 5, training loss: 0.2911\n",
      "Epoch 5, Val Loss: 0.2849\n",
      "Epoch: 6, training loss: 0.2915\n",
      "Epoch 6, Val Loss: 0.2827\n",
      "Epoch: 7, training loss: 0.2913\n",
      "Epoch 7, Val Loss: 0.2934\n",
      "Epoch: 8, training loss: 0.2926\n",
      "Epoch 8, Val Loss: 0.2868\n",
      "Epoch: 9, training loss: 0.2899\n",
      "Epoch 9, Val Loss: 0.2943\n",
      "Epoch: 10, training loss: 0.2908\n",
      "Epoch 10, Val Loss: 0.2894\n",
      "Fold 2:\n",
      "Epoch: 1, training loss: 0.3812\n",
      "Epoch 1, Val Loss: 0.3282\n",
      "Epoch: 2, training loss: 0.3211\n",
      "Epoch 2, Val Loss: 0.3113\n",
      "Epoch: 3, training loss: 0.3071\n",
      "Epoch 3, Val Loss: 0.3009\n",
      "Epoch: 4, training loss: 0.2971\n",
      "Epoch 4, Val Loss: 0.2970\n",
      "Epoch: 5, training loss: 0.2926\n",
      "Epoch 5, Val Loss: 0.2960\n",
      "Epoch: 6, training loss: 0.2932\n",
      "Epoch 6, Val Loss: 0.2886\n",
      "Epoch: 7, training loss: 0.2930\n",
      "Epoch 7, Val Loss: 0.2925\n",
      "Epoch: 8, training loss: 0.2927\n",
      "Epoch 8, Val Loss: 0.2932\n",
      "Epoch: 9, training loss: 0.2892\n",
      "Epoch 9, Val Loss: 0.2883\n",
      "Epoch: 10, training loss: 0.2892\n",
      "Epoch 10, Val Loss: 0.2900\n",
      "Fold 3:\n",
      "Epoch: 1, training loss: 0.4166\n",
      "Epoch 1, Val Loss: 0.3294\n",
      "Epoch: 2, training loss: 0.3262\n",
      "Epoch 2, Val Loss: 0.3136\n",
      "Epoch: 3, training loss: 0.3111\n",
      "Epoch 3, Val Loss: 0.3051\n",
      "Epoch: 4, training loss: 0.2989\n",
      "Epoch 4, Val Loss: 0.3010\n",
      "Epoch: 5, training loss: 0.2897\n",
      "Epoch 5, Val Loss: 0.2947\n",
      "Epoch: 6, training loss: 0.2909\n",
      "Epoch 6, Val Loss: 0.2945\n",
      "Epoch: 7, training loss: 0.2891\n",
      "Epoch 7, Val Loss: 0.3014\n",
      "Epoch: 8, training loss: 0.2875\n",
      "Epoch 8, Val Loss: 0.2940\n",
      "Epoch: 9, training loss: 0.2879\n",
      "Epoch 9, Val Loss: 0.2946\n",
      "Epoch: 10, training loss: 0.2861\n",
      "Epoch 10, Val Loss: 0.2954\n",
      "Fold 4:\n",
      "Epoch: 1, training loss: 0.4142\n",
      "Epoch 1, Val Loss: 0.3298\n",
      "Epoch: 2, training loss: 0.3224\n",
      "Epoch 2, Val Loss: 0.3122\n",
      "Epoch: 3, training loss: 0.3088\n",
      "Epoch 3, Val Loss: 0.2984\n",
      "Epoch: 4, training loss: 0.3008\n",
      "Epoch 4, Val Loss: 0.2902\n",
      "Epoch: 5, training loss: 0.2919\n",
      "Epoch 5, Val Loss: 0.2889\n",
      "Epoch: 6, training loss: 0.2922\n",
      "Epoch 6, Val Loss: 0.2922\n",
      "Epoch: 7, training loss: 0.2897\n",
      "Epoch 7, Val Loss: 0.2908\n",
      "Epoch: 8, training loss: 0.2931\n",
      "Epoch 8, Val Loss: 0.2894\n",
      "Epoch: 9, training loss: 0.2940\n",
      "Epoch 9, Val Loss: 0.2881\n",
      "Epoch: 10, training loss: 0.2910\n",
      "Epoch 10, Val Loss: 0.3027\n",
      "Fold 5:\n",
      "Epoch: 1, training loss: 0.3907\n",
      "Epoch 1, Val Loss: 0.3374\n",
      "Epoch: 2, training loss: 0.3230\n",
      "Epoch 2, Val Loss: 0.3171\n",
      "Epoch: 3, training loss: 0.3080\n",
      "Epoch 3, Val Loss: 0.3052\n",
      "Epoch: 4, training loss: 0.2955\n",
      "Epoch 4, Val Loss: 0.2969\n",
      "Epoch: 5, training loss: 0.2958\n",
      "Epoch 5, Val Loss: 0.2944\n",
      "Epoch: 6, training loss: 0.2882\n",
      "Epoch 6, Val Loss: 0.2967\n",
      "Epoch: 7, training loss: 0.2932\n",
      "Epoch 7, Val Loss: 0.3029\n",
      "Epoch: 8, training loss: 0.2926\n",
      "Epoch 8, Val Loss: 0.2947\n",
      "Epoch: 9, training loss: 0.2910\n",
      "Epoch 9, Val Loss: 0.2972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-07 19:16:22,971] Trial 6 finished with value: 0.3735721400524068 and parameters: {'hidden_dim': 77, 'dropout': 0.2975237045589857, 'initial_lr': 0.0008177914385283412, 'max_lr': 0.03359740840158348, 'criterion': 'BCEWithLogitsLoss', 'pos_weight': 1}. Best is trial 2 with value: 0.3762004050291111.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, training loss: 0.2896\n",
      "Epoch 10, Val Loss: 0.2962\n",
      "Fold 1:\n",
      "Epoch: 1, training loss: 1.3704\n",
      "Epoch 1, Val Loss: 1.3306\n",
      "Epoch: 2, training loss: 1.3505\n",
      "Epoch 2, Val Loss: 1.3197\n",
      "Epoch: 3, training loss: 1.3554\n",
      "Epoch 3, Val Loss: 1.3092\n",
      "Epoch: 4, training loss: 1.3425\n",
      "Epoch 4, Val Loss: 1.2984\n",
      "Epoch: 5, training loss: 1.3268\n",
      "Epoch 5, Val Loss: 1.2892\n",
      "Epoch: 6, training loss: 1.3134\n",
      "Epoch 6, Val Loss: 1.2810\n",
      "Epoch: 7, training loss: 1.3090\n",
      "Epoch 7, Val Loss: 1.2736\n",
      "Epoch: 8, training loss: 1.3052\n",
      "Epoch 8, Val Loss: 1.2668\n",
      "Epoch: 9, training loss: 1.2996\n",
      "Epoch 9, Val Loss: 1.2596\n",
      "Epoch: 10, training loss: 1.2820\n",
      "Epoch 10, Val Loss: 1.2516\n",
      "Fold 2:\n",
      "Epoch: 1, training loss: 1.3588\n",
      "Epoch 1, Val Loss: 1.3555\n",
      "Epoch: 2, training loss: 1.3479\n",
      "Epoch 2, Val Loss: 1.3518\n",
      "Epoch: 3, training loss: 1.3487\n",
      "Epoch 3, Val Loss: 1.3474\n",
      "Epoch: 4, training loss: 1.3440\n",
      "Epoch 4, Val Loss: 1.3420\n",
      "Epoch: 5, training loss: 1.3363\n",
      "Epoch 5, Val Loss: 1.3367\n",
      "Epoch: 6, training loss: 1.3262\n",
      "Epoch 6, Val Loss: 1.3311\n",
      "Epoch: 7, training loss: 1.3168\n",
      "Epoch 7, Val Loss: 1.3246\n",
      "Epoch: 8, training loss: 1.3167\n",
      "Epoch 8, Val Loss: 1.3176\n",
      "Epoch: 9, training loss: 1.3085\n",
      "Epoch 9, Val Loss: 1.3101\n",
      "Epoch: 10, training loss: 1.2923\n",
      "Epoch 10, Val Loss: 1.3011\n",
      "Fold 3:\n",
      "Epoch: 1, training loss: 1.3403\n",
      "Epoch 1, Val Loss: 1.3443\n",
      "Epoch: 2, training loss: 1.3419\n",
      "Epoch 2, Val Loss: 1.3396\n",
      "Epoch: 3, training loss: 1.3341\n",
      "Epoch 3, Val Loss: 1.3338\n",
      "Epoch: 4, training loss: 1.3262\n",
      "Epoch 4, Val Loss: 1.3271\n",
      "Epoch: 5, training loss: 1.3336\n",
      "Epoch 5, Val Loss: 1.3210\n",
      "Epoch: 6, training loss: 1.3074\n",
      "Epoch 6, Val Loss: 1.3160\n",
      "Epoch: 7, training loss: 1.3068\n",
      "Epoch 7, Val Loss: 1.3103\n",
      "Epoch: 8, training loss: 1.3039\n",
      "Epoch 8, Val Loss: 1.3043\n",
      "Epoch: 9, training loss: 1.2954\n",
      "Epoch 9, Val Loss: 1.2980\n",
      "Epoch: 10, training loss: 1.2898\n",
      "Epoch 10, Val Loss: 1.2915\n",
      "Fold 4:\n",
      "Epoch: 1, training loss: 1.4168\n",
      "Epoch 1, Val Loss: 1.4158\n",
      "Epoch: 2, training loss: 1.3979\n",
      "Epoch 2, Val Loss: 1.4004\n",
      "Epoch: 3, training loss: 1.3927\n",
      "Epoch 3, Val Loss: 1.3834\n",
      "Epoch: 4, training loss: 1.3717\n",
      "Epoch 4, Val Loss: 1.3657\n",
      "Epoch: 5, training loss: 1.3531\n",
      "Epoch 5, Val Loss: 1.3496\n",
      "Epoch: 6, training loss: 1.3307\n",
      "Epoch 6, Val Loss: 1.3344\n",
      "Epoch: 7, training loss: 1.3288\n",
      "Epoch 7, Val Loss: 1.3217\n",
      "Epoch: 8, training loss: 1.3189\n",
      "Epoch 8, Val Loss: 1.3098\n",
      "Epoch: 9, training loss: 1.3026\n",
      "Epoch 9, Val Loss: 1.2979\n",
      "Epoch: 10, training loss: 1.2909\n",
      "Epoch 10, Val Loss: 1.2865\n",
      "Fold 5:\n",
      "Epoch: 1, training loss: 1.3323\n",
      "Epoch 1, Val Loss: 1.3496\n",
      "Epoch: 2, training loss: 1.3374\n",
      "Epoch 2, Val Loss: 1.3458\n",
      "Epoch: 3, training loss: 1.3293\n",
      "Epoch 3, Val Loss: 1.3426\n",
      "Epoch: 4, training loss: 1.3238\n",
      "Epoch 4, Val Loss: 1.3383\n",
      "Epoch: 5, training loss: 1.3256\n",
      "Epoch 5, Val Loss: 1.3340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-04-07 19:16:51,321] Trial 7 failed with parameters: {'hidden_dim': 18, 'dropout': 0.30357848033207613, 'initial_lr': 0.000422757556471017, 'max_lr': 0.00105278018378025, 'criterion': 'BCEWithLogitsLoss', 'pos_weight': 10} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tanle\\AppData\\Local\\Temp\\ipykernel_28972\\3996966297.py\", line 52, in maximise_combined_score\n",
      "    model, accuracy, precision, recall, f1, auc = train_and_evaluate(\n",
      "                                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tanle\\AppData\\Local\\Temp\\ipykernel_28972\\6689063.py\", line 23, in train_and_evaluate\n",
      "    for batch, (inputs, labels) in enumerate(train_loader,start=1):\n",
      "  File \"d:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 708, in __next__\n",
      "    data = self._next_data()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 764, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 55, in fetch\n",
      "    return self.collate_fn(data)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 398, in default_collate\n",
      "    return collate(batch, collate_fn_map=default_collate_fn_map)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 211, in collate\n",
      "    return [\n",
      "           ^\n",
      "  File \"d:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 212, in <listcomp>\n",
      "    collate(samples, collate_fn_map=collate_fn_map)\n",
      "  File \"d:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 155, in collate\n",
      "    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 272, in collate_tensor_fn\n",
      "    return torch.stack(batch, 0, out=out)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2025-04-07 19:16:51,325] Trial 7 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[60]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      4\u001b[39m storage = optuna.storages.InMemoryStorage()\n\u001b[32m      6\u001b[39m study = optuna.create_study(direction=\u001b[33m\"\u001b[39m\u001b[33mmaximize\u001b[39m\u001b[33m\"\u001b[39m,storage=storage,  \u001b[38;5;66;03m# Specify the storage URL here.\u001b[39;00m\n\u001b[32m      7\u001b[39m     study_name=\u001b[33m\"\u001b[39m\u001b[33mBasic\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43mstudy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaximise_combined_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# You can adjust the number of trials\u001b[39;00m\n\u001b[32m      9\u001b[39m run_server(storage)\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBest trial:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\optuna\\study\\study.py:475\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    373\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    374\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    375\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    382\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    383\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    384\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    385\u001b[39m \n\u001b[32m    386\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    473\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    474\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:63\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     76\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     frozen_trial = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    163\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:248\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    241\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    244\u001b[39m     frozen_trial.state == TrialState.FAIL\n\u001b[32m    245\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    246\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    247\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m248\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:197\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    196\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    198\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    199\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    200\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[57]\u001b[39m\u001b[32m, line 52\u001b[39m, in \u001b[36mmaximise_combined_score\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# Train and evaluate the model on the current fold\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m model, accuracy, precision, recall, f1, auc = \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimiser\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# Append the metrics from the current fold\u001b[39;00m\n\u001b[32m     57\u001b[39m accuracy_list.append(accuracy)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[59]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mtrain_and_evaluate\u001b[39m\u001b[34m(model, criterion, optimiser, scheduler, train_loader, val_loader, epochs, patience, device)\u001b[39m\n\u001b[32m     21\u001b[39m running_loss = \u001b[32m0.0\u001b[39m \u001b[38;5;66;03m#? loss for this epoch\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m#* Mini-batch training loop\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimiser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#? Zero the gradients\u001b[39;49;00m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#? Forward pass through the model\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    763\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    765\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    766\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:398\u001b[39m, in \u001b[36mdefault_collate\u001b[39m\u001b[34m(batch)\u001b[39m\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdefault_collate\u001b[39m(batch):\n\u001b[32m    338\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    339\u001b[39m \u001b[33;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[32m    340\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    396\u001b[39m \u001b[33;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[32m    397\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:211\u001b[39m, in \u001b[36mcollate\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    208\u001b[39m transposed = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(*batch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m211\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtransposed\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:212\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    208\u001b[39m transposed = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(*batch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m         \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    213\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[32m    214\u001b[39m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:155\u001b[39m, in \u001b[36mcollate\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    157\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[32m    158\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:272\u001b[39m, in \u001b[36mcollate_tensor_fn\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    270\u001b[39m     storage = elem._typed_storage()._new_shared(numel, device=elem.device)\n\u001b[32m    271\u001b[39m     out = elem.new(storage).resize_(\u001b[38;5;28mlen\u001b[39m(batch), *\u001b[38;5;28mlist\u001b[39m(elem.size()))\n\u001b[32m--> \u001b[39m\u001b[32m272\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from optuna_dashboard import run_server\n",
    "\n",
    "storage = optuna.storages.InMemoryStorage()\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\",storage=storage,  # Specify the storage URL here.\n",
    "    study_name=\"Basic\")\n",
    "study.optimize(maximise_combined_score, n_trials=30)  # You can adjust the number of trials\n",
    "run_server(storage)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(f\"  Combined score: {trial.value}\")\n",
    "print(\"  Best hyperparameters:\")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3fea50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
