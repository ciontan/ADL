{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7ea3e5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import (\n",
    "    MaxAbsScaler,\n",
    "    MinMaxScaler,\n",
    "    Normalizer,\n",
    "    PowerTransformer,\n",
    "    QuantileTransformer,\n",
    "    RobustScaler,\n",
    "    StandardScaler,\n",
    "    minmax_scale,\n",
    ")\n",
    "from sklearn.metrics import recall_score, accuracy_score,f1_score, precision_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import optuna\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "befd96c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "randomState = 42\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "raw_dataset = pd.read_csv(\"./data/processed_data.csv\") #data has X and Y\n",
    "X = raw_dataset.drop(columns=[\"DR\"])\n",
    "Y = pd.DataFrame(raw_dataset[\"DR\"])\n",
    "\n",
    "#* 90/10 split for training and final test\n",
    "X_FOR_FOLDS, X_FINAL_TEST, Y_FOR_FOLDS, Y_FINAL_TEST = train_test_split(X, Y, test_size=0.1, random_state=randomState, stratify=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9c9c23e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FOLDS_GENERATOR(X, Y, normalisation_method=MinMaxScaler(), n_splits=5, randomState=None, oversample=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    Generates stratified folds with specified normalization.\n",
    "    \n",
    "    For list of scalers, see:\n",
    "    https://scikit-learn.org/stable/api/sklearn.preprocessing.html\n",
    "    \n",
    "    For more details on scaling and normalization effects, see:\n",
    "    https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#\n",
    "    \n",
    "    normalisation_method should be an instance of a scaler, e.g.,\n",
    "    - MinMaxScaler()\n",
    "    - MaxAbsScaler()\n",
    "    - Quantile_Transform(output_distribution='uniform')\n",
    "    \n",
    "    Returns a list of tuples, each containing:\n",
    "    (X_train_scaled, X_test_scaled, Y_train, Y_test), representing data for each fold\n",
    "    \"\"\"\n",
    "    kF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=randomState)\n",
    "    kFolds_list = []\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(kF.split(X, Y)):\n",
    "        # Split the data into training and testing sets for this fold\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        Y_train, Y_test = Y.iloc[train_idx], Y.iloc[test_idx]\n",
    "        \n",
    "        # Fit the scaler on the training data and transform both train and test sets\n",
    "        X_train_scaled = normalisation_method.fit_transform(X_train)\n",
    "        X_test_scaled = normalisation_method.transform(X_test)\n",
    "        \n",
    "        if oversample:\n",
    "            # Oversample the training data if needed (e.g., using SMOTE or similar techniques)\n",
    "            # This is a placeholder; actual oversampling code should be implemented here\n",
    "            # X_train_scaled....\n",
    "            pass\n",
    "        \n",
    "        # Convert back to DataFrame to maintain column names\n",
    "        X_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns, index=X_train.index)\n",
    "        X_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns, index=X_test.index)\n",
    "        \n",
    "        # Ensure 'gender' is still binary (0 or 1)\n",
    "        if X_train_scaled['Gender'].isin([0, 1]).all():\n",
    "            kFolds_list.append((X_train_scaled, X_test_scaled, Y_train, Y_test))\n",
    "        else:\n",
    "            print(\"Warning: 'gender' column contains unexpected values after scaling.\") \n",
    "               \n",
    "        print(f\"Fold: {fold+1}, Train: {kFolds_list[fold][0].shape}, Test: {kFolds_list[fold][1].shape}\")   \n",
    "    return kFolds_list\n",
    "\n",
    "def init_weights(model): #tested already\n",
    "    if isinstance(model, nn.Linear):  # Apply only to linear layers\n",
    "        nn.init.xavier_uniform_(model.weight)\n",
    "        if model.bias is not None:\n",
    "            nn.init.zeros_(model.bias)\n",
    "            \n",
    "def fold_to_dataloader_tensor(train_x, test_x, train_y, test_y, batch_size=64, device=device):\n",
    "    train_dataset = TensorDataset(\n",
    "        torch.tensor(train_x.values,dtype=torch.float32).to(device), \n",
    "        torch.tensor(train_y.values,dtype=torch.float32).to(device))\n",
    "    val_dataset = TensorDataset(\n",
    "        torch.tensor(test_x.values,dtype=torch.float32).to(device), \n",
    "        torch.tensor(test_y.values,dtype=torch.float32).to(device))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=False)\n",
    "    return train_loader, val_loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e6f6eff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1, Train: (4593, 28), Test: (1149, 28)\n",
      "Fold: 2, Train: (4593, 28), Test: (1149, 28)\n",
      "Fold: 3, Train: (4594, 28), Test: (1148, 28)\n",
      "Fold: 4, Train: (4594, 28), Test: (1148, 28)\n",
      "Fold: 5, Train: (4594, 28), Test: (1148, 28)\n"
     ]
    }
   ],
   "source": [
    "kFolds = FOLDS_GENERATOR(X_FOR_FOLDS, Y_FOR_FOLDS, normalisation_method=MinMaxScaler(), n_splits=5, randomState=randomState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e65d95ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from modularModels1 import BlockMaker, modularNN, BasicModel\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using\", device)\n",
    "\n",
    "def init_weights(model): #tested already\n",
    "    if isinstance(model, nn.Linear):  # Apply only to linear layers\n",
    "        nn.init.xavier_uniform_(model.weight)\n",
    "        if model.bias is not None:\n",
    "            nn.init.zeros_(model.bias)\n",
    "            \n",
    "def fold_to_dataloader_tensor(train_x, test_x, train_y, test_y, batch_size=64, device=device):\n",
    "    train_dataset = TensorDataset(\n",
    "        torch.tensor(train_x.values,dtype=torch.float32).to(device), \n",
    "        torch.tensor(train_y.values,dtype=torch.float32).to(device))\n",
    "    val_dataset = TensorDataset(\n",
    "        torch.tensor(test_x.values,dtype=torch.float32).to(device), \n",
    "        torch.tensor(test_y.values,dtype=torch.float32).to(device))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=False)\n",
    "    return train_loader, val_loader \n",
    "\n",
    "\n",
    "def get_feature_count(loader):\n",
    "    \"\"\"returns the number of features in the dataset\"\"\"\n",
    "    return next(iter(loader))[0].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9231fa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Criterion_Models import *\n",
    "def criterion_mapping(criterion_choice:str, pos_weight:float=None):\n",
    "    \"\"\"\n",
    "    Feel free to add any custom loss functions here.\n",
    "    returns function for criterion\n",
    "    \"\"\"\n",
    "    if criterion_choice == \"FocalLoss\":\n",
    "        return FocalLoss()\n",
    "    elif criterion_choice == \"DiceLoss\":\n",
    "        return DiceLoss()\n",
    "    elif criterion_choice == \"BCEWithLogitsLoss\":\n",
    "        return nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight])) if pos_weight else nn.BCEWithLogitsLoss()\n",
    "    return nn.BCEWithLogitsLoss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2f5043f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            # nn.Sigmoid()\n",
    "   \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "    def last_layer(self):\n",
    "        return self.net[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2c9450e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_model = BinaryClassifier(input_dim=get_feature_count(train_loader), hidden_dim=64, dropout=0.5).to(device)\n",
    "# print(get_feature_count(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "568e7848",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, criterion, optimiser, scheduler, train_loader, val_loader, epochs=20, patience=5, device=device):\n",
    "    if isinstance(model.last_layer(), nn.Sigmoid) and isinstance(criterion, nn.BCEWithLogitsLoss):\n",
    "        raise ValueError(\"Model output is Sigmoid but criterion is BCEWithLogitsLoss. Please check your model and criterion compatibility.\")\n",
    "\n",
    "    \n",
    "    accuracy_list = []\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    f1_list = []\n",
    "    auc_list = []\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    wait = 0\n",
    "\n",
    "    #* Set model to training mode: essential for dropout and batch norm layers\n",
    "    model.train()\n",
    "    #* Epoch Training loop for this fold\n",
    "    for epoch in range(1,epochs+1):\n",
    "        running_loss = 0.0 #? loss for this epoch\n",
    "        #* Mini-batch training loop\n",
    "        for batch, (inputs, labels) in enumerate(train_loader,start=1):\n",
    "            optimiser.zero_grad() #? Zero the gradients\n",
    "            outputs = model(inputs) #? Forward pass through the model\n",
    "            loss = criterion(outputs, labels) #? Calculate loss\n",
    "            loss.backward() #? Backpropagation\n",
    "            running_loss += loss.item()\n",
    "            optimiser.step() #? Update weights\n",
    "            if scheduler:\n",
    "                scheduler.step()\n",
    "                \n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch: {epoch}, training loss: {train_loss:.4f}\")\n",
    "    \n",
    "        #* Now we evaluate the model on the validation set, to track training vs validation loss\n",
    "        model.eval() #? Set model to evaluation mode\n",
    "        with torch.no_grad(): #? No need to track gradients during evaluation\n",
    "            val_loss = 0.0    \n",
    "            for batch, (inputs, labels) in enumerate(val_loader,start=1):#! one pass because val_loader batch size is all, if you want to do it in mini-batches, you MUST change the metric calculations to accept mini-batches\n",
    "                outputs = model(inputs)\n",
    "                if isinstance(model.last_layer(), nn.Sigmoid):\n",
    "                    predictions = (outputs > 0.5).float().cpu() #? assume model output is 1s and 0s\n",
    "                else: #? if model output is logits, convert to binary predictions\n",
    "                    predictions = (torch.sigmoid(outputs) > 0.5).float().cpu()\n",
    "                labels = labels.cpu() \n",
    "                loss = criterion(predictions, labels)\n",
    "                val_loss += loss.item() #? Calculate loss\n",
    "                avg_val_loss = val_loss / len(val_loader)\n",
    "                print(f\"Epoch {epoch}, Val Loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "                # Early stopping\n",
    "                if avg_val_loss < best_val_loss:\n",
    "                    best_val_loss = avg_val_loss\n",
    "                    best_model_state = model.state_dict()\n",
    "                    wait = 0\n",
    "                else:\n",
    "                    wait += 1\n",
    "                    if wait >= patience:\n",
    "                        print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "                        break\n",
    "    \n",
    "    #* Use best model to calculate metrics on the validation set\n",
    "    #! must be outside epoch loop, it comes after the training and cv loop\n",
    "    model.load_state_dict(best_model_state) #? Load the best model state\n",
    "    with torch.no_grad():\n",
    "        for batch, (inputs, labels) in enumerate(val_loader,start=1):#! one pass because val_loader batch size is all, if you want to do it in mini-batches, you MUST change the metric calculations to accept mini-batches\n",
    "                outputs = model(inputs)\n",
    "                if isinstance(model.last_layer(), nn.Sigmoid):\n",
    "                    predictions = (outputs > 0.5).float().cpu() #? assume model output is 1s and 0s\n",
    "                else: #? if model output is logits, convert to binary predictions\n",
    "                    predictions = (torch.sigmoid(outputs) > 0.5).float().cpu()\n",
    "                labels = labels.cpu() \n",
    "                loss = criterion(predictions, labels)\n",
    "                val_loss += loss.item() #? Calculate loss\n",
    "                \n",
    "    #! The following should have length equal to fold number           \n",
    "    accuracy_list.append(accuracy_score(labels, predictions)) \n",
    "    precision_list.append(precision_score(labels, predictions, pos_label=1, zero_division=0)) \n",
    "    recall_list.append(recall_score(labels, predictions, pos_label=1))\n",
    "    f1_list.append(f1_score(labels, predictions, pos_label=1))\n",
    "    auc_list.append(roc_auc_score(labels, predictions)) \n",
    "\n",
    "    return model, accuracy_list, precision_list, recall_list, f1_list, auc_list \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "437927c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximise_combined_score(trial):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # Model hyperparameters (first-level optimization)\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 16, 128)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    initial_lr = trial.suggest_float(\"initial_lr\", 1e-5, 1e-3, log=True)\n",
    "    max_lr = trial.suggest_float(\"max_lr\", 1e-3, 1e-1, log=True)\n",
    "    \n",
    "    # Loss function hyperparameters\n",
    "    criterion_choice = trial.suggest_categorical(\"criterion\", [\"BCEWithLogitsLoss\", \"FocalLoss\", \"DiceLoss\"])\n",
    "    \n",
    "    # Hyperparameter exploration optimization\n",
    "    if criterion_choice == \"BCEWithLogitsLoss\":\n",
    "        pos_weight = trial.suggest_int(\"pos_weight\", 1, 10)\n",
    "    else:\n",
    "        pos_weight = None\n",
    "    \n",
    "    # Initialize lists for metrics across folds\n",
    "    accuracy_list = []\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    f1_list = []\n",
    "    auc_list = []\n",
    "\n",
    "    # Cross-validation loop\n",
    "    for fold, (train_x, test_x, train_y, test_y) in enumerate(kFolds, start=1):\n",
    "        # Create DataLoader for current fold\n",
    "        train_loader, val_loader = fold_to_dataloader_tensor(train_x, test_x, train_y, test_y, batch_size=64, device=device)\n",
    "        # Calculate steps_per_epoch from the current fold's train_loader\n",
    "        train_loader_len = len(train_loader)\n",
    "        \n",
    "        # Instantiate and initialize the model\n",
    "        model = BinaryClassifier(input_dim=get_feature_count(train_loader), hidden_dim=hidden_dim, dropout=dropout)\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model.to(device)\n",
    "        model.apply(init_weights)\n",
    "        \n",
    "        # Map the choice to the actual loss function\n",
    "        criterion = criterion_mapping(criterion_choice, pos_weight).to(device)\n",
    "        optimiser = optim.Adam(model.parameters(), lr=initial_lr)\n",
    "        \n",
    "        # Initialize scheduler\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimiser,\n",
    "            max_lr=max_lr,\n",
    "            steps_per_epoch=train_loader_len,\n",
    "            epochs=100,\n",
    "            anneal_strategy='linear'\n",
    "        )\n",
    "        print(f\"Fold {fold}:\")\n",
    "        # Train and evaluate the model on the current fold\n",
    "        model, accuracy, precision, recall, f1, auc = train_and_evaluate(\n",
    "            model, criterion, optimiser, scheduler, train_loader, val_loader, epochs=10, patience=10, device=device\n",
    "        )\n",
    "\n",
    "        # Append the metrics from the current fold\n",
    "        accuracy_list.append(accuracy)\n",
    "        precision_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "        f1_list.append(f1)\n",
    "        auc_list.append(auc)\n",
    "\n",
    "    # Calculate the average metrics across all folds\n",
    "    avg_accuracy = np.sum(accuracy_list) / len(accuracy_list)\n",
    "    avg_precision = np.sum(precision_list) / len(precision_list)\n",
    "    avg_recall = np.sum(recall_list) / len(recall_list)\n",
    "    avg_f1 = np.sum(f1_list) / len(f1_list)\n",
    "    avg_auc = np.sum(auc_list) / len(auc_list)\n",
    "\n",
    "    # Combine metrics into a single \"score\"\n",
    "    combined_score = (avg_f1 + avg_precision + avg_recall + avg_accuracy + avg_auc) / 5\n",
    "\n",
    "    return combined_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8cfd0e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-06 13:37:28,964] A new study created in memory with name: Basic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1:\n",
      "Epoch: 1, training loss: 0.8288\n",
      "Epoch 1, Val Loss: 0.8166\n",
      "Epoch: 2, training loss: 0.8047\n",
      "Epoch 2, Val Loss: 0.8166\n",
      "Epoch: 3, training loss: 0.7719\n",
      "Epoch 3, Val Loss: 0.7675\n",
      "Epoch: 4, training loss: 0.8283\n",
      "Epoch 4, Val Loss: 0.7571\n",
      "Epoch: 5, training loss: 0.8357\n",
      "Epoch 5, Val Loss: 1.0000\n",
      "Epoch: 6, training loss: 0.8246\n",
      "Epoch 6, Val Loss: 1.0000\n",
      "Epoch: 7, training loss: 0.8251\n",
      "Epoch 7, Val Loss: 1.0000\n",
      "Epoch: 8, training loss: 0.8230\n",
      "Epoch 8, Val Loss: 1.0000\n",
      "Epoch: 9, training loss: 0.8225\n",
      "Epoch 9, Val Loss: 1.0000\n",
      "Epoch: 10, training loss: 0.8226\n",
      "Epoch 10, Val Loss: 1.0000\n",
      "Fold 2:\n",
      "Epoch: 1, training loss: 1.5465\n",
      "Epoch 1, Val Loss: 0.8337\n",
      "Epoch: 2, training loss: 0.9339\n",
      "Epoch 2, Val Loss: 0.8370\n",
      "Epoch: 3, training loss: 0.9264\n",
      "Epoch 3, Val Loss: 0.8378\n",
      "Epoch: 4, training loss: 0.9227\n",
      "Epoch 4, Val Loss: 0.8365\n",
      "Epoch: 5, training loss: 0.9228\n",
      "Epoch 5, Val Loss: 0.8367\n",
      "Epoch: 6, training loss: 0.9211\n",
      "Epoch 6, Val Loss: 0.8373\n",
      "Epoch: 7, training loss: 0.9098\n",
      "Epoch 7, Val Loss: 0.8331\n",
      "Epoch: 8, training loss: 0.8900\n",
      "Epoch 8, Val Loss: 0.8264\n",
      "Epoch: 9, training loss: 0.8768\n",
      "Epoch 9, Val Loss: 0.8320\n",
      "Epoch: 10, training loss: 0.8683\n",
      "Epoch 10, Val Loss: 0.8289\n",
      "Fold 3:\n",
      "Epoch: 1, training loss: 0.9893\n",
      "Epoch 1, Val Loss: 0.8298\n",
      "Epoch: 2, training loss: 0.9142\n",
      "Epoch 2, Val Loss: 0.8252\n",
      "Epoch: 3, training loss: 0.9006\n",
      "Epoch 3, Val Loss: 0.8225\n",
      "Epoch: 4, training loss: 0.8846\n",
      "Epoch 4, Val Loss: 0.8185\n",
      "Epoch: 5, training loss: 0.8724\n",
      "Epoch 5, Val Loss: 0.8245\n",
      "Epoch: 6, training loss: 0.8581\n",
      "Epoch 6, Val Loss: 0.8201\n",
      "Epoch: 7, training loss: 0.8422\n",
      "Epoch 7, Val Loss: 0.8101\n",
      "Epoch: 8, training loss: 0.8292\n",
      "Epoch 8, Val Loss: 0.8076\n",
      "Epoch: 9, training loss: 0.8156\n",
      "Epoch 9, Val Loss: 0.7993\n",
      "Epoch: 10, training loss: 0.7990\n",
      "Epoch 10, Val Loss: 0.7941\n",
      "Fold 4:\n",
      "Epoch: 1, training loss: 0.8586\n",
      "Epoch 1, Val Loss: 0.8098\n",
      "Epoch: 2, training loss: 0.8129\n",
      "Epoch 2, Val Loss: 0.8144\n",
      "Epoch: 3, training loss: 0.7867\n",
      "Epoch 3, Val Loss: 0.8108\n",
      "Epoch: 4, training loss: 0.7439\n",
      "Epoch 4, Val Loss: 0.7633\n",
      "Epoch: 5, training loss: 0.6616\n",
      "Epoch 5, Val Loss: 1.0000\n",
      "Epoch: 6, training loss: 0.8103\n",
      "Epoch 6, Val Loss: 1.0000\n",
      "Epoch: 7, training loss: 0.8069\n",
      "Epoch 7, Val Loss: 1.0000\n",
      "Epoch: 8, training loss: 0.8011\n",
      "Epoch 8, Val Loss: 1.0000\n",
      "Epoch: 9, training loss: 0.7879\n",
      "Epoch 9, Val Loss: 1.0000\n",
      "Epoch: 10, training loss: 0.7672\n",
      "Epoch 10, Val Loss: 1.0000\n",
      "Fold 5:\n",
      "Epoch: 1, training loss: 4.7142\n",
      "Epoch 1, Val Loss: 0.8226\n",
      "Epoch: 2, training loss: 0.9426\n",
      "Epoch 2, Val Loss: 0.8236\n",
      "Epoch: 3, training loss: 0.9269\n",
      "Epoch 3, Val Loss: 0.8140\n",
      "Epoch: 4, training loss: 0.8565\n",
      "Epoch 4, Val Loss: 0.8165\n",
      "Epoch: 5, training loss: 0.8168\n",
      "Epoch 5, Val Loss: 0.8165\n",
      "Epoch: 6, training loss: 0.7995\n",
      "Epoch 6, Val Loss: 0.8165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-06 13:37:32,376] Trial 0 finished with value: 0.3463750362400894 and parameters: {'hidden_dim': 25, 'dropout': 0.32749284446582455, 'initial_lr': 0.00037872885677779264, 'max_lr': 0.01066130234037569, 'criterion': 'DiceLoss'}. Best is trial 0 with value: 0.3463750362400894.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, training loss: 0.7823\n",
      "Epoch 7, Val Loss: 0.8162\n",
      "Epoch: 8, training loss: 0.7634\n",
      "Epoch 8, Val Loss: 0.8118\n",
      "Epoch: 9, training loss: 0.7464\n",
      "Epoch 9, Val Loss: 0.8086\n",
      "Epoch: 10, training loss: 0.7292\n",
      "Epoch 10, Val Loss: 0.8055\n",
      "Fold 1:\n",
      "Epoch: 1, training loss: 0.8184\n",
      "Epoch 1, Val Loss: 0.9031\n",
      "Epoch: 2, training loss: 0.7508\n",
      "Epoch 2, Val Loss: 0.8896\n",
      "Epoch: 3, training loss: 0.7139\n",
      "Epoch 3, Val Loss: 0.9069\n",
      "Epoch: 4, training loss: 0.6979\n",
      "Epoch 4, Val Loss: 0.8934\n",
      "Epoch: 5, training loss: 0.7055\n",
      "Epoch 5, Val Loss: 0.8861\n",
      "Epoch: 6, training loss: 0.6818\n",
      "Epoch 6, Val Loss: 0.8937\n",
      "Epoch: 7, training loss: 0.6820\n",
      "Epoch 7, Val Loss: 0.9139\n",
      "Epoch: 8, training loss: 0.6841\n",
      "Epoch 8, Val Loss: 0.8913\n",
      "Epoch: 9, training loss: 0.6872\n",
      "Epoch 9, Val Loss: 0.8948\n",
      "Epoch: 10, training loss: 0.6822\n",
      "Epoch 10, Val Loss: 0.8975\n",
      "Fold 2:\n",
      "Epoch: 1, training loss: 0.7937\n",
      "Epoch 1, Val Loss: 0.9031\n",
      "Epoch: 2, training loss: 0.7296\n",
      "Epoch 2, Val Loss: 0.9014\n",
      "Epoch: 3, training loss: 0.7025\n",
      "Epoch 3, Val Loss: 0.9044\n",
      "Epoch: 4, training loss: 0.6848\n",
      "Epoch 4, Val Loss: 0.9033\n",
      "Epoch: 5, training loss: 0.6810\n",
      "Epoch 5, Val Loss: 0.8929\n",
      "Epoch: 6, training loss: 0.6906\n",
      "Epoch 6, Val Loss: 0.8992\n",
      "Epoch: 7, training loss: 0.7004\n",
      "Epoch 7, Val Loss: 0.8967\n",
      "Epoch: 8, training loss: 0.6752\n",
      "Epoch 8, Val Loss: 0.8999\n",
      "Epoch: 9, training loss: 0.6842\n",
      "Epoch 9, Val Loss: 0.8966\n",
      "Epoch: 10, training loss: 0.6931\n",
      "Epoch 10, Val Loss: 0.9031\n",
      "Fold 3:\n",
      "Epoch: 1, training loss: 0.8154\n",
      "Epoch 1, Val Loss: 0.9033\n",
      "Epoch: 2, training loss: 0.7353\n",
      "Epoch 2, Val Loss: 0.8982\n",
      "Epoch: 3, training loss: 0.6993\n",
      "Epoch 3, Val Loss: 0.8966\n",
      "Epoch: 4, training loss: 0.6854\n",
      "Epoch 4, Val Loss: 0.8994\n",
      "Epoch: 5, training loss: 0.6819\n",
      "Epoch 5, Val Loss: 0.9331\n",
      "Epoch: 6, training loss: 0.6836\n",
      "Epoch 6, Val Loss: 0.9062\n",
      "Epoch: 7, training loss: 0.6845\n",
      "Epoch 7, Val Loss: 0.9166\n",
      "Epoch: 8, training loss: 0.6759\n",
      "Epoch 8, Val Loss: 0.9026\n",
      "Epoch: 9, training loss: 0.6810\n",
      "Epoch 9, Val Loss: 0.9013\n",
      "Epoch: 10, training loss: 0.6905\n",
      "Epoch 10, Val Loss: 0.9195\n",
      "Fold 4:\n",
      "Epoch: 1, training loss: 0.8287\n",
      "Epoch 1, Val Loss: 0.9033\n",
      "Epoch: 2, training loss: 0.7424\n",
      "Epoch 2, Val Loss: 0.9043\n",
      "Epoch: 3, training loss: 0.7105\n",
      "Epoch 3, Val Loss: 0.9042\n",
      "Epoch: 4, training loss: 0.6923\n",
      "Epoch 4, Val Loss: 0.9208\n",
      "Epoch: 5, training loss: 0.6940\n",
      "Epoch 5, Val Loss: 0.8923\n",
      "Epoch: 6, training loss: 0.6923\n",
      "Epoch 6, Val Loss: 0.9052\n",
      "Epoch: 7, training loss: 0.6825\n",
      "Epoch 7, Val Loss: 0.8973\n",
      "Epoch: 8, training loss: 0.6826\n",
      "Epoch 8, Val Loss: 0.9269\n",
      "Epoch: 9, training loss: 0.6830\n",
      "Epoch 9, Val Loss: 0.8957\n",
      "Epoch: 10, training loss: 0.6894\n",
      "Epoch 10, Val Loss: 0.8886\n",
      "Fold 5:\n",
      "Epoch: 1, training loss: 0.7998\n",
      "Epoch 1, Val Loss: 0.9025\n",
      "Epoch: 2, training loss: 0.7313\n",
      "Epoch 2, Val Loss: 0.9597\n",
      "Epoch: 3, training loss: 0.7062\n",
      "Epoch 3, Val Loss: 0.9088\n",
      "Epoch: 4, training loss: 0.6905\n",
      "Epoch 4, Val Loss: 0.8891\n",
      "Epoch: 5, training loss: 0.6893\n",
      "Epoch 5, Val Loss: 0.8969\n",
      "Epoch: 6, training loss: 0.6826\n",
      "Epoch 6, Val Loss: 0.9015\n",
      "Epoch: 7, training loss: 0.6861\n",
      "Epoch 7, Val Loss: 0.9599\n",
      "Epoch: 8, training loss: 0.6849\n",
      "Epoch 8, Val Loss: 0.9191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-06 13:37:35,771] Trial 1 finished with value: 0.4843392743189505 and parameters: {'hidden_dim': 78, 'dropout': 0.22960011833570607, 'initial_lr': 7.198185222252616e-05, 'max_lr': 0.03237002760060326, 'criterion': 'BCEWithLogitsLoss', 'pos_weight': 4}. Best is trial 1 with value: 0.4843392743189505.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, training loss: 0.6804\n",
      "Epoch 9, Val Loss: 0.9049\n",
      "Epoch: 10, training loss: 0.6816\n",
      "Epoch 10, Val Loss: 0.9171\n",
      "Fold 1:\n",
      "Epoch: 1, training loss: 0.0270\n",
      "Epoch 1, Val Loss: 0.0433\n",
      "Epoch: 2, training loss: 0.0221\n",
      "Epoch 2, Val Loss: 0.0433\n",
      "Epoch: 3, training loss: 0.0208\n",
      "Epoch 3, Val Loss: 0.0433\n",
      "Epoch: 4, training loss: 0.0199\n",
      "Epoch 4, Val Loss: 0.0433\n",
      "Epoch: 5, training loss: 0.0195\n",
      "Epoch 5, Val Loss: 0.0436\n",
      "Epoch: 6, training loss: 0.0193\n",
      "Epoch 6, Val Loss: 0.0435\n",
      "Epoch: 7, training loss: 0.0196\n",
      "Epoch 7, Val Loss: 0.0434\n",
      "Epoch: 8, training loss: 0.0191\n",
      "Epoch 8, Val Loss: 0.0436\n",
      "Epoch: 9, training loss: 0.0192\n",
      "Epoch 9, Val Loss: 0.0434\n",
      "Epoch: 10, training loss: 0.0190\n",
      "Epoch 10, Val Loss: 0.0435\n",
      "Fold 2:\n",
      "Epoch: 1, training loss: 0.0270\n",
      "Epoch 1, Val Loss: 0.0433\n",
      "Epoch: 2, training loss: 0.0214\n",
      "Epoch 2, Val Loss: 0.0433\n",
      "Epoch: 3, training loss: 0.0203\n",
      "Epoch 3, Val Loss: 0.0433\n",
      "Epoch: 4, training loss: 0.0197\n",
      "Epoch 4, Val Loss: 0.0433\n",
      "Epoch: 5, training loss: 0.0193\n",
      "Epoch 5, Val Loss: 0.0435\n",
      "Epoch: 6, training loss: 0.0191\n",
      "Epoch 6, Val Loss: 0.0435\n",
      "Epoch: 7, training loss: 0.0191\n",
      "Epoch 7, Val Loss: 0.0439\n",
      "Epoch: 8, training loss: 0.0191\n",
      "Epoch 8, Val Loss: 0.0441\n",
      "Epoch: 9, training loss: 0.0189\n",
      "Epoch 9, Val Loss: 0.0435\n",
      "Epoch: 10, training loss: 0.0189\n",
      "Epoch 10, Val Loss: 0.0436\n",
      "Fold 3:\n",
      "Epoch: 1, training loss: 0.0274\n",
      "Epoch 1, Val Loss: 0.0433\n",
      "Epoch: 2, training loss: 0.0217\n",
      "Epoch 2, Val Loss: 0.0433\n",
      "Epoch: 3, training loss: 0.0207\n",
      "Epoch 3, Val Loss: 0.0433\n",
      "Epoch: 4, training loss: 0.0198\n",
      "Epoch 4, Val Loss: 0.0433\n",
      "Epoch: 5, training loss: 0.0193\n",
      "Epoch 5, Val Loss: 0.0433\n",
      "Epoch: 6, training loss: 0.0190\n",
      "Epoch 6, Val Loss: 0.0434\n",
      "Epoch: 7, training loss: 0.0190\n",
      "Epoch 7, Val Loss: 0.0438\n",
      "Epoch: 8, training loss: 0.0190\n",
      "Epoch 8, Val Loss: 0.0433\n",
      "Epoch: 9, training loss: 0.0191\n",
      "Epoch 9, Val Loss: 0.0439\n",
      "Epoch: 10, training loss: 0.0190\n",
      "Epoch 10, Val Loss: 0.0433\n",
      "Fold 4:\n",
      "Epoch: 1, training loss: 0.0275\n",
      "Epoch 1, Val Loss: 0.0433\n",
      "Epoch: 2, training loss: 0.0213\n",
      "Epoch 2, Val Loss: 0.0433\n",
      "Epoch: 3, training loss: 0.0205\n",
      "Epoch 3, Val Loss: 0.0433\n",
      "Epoch: 4, training loss: 0.0198\n",
      "Epoch 4, Val Loss: 0.0433\n",
      "Epoch: 5, training loss: 0.0195\n",
      "Epoch 5, Val Loss: 0.0434\n",
      "Epoch: 6, training loss: 0.0195\n",
      "Epoch 6, Val Loss: 0.0438\n",
      "Epoch: 7, training loss: 0.0191\n",
      "Epoch 7, Val Loss: 0.0436\n",
      "Epoch: 8, training loss: 0.0191\n",
      "Epoch 8, Val Loss: 0.0441\n",
      "Epoch: 9, training loss: 0.0190\n",
      "Epoch 9, Val Loss: 0.0440\n",
      "Epoch: 10, training loss: 0.0188\n",
      "Epoch 10, Val Loss: 0.0441\n",
      "Fold 5:\n",
      "Epoch: 1, training loss: 0.0281\n",
      "Epoch 1, Val Loss: 0.0433\n",
      "Epoch: 2, training loss: 0.0221\n",
      "Epoch 2, Val Loss: 0.0434\n",
      "Epoch: 3, training loss: 0.0208\n",
      "Epoch 3, Val Loss: 0.0434\n",
      "Epoch: 4, training loss: 0.0200\n",
      "Epoch 4, Val Loss: 0.0434\n",
      "Epoch: 5, training loss: 0.0194\n",
      "Epoch 5, Val Loss: 0.0434\n",
      "Epoch: 6, training loss: 0.0192\n",
      "Epoch 6, Val Loss: 0.0435\n",
      "Epoch: 7, training loss: 0.0193\n",
      "Epoch 7, Val Loss: 0.0433\n",
      "Epoch: 8, training loss: 0.0193\n",
      "Epoch 8, Val Loss: 0.0441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-06 13:37:39,888] Trial 2 finished with value: 0.42964575059529864 and parameters: {'hidden_dim': 102, 'dropout': 0.31713503581462016, 'initial_lr': 2.9568516779675654e-05, 'max_lr': 0.009126015472307208, 'criterion': 'FocalLoss'}. Best is trial 1 with value: 0.4843392743189505.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, training loss: 0.0190\n",
      "Epoch 9, Val Loss: 0.0440\n",
      "Epoch: 10, training loss: 0.0191\n",
      "Epoch 10, Val Loss: 0.0439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bottle v0.13.2 server starting up (using WSGIRefServer())...\n",
      "Listening on http://localhost:8080/\n",
      "Hit Ctrl-C to quit.\n",
      "\n",
      "127.0.0.1 - - [06/Apr/2025 13:37:41] \"GET /api/studies/0?after=3 HTTP/1.1\" 200 3874\n",
      "127.0.0.1 - - [06/Apr/2025 13:37:43] \"GET /api/studies/0?after=3 HTTP/1.1\" 200 3874\n",
      "127.0.0.1 - - [06/Apr/2025 13:37:45] \"GET /api/studies/0?after=3 HTTP/1.1\" 200 3874\n",
      "127.0.0.1 - - [06/Apr/2025 13:37:57] \"GET /api/studies/0?after=3 HTTP/1.1\" 200 3874\n",
      "127.0.0.1 - - [06/Apr/2025 13:38:02] \"GET /api/studies HTTP/1.1\" 200 133\n",
      "127.0.0.1 - - [06/Apr/2025 13:38:03] \"GET /api/studies/0?after=3 HTTP/1.1\" 200 3874\n",
      "127.0.0.1 - - [06/Apr/2025 13:38:04] \"GET /api/meta HTTP/1.1\" 200 64\n",
      "127.0.0.1 - - [06/Apr/2025 13:38:14] \"GET /api/studies/0?after=3 HTTP/1.1\" 200 3874\n",
      "127.0.0.1 - - [06/Apr/2025 13:38:27] \"GET /api/studies/0?after=3 HTTP/1.1\" 200 3874\n",
      "127.0.0.1 - - [06/Apr/2025 13:38:38] \"GET /api/studies/0?after=3 HTTP/1.1\" 200 3874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      "  Combined score: 0.4843392743189505\n",
      "  Best hyperparameters:\n",
      "    hidden_dim: 78\n",
      "    dropout: 0.22960011833570607\n",
      "    initial_lr: 7.198185222252616e-05\n",
      "    max_lr: 0.03237002760060326\n",
      "    criterion: BCEWithLogitsLoss\n",
      "    pos_weight: 4\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from optuna_dashboard import run_server\n",
    "\n",
    "storage = optuna.storages.InMemoryStorage()\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\",storage=storage,  # Specify the storage URL here.\n",
    "    study_name=\"Basic\")\n",
    "study.optimize(maximise_combined_score, n_trials=3)  # You can adjust the number of trials\n",
    "run_server(storage)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(f\"  Combined score: {trial.value}\")\n",
    "print(\"  Best hyperparameters:\")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f68eb9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
