{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code not fully implemented yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model.eval() #? Set model to evaluation mode\n",
    "with torch.no_grad(): #? No need to track gradients during evaluation       \n",
    "    for batch, (inputs, labels) in enumerate(val_loader,start=1):#! one pass because val_loader batch size is all   \n",
    "        outputs = model(inputs)  \n",
    "        predictions = (torch.sigmoid(outputs) > 0.5).float().cpu()#? Convert logits to binary predictions\n",
    "        labels = labels.cpu() #? Move labels to CPU for compatibility with sklearn metrics\n",
    "        \n",
    "    precision = precision_score(labels,predictions, average='macro')\n",
    "    accuracyScore = accuracy_score(labels,predictions)\n",
    "    recallScore = recall_score(labels,predictions, average='macro', zero_division=1)\n",
    "    f1Score = f1_score(labels,predictions, average='macro', zero_division=1)\n",
    "    conf_matrix = confusion_matrix(labels,predictions)\n",
    "\n",
    "    print(f\"Fold: {fold}\".ljust(12),\n",
    "            # f\"AccuracyScore: {(accuracyScore):.4f}\".ljust(20),\n",
    "            # f\"RecallScore: {(recallScore):.2f}\".ljust(20),\n",
    "            # f\"f1Score: {(f1Score):.2f}\".ljust(20),  \n",
    "            f\"Confusion Matrix:\\n{conf_matrix}\".ljust(20),\n",
    "            \"\\nClassification Report:\\n\", classification_report(labels,predictions)\n",
    "            \n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "auc = roc_auc_score(labels,predictions)\n",
    "print(f\"ROC AUC Score: {auc:.4f}\")\n",
    "\n",
    "# Step 3: Get ROC Curve values\n",
    "fpr, tpr, thresholds = roc_curve(labels,predictions)\n",
    "\n",
    "# Step 4: Plot ROC Curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='blue', label=f'ROC Curve (AUC = {auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate (Recall)')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()      \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
