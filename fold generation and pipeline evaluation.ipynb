{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a3d72aa-b704-47ae-a96b-6293bf6c57d3",
   "metadata": {},
   "source": [
    "# Installing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "934b73ff-e8e9-491c-85cb-9d90cc49cce6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (1.26.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: optuna in /opt/conda/lib/python3.11/site-packages (4.3.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /opt/conda/lib/python3.11/site-packages (from optuna) (1.12.1)\n",
      "Requirement already satisfied: colorlog in /opt/conda/lib/python3.11/site-packages (from optuna) (6.9.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from optuna) (1.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from optuna) (23.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /opt/conda/lib/python3.11/site-packages (from optuna) (2.0.23)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from optuna) (4.66.1)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.11/site-packages (from optuna) (6.0.1)\n",
      "Requirement already satisfied: Mako in /opt/conda/lib/python3.11/site-packages (from alembic>=1.5.0->optuna) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4 in /opt/conda/lib/python3.11/site-packages (from alembic>=1.5.0->optuna) (4.8.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.11/site-packages (from sqlalchemy>=1.4.2->optuna) (3.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /opt/conda/lib/python3.11/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n",
      "Requirement already satisfied: optuna-dashboard in /opt/conda/lib/python3.11/site-packages (0.18.0)\n",
      "Requirement already satisfied: bottle>=0.13.0 in /opt/conda/lib/python3.11/site-packages (from optuna-dashboard) (0.13.2)\n",
      "Requirement already satisfied: optuna>=3.1.0 in /opt/conda/lib/python3.11/site-packages (from optuna-dashboard) (4.3.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from optuna-dashboard) (23.2)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.11/site-packages (from optuna-dashboard) (1.6.1)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /opt/conda/lib/python3.11/site-packages (from optuna>=3.1.0->optuna-dashboard) (1.12.1)\n",
      "Requirement already satisfied: colorlog in /opt/conda/lib/python3.11/site-packages (from optuna>=3.1.0->optuna-dashboard) (6.9.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from optuna>=3.1.0->optuna-dashboard) (1.26.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /opt/conda/lib/python3.11/site-packages (from optuna>=3.1.0->optuna-dashboard) (2.0.23)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from optuna>=3.1.0->optuna-dashboard) (4.66.1)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.11/site-packages (from optuna>=3.1.0->optuna-dashboard) (6.0.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn->optuna-dashboard) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn->optuna-dashboard) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn->optuna-dashboard) (3.6.0)\n",
      "Requirement already satisfied: Mako in /opt/conda/lib/python3.11/site-packages (from alembic>=1.5.0->optuna>=3.1.0->optuna-dashboard) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4 in /opt/conda/lib/python3.11/site-packages (from alembic>=1.5.0->optuna>=3.1.0->optuna-dashboard) (4.8.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.11/site-packages (from sqlalchemy>=1.4.2->optuna>=3.1.0->optuna-dashboard) (3.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /opt/conda/lib/python3.11/site-packages (from Mako->alembic>=1.5.0->optuna>=3.1.0->optuna-dashboard) (2.1.3)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement scikit (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for scikit\u001b[0m\u001b[31m\n",
      "\u001b[0mRequirement already satisfied: imbalanced-learn in /opt/conda/lib/python3.11/site-packages (0.13.0)\n",
      "Requirement already satisfied: numpy<3,>=1.24.3 in /opt/conda/lib/python3.11/site-packages (from imbalanced-learn) (1.26.0)\n",
      "Requirement already satisfied: scipy<2,>=1.10.1 in /opt/conda/lib/python3.11/site-packages (from imbalanced-learn) (1.15.2)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.3.2 in /opt/conda/lib/python3.11/site-packages (from imbalanced-learn) (1.6.1)\n",
      "Requirement already satisfied: sklearn-compat<1,>=0.1 in /opt/conda/lib/python3.11/site-packages (from imbalanced-learn) (0.1.3)\n",
      "Requirement already satisfied: joblib<2,>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from imbalanced-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from imbalanced-learn) (3.6.0)\n",
      "Requirement already satisfied: seaborn in /opt/conda/lib/python3.11/site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /opt/conda/lib/python3.11/site-packages (from seaborn) (1.26.0)\n",
      "Requirement already satisfied: pandas>=1.2 in /opt/conda/lib/python3.11/site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /opt/conda/lib/python3.11/site-packages (from seaborn) (3.10.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (9.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.2->seaborn) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.11/site-packages (3.10.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (1.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (9.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.11/site-packages (1.15.2)\n",
      "Requirement already satisfied: numpy<2.5,>=1.23.5 in /opt/conda/lib/python3.11/site-packages (from scipy) (1.26.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install optuna\n",
    "!pip install optuna-dashboard\n",
    "!pip install scikit\n",
    "!pip install imbalanced-learn\n",
    "!pip install seaborn\n",
    "!pip install matplotlib\n",
    "!pip install scipy\n",
    "\n",
    "from sklearn.preprocessing import (\n",
    "    MaxAbsScaler,\n",
    "    MinMaxScaler,\n",
    "    Normalizer,\n",
    "    PowerTransformer,\n",
    "    QuantileTransformer,\n",
    "    RobustScaler,\n",
    "    StandardScaler,\n",
    "    minmax_scale,\n",
    ")\n",
    "from sklearn.metrics import recall_score, accuracy_score,f1_score, precision_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import warnings\n",
    "import optuna\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, roc_auc_score\n",
    "from scipy.stats import ks_2samp\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06d1132-702d-4afc-ac42-5f66e497ca32",
   "metadata": {},
   "source": [
    "# Step 1\n",
    "1. device selection (quite self explanatory ya)\n",
    "2. init_weights initializes weights of nn.Linear using Xavier uniform distribution and sets biases to zero\n",
    "3. fold_to_dataloader_tensor converts tabular train and test data (from Pandas) into PyTorch DataLoaders with tensors on the right device.\n",
    "4. get_feature_count returns the number of input features from a DataLoader batch.\n",
    "5. criterion_mapping selects the appropriate loss function (BCE, Dice, or Focal Loss) based on user input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "038a231b-fd9c-48e4-8b26-47258d84a49d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing\u001b[39m\u001b[38;5;124m\"\u001b[39m, device)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minit_weights\u001b[39m(model): \u001b[38;5;66;03m#tested already\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using\", device)\n",
    "def init_weights(model): #tested already\n",
    "    if isinstance(model, nn.Linear):  # Apply only to linear layers\n",
    "        nn.init.xavier_uniform_(model.weight)\n",
    "        if model.bias is not None:\n",
    "            nn.init.zeros_(model.bias)\n",
    "            \n",
    "def fold_to_dataloader_tensor(train_x, test_x, train_y, test_y, batch_size=64, device=device):\n",
    "    train_dataset = TensorDataset(\n",
    "        torch.tensor(train_x.values,dtype=torch.float32).to(device), \n",
    "        torch.tensor(train_y.values,dtype=torch.float32).to(device))\n",
    "    val_dataset = TensorDataset(\n",
    "        torch.tensor(test_x.values,dtype=torch.float32).to(device), \n",
    "        torch.tensor(test_y.values,dtype=torch.float32).to(device))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=False, drop_last=True)\n",
    "    return train_loader, val_loader \n",
    "\n",
    "def get_feature_count(loader):\n",
    "    \"\"\"returns the number of features in the dataset\"\"\"\n",
    "    return next(iter(loader))[0].shape[1]\n",
    "\n",
    "from Criterion_Models import *\n",
    "def criterion_mapping(criterion_choice:str, pos_weight:float=None, alpha:float=None, gamma:float=None):\n",
    "    \"\"\"\n",
    "    Feel free to add any custom loss functions here.\n",
    "    returns function for criterion\n",
    "    \"\"\"\n",
    "    if criterion_choice == \"FocalLoss\":\n",
    "        return FocalLoss(alpha =alpha, gamma=gamma)\n",
    "    elif criterion_choice == \"DiceLoss\":\n",
    "        return DiceLoss()\n",
    "    elif criterion_choice == \"BCEWithLogitsLoss\":\n",
    "        return nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight])) if pos_weight else nn.BCEWithLogitsLoss()\n",
    "    return nn.BCEWithLogitsLoss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "646878a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "raw_dataset = pd.read_csv(\"./data/processed_data.csv\") #data has X and Y\n",
    "X = raw_dataset.drop(columns=[\"DR\"])\n",
    "Y = pd.DataFrame(raw_dataset[\"DR\"])\n",
    "\n",
    "# clinical_units = X.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "40c058ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop cols here\n",
    "# numeric_columns = ['Age', 'UAlb', 'Ucr', 'UACR', 'TC', 'TG', 'TCTG', 'LDLC', 'HDLC', 'Scr', 'BUN', 'FPG', 'HbA1c', 'Height', 'Weight', 'BMI', 'Duration']\n",
    "# binary_columns = ['Gender', 'DR', 'Community_baihe', 'Community_chonggu', 'Community_huaxin', 'Community_jinze', 'Community_liantang', 'Community_xianghuaqiao', 'Community_xujin', 'Community_yingpu', 'Community_zhaoxian', 'Community_zhujiajiao']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d4c9b002",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_FOR_FOLDS, X_FINAL_TEST, Y_FOR_FOLDS, Y_FINAL_TEST = train_test_split(X, Y, test_size=0.1, random_state=random_state, stratify=Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8600b6-26a7-4daf-8da1-97df7991a87c",
   "metadata": {},
   "source": [
    "# Step 2: Tabular data preprocessing and augmentation\n",
    "(Modified to use P() as sent by lz\n",
    "\n",
    "The new augment_data_in_place prepares input features for model training by:\n",
    "\n",
    "1. Identifying numerical columns to transform (e.g. lab values, BMI, etc.).\n",
    "2. Log-transforming these columns to reduce skew.\n",
    "3. Adding Gaussian noise to only the negative class (DR=0) if noise is set.\n",
    "4. Returning transformed copies of X and X_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "06f3d3e3-4abc-4a52-a03c-382c66ed23ee",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def augment_data_in_place(X, X_test, Y=None, normalisation_method=MinMaxScaler(), noise=None):\n",
    "    all_numerical_columns = ['Age', 'UAlb', 'Ucr', 'UACR', 'TC', 'TG', 'TCTG', 'LDLC', 'HDLC', 'Scr', 'BUN', 'FPG', 'HbA1c', 'Height', 'Weight', 'BMI', 'Duration']\n",
    "    binary_columns = ['Gender', 'DR', 'Community_baihe', 'Community_chonggu', 'Community_huaxin', 'Community_jinze', 'Community_liantang', 'Community_xianghuaqiao', 'Community_xujin', 'Community_yingpu', 'Community_zhaoxian', 'Community_zhujiajiao']\n",
    "    \n",
    "    existing_columns = [col for col in all_numerical_columns if col in X.columns and col in X_test.columns]\n",
    "\n",
    "    if not existing_columns:\n",
    "        print(\"No matching columns found for augmentation. Normalised data only.\")\n",
    "        X = normalisation_method.fit_transform(X)\n",
    "        X_test = normalisation_method.transform(X_test)\n",
    "        return X, X_test\n",
    "\n",
    "    X_copy = X.copy()\n",
    "    X_test_copy = X_test.copy()\n",
    "    \n",
    "    # Log-transform\n",
    "    X_copy.loc[:, existing_columns] = X_copy.loc[:, existing_columns].apply(np.log1p)\n",
    "    X_test_copy.loc[:, existing_columns] = X_test_copy.loc[:, existing_columns].apply(np.log1p)\n",
    "\n",
    "    # Add noise ONLY to negatives (class 0) if Y is provided and noise is set\n",
    "    if noise and noise > 0:\n",
    "        if Y is None:\n",
    "            raise ValueError(\"Y must be provided if noise is being added selectively.\")\n",
    "        # Identify negative class indices (class 0)\n",
    "        negative_indices = Y[Y.iloc[:, 0] == 0].index\n",
    "        noise_matrix = np.random.normal(0, noise, X_copy.loc[negative_indices, existing_columns].shape)\n",
    "        X_copy.loc[negative_indices, existing_columns] += noise_matrix\n",
    "\n",
    "    # Scale\n",
    "    # scaler = normalisation_method\n",
    "    # X_copy.loc[:, existing_columns] = scaler.fit_transform(X_copy.loc[:, existing_columns])\n",
    "    # X_test_copy.loc[:, existing_columns] = scaler.transform(X_test_copy.loc[:, existing_columns])\n",
    "\n",
    "    return X_copy, X_test_copy\n",
    "\n",
    "\n",
    "# def iso_forest(X_train, Y_train, contamination=None, random_state=42):\n",
    "#     # print(\"Original\\n\", X_train.shape, Y_train.shape, X_test.shape, Y_test.shape)\n",
    "#     X_train_cleaned, Y_train_cleaned = X_train.copy(), Y_train.copy()\n",
    "    \n",
    "#     X_train_zeros = X_train[Y_train.iloc[:, 0] == 0]\n",
    "#     X_train_ones = X_train[Y_train.iloc[:, 0] == 1]\n",
    "#     Y_train_zeros = Y_train[Y_train.iloc[:, 0] == 0]\n",
    "#     Y_train_ones = Y_train[Y_train.iloc[:, 0] == 1] \n",
    "#     # print(\"Ones and zeros\\n\", X_train_zeros.shape, Y_train_zeros.shape, X_train_ones.shape, Y_train_ones.shape)\n",
    "#     #only class 0s\n",
    "#     if X_train_zeros.isna().any().any():\n",
    "#         print(\"got NaN values in the training set\")\n",
    "    \n",
    "#     # Apply Isolation Forest to majority class only\n",
    "#     iso_forest = IsolationForest(contamination=contamination, random_state=random_state)\n",
    "#     try:\n",
    "#         outliers = iso_forest.fit_predict(X_train_zeros)\n",
    "#     except UserWarning as e:\n",
    "#         print(\"Caught warning during IsolationForest fitting:\", e)\n",
    "#         outliers = np.ones(len(X_train_zeros))  # If warning occurs, keep all data\n",
    "#     # Keep only non-outlier majority samples\n",
    "#     X_train_zeros = X_train_zeros[outliers == 1]\n",
    "#     Y_train_zeros = Y_train_zeros[outliers == 1]\n",
    "#     # print(\"After iso:\\n\", X_train_zeros.shape, Y_train_zeros.shape, X_train_ones.shape, Y_train_ones.shape)\n",
    "    \n",
    "#     # Combine the cleaned majority class with the untouched minority class\n",
    "#     X_train_cleaned = pd.concat([X_train_zeros, X_train_ones])\n",
    "#     Y_train_cleaned = pd.concat([Y_train_zeros, Y_train_ones])\n",
    "#     return X_train_cleaned, Y_train_cleaned\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574dec4f-e322-466a-8ecf-20fc9e271338",
   "metadata": {},
   "source": [
    "# Step 3: Quantitative Check on Augmentation\n",
    "\n",
    "analyze_augmentation_impact evaluates how data augmentation and preprocessing steps affect:\n",
    "\n",
    "1. Class Distribution:\n",
    "   Compares the number of samples in class 0 and 1 before and after augmentation/oversampling.\n",
    "\n",
    "2. Feature Distribution Shifts:\n",
    "   For all shared, non-constant numerical columns, it compares:\n",
    "       - Means and standard deviations\n",
    "       - Kolmogorov–Smirnov test statistics to detect distributional drift\n",
    "\n",
    "3. Returns a summary dictionary with class counts and feature-wise stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d45ce4-e90f-495c-8283-ac38bda6a016",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def analyze_augmentation_impact(original_X, original_y, processed_fold):\n",
    "    \"\"\"\n",
    "    Analyze the effect of augmentation on class distribution and feature statistics.\n",
    "    \n",
    "    Parameters:\n",
    "        original_X: DataFrame of raw features before any processing.\n",
    "        original_y: Series of raw target labels.\n",
    "        processed_fold: Tuple (X_train, X_test, y_train, y_test) after processing.\n",
    "        clinical_units: Optional raw (unnormalized) data for clinical comparison.\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with summary statistics and drift measures.\n",
    "    \"\"\"\n",
    "    X_train_fold, _, y_train_fold, _ = processed_fold\n",
    "    \n",
    "    print(\"\\n=== Class Distribution ===\")\n",
    "    class_counts = {\n",
    "        \"original_0\": (original_y == 0).sum(),\n",
    "        \"original_1\": (original_y == 1).sum(),\n",
    "        \"processed_0\": (y_train_fold == 0).sum(),\n",
    "        \"processed_1\": (y_train_fold == 1).sum(),\n",
    "    }\n",
    "    print(f\"Original: 0={class_counts['original_0']} | 1={class_counts['original_1']}\")\n",
    "    print(f\"Processed: 0={class_counts['processed_0']} | 1={class_counts['processed_1']}\")\n",
    "    \n",
    "    # Compare feature distributions for numerical columns\n",
    "    # num_cols = ['Age', 'UAlb', 'Ucr', 'TCTG', 'HDLC', 'BUN', 'FPG', 'Weight', 'UACR', 'BMI', 'HbA1c']  # Key numerical features to compare\n",
    "    # num_cols = original_X.columns.intersection(X_train_fold.columns)\n",
    "\n",
    "    shared_cols = original_X.columns.intersection(X_train_fold.columns)\n",
    "    combined = pd.concat([original_X[shared_cols], X_train_fold[shared_cols]])\n",
    "    non_constant_cols = combined.loc[:, combined.nunique() > 1].columns\n",
    "    num_cols = non_constant_cols\n",
    "    \n",
    "    stats_summary = {}\n",
    "    print(\"\\n=== Feature Statistics (Normalized Units) ===\")\n",
    "    for col in num_cols:\n",
    "        orig = original_X[col]\n",
    "        proc = X_train_fold[col]\n",
    "        ks_stat, ks_p = ks_2samp(orig, proc)\n",
    "        print(f\"{col}: {orig.mean():.2f}±{orig.std():.2f} → {proc.mean():.2f}±{proc.std():.2f} | KS={ks_stat:.3f}, p={ks_p:.3f}\")\n",
    "        stats_summary[col] = {\n",
    "            \"mean_orig\": orig.mean(), \"std_orig\": orig.std(),\n",
    "            \"mean_proc\": proc.mean(), \"std_proc\": proc.std(),\n",
    "            \"ks_stat\": ks_stat, \"ks_p\": ks_p\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        \"class_counts\": class_counts,\n",
    "        \"feature_stats\": stats_summary\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32ac117-3a9b-4735-bdd2-812a2789388c",
   "metadata": {},
   "source": [
    "# Step 4: Visual and Quantitative check on synthetic data quality\n",
    "\n",
    "evaluate_augmentation_quality assesses how similar or different synthetic samples are from original minority samples, focusing on distribution, spread, and outliers.\n",
    "\n",
    "1. PCA visualization:\n",
    "   - Projects both original and synthtic minority class into 2D using PCA\n",
    "   - Plots them to visually inspect overlap or drift\n",
    "2. Data cleaning:\n",
    "   - Drops constant columns\n",
    "   - Imputes missing values (mean or zero-fill) before PCA\n",
    "3. Outlier detection in PCA space:\n",
    "   - Uses Mahalanobis distance to identify synthetic samples that are unusually far from the original minority clustrer.\n",
    "   - Computes the outlier rate among synthethic samples\n",
    "  \n",
    "It returns a dictionary with:\n",
    "- Number of original vs synthethic minority samples\n",
    "- Total PCA variance explained (how much info PCA retaiend)\n",
    "- Number and proportion of outlier synthetic samples\n",
    "- n_neighbors info for SMOTE/ ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4c53b0f4-c302-42b9-a606-63bc15164233",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_augmentation_quality(original_X, original_y, processed_fold, n_neighbors=None):\n",
    "    \"\"\"\n",
    "    Evaluate how synthetic samples differ from original ones via PCA and metrics.\n",
    "    \n",
    "    Parameters:\n",
    "        original_X: DataFrame of raw features.\n",
    "        original_y: Series of raw targets.\n",
    "        processed_fold: Tuple (X_train, X_test, y_train, y_test) from processed data.\n",
    "        n_neighbors: Optional SMOTE/ADASYN config parameter for logging.\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of quality metrics.\n",
    "    \"\"\"\n",
    "    X_train_fold, _, y_train_fold, _ = processed_fold\n",
    "    \n",
    "    # Extract original vs. synthetic minority samples\n",
    "    minority_mask = original_y == 1\n",
    "    original_minority = original_X[minority_mask]\n",
    "    processed_minority = X_train_fold[y_train_fold == 1]\n",
    "    \n",
    "    # PCA projection\n",
    "    pca = PCA(n_components=2)\n",
    "    combined = pd.concat([original_minority, processed_minority])\n",
    "\n",
    "    # Combine both for shared column filtering\n",
    "    combined_df = pd.concat([original_minority, processed_minority])\n",
    "    \n",
    "    # Keep only columns that vary (non-constant across the combined set)\n",
    "    non_constant_cols = combined_df.loc[:, combined_df.nunique() > 1].columns\n",
    "    \n",
    "    # Apply same column filtering to both datasets\n",
    "    original_minority = original_minority[non_constant_cols]\n",
    "    processed_minority = processed_minority[non_constant_cols]\n",
    "\n",
    "    # Using imputer to handle NaN columns\n",
    "    fully_nan_cols = combined.columns[combined.isna().all()].tolist()\n",
    "    other_cols = [col for col in combined.columns if col not in fully_nan_cols]\n",
    "    transformer = ColumnTransformer(transformers=[\n",
    "        ('mean_imputer', SimpleImputer(strategy='mean'), other_cols),\n",
    "        ('zero_imputer', SimpleImputer(strategy='constant', fill_value=0), fully_nan_cols)\n",
    "    ])\n",
    "    \n",
    "    combined_imputed_array = transformer.fit_transform(combined)\n",
    "    combined_imputed = pd.DataFrame(combined_imputed_array, columns=other_cols + fully_nan_cols)\n",
    "    pca_result = pca.fit_transform(combined_imputed)\n",
    "\n",
    "    original_pca = pca_result[:len(original_minority)]\n",
    "    synthetic_pca = pca_result[len(original_minority):]\n",
    "\n",
    "    # PCA plot\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.scatter(original_pca[:,0], original_pca[:,1], alpha=0.4, label='Original Minority', color='blue')\n",
    "    plt.scatter(synthetic_pca[:,0], synthetic_pca[:,1], alpha=0.4, label='Synthetic Minority', color='orange')\n",
    "    plt.title(\"PCA of Minority Class Samples\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Outlier detection using Mahalanobis distance in PCA space\n",
    "    original_mean = original_pca.mean(axis=0)\n",
    "    original_cov = np.cov(original_pca, rowvar=False)\n",
    "    inv_covmat = np.linalg.pinv(original_cov)\n",
    "\n",
    "    def is_outlier(point):\n",
    "        return distance.mahalanobis(point, original_mean, inv_covmat) > 2.5\n",
    "\n",
    "    synthetic_outliers = [is_outlier(pt) for pt in synthetic_pca]\n",
    "    num_outliers = int(np.sum(synthetic_outliers))\n",
    "\n",
    "    return {\n",
    "        \"original_minority_count\": len(original_minority),\n",
    "        \"processed_minority_count\": len(processed_minority),\n",
    "        \"pca_variance_ratio\": pca.explained_variance_ratio_.sum(),\n",
    "        \"synthetic_outliers\": num_outliers,\n",
    "        \"outlier_rate\": num_outliers / len(processed_minority),\n",
    "        \"n_neighbors\": n_neighbors\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b327d5-34ec-44f7-8502-607fbc3d9aba",
   "metadata": {},
   "source": [
    "# Step 5: Outlier filtering and feature scaling pipeline\n",
    "P performs two key operations for basic preprocessing:\n",
    "1. Outlier detection\n",
    "   - Splits the data by class (DR = 0 vs 1)\n",
    "   - Optionally removes outliers separately for majority and minority classes using outlier detection model\n",
    "2. Feature scaling\n",
    "   - Applies a scaler only on continuous columns\n",
    "   - Ensures the same scaer is used on both training and test sets for consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fb52951a-8e69-4094-ad48-dca25640ee6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def P(df, df_test, OD_majority, OD_minority, scaler):\n",
    "    \"\"\"\n",
    "    Preprocess the data by applying optional outlier detection and mandatory scaling.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): Training data.\n",
    "    df_test (DataFrame): Test data.\n",
    "    OD_majority (model or None): Outlier detection model for majority class (or None to skip).\n",
    "    OD_minority (model or None): Outlier detection model for minority class (or None to skip).\n",
    "    scaler (scaler): A fitted scaler instance (like StandardScaler).\n",
    "\n",
    "    Returns:\n",
    "    tuple: Processed (df_train, df_test)\n",
    "    \"\"\"\n",
    "    \n",
    "    cont_cols = ['Age', 'UAlb', 'Ucr', 'UACR', 'TC', 'TG', 'TCTG', \n",
    "                 'LDLC', 'HDLC', 'Scr', 'BUN', 'FPG', 'HbA1c', \n",
    "                 'Height', 'Weight', 'BMI', 'Duration']\n",
    "    \n",
    "    y_col = 'DR'\n",
    "    \n",
    "    # Split into classes\n",
    "    df_majority = df[df[y_col] == 0].copy()\n",
    "    df_minority = df[df[y_col] == 1].copy()\n",
    "\n",
    "    # Apply outlier detection to majority class if model is given\n",
    "    if OD_majority is not None:\n",
    "        outliers_majority = OD_majority.fit_predict(df_majority[cont_cols])\n",
    "        df_majority = df_majority[outliers_majority == 1]\n",
    "\n",
    "    # Apply outlier detection to minority class if model is given\n",
    "    if OD_minority is not None:\n",
    "        outliers_minority = OD_minority.fit_predict(df_minority[cont_cols])\n",
    "        df_minority = df_minority[outliers_minority == 1]\n",
    "\n",
    "    # Combine cleaned data\n",
    "    df_after_OD = pd.concat([df_majority, df_minority], ignore_index=True)\n",
    "\n",
    "    # Apply scaling\n",
    "    df_after_OD[cont_cols] = scaler.fit_transform(df_after_OD[cont_cols])\n",
    "    df_test[cont_cols] = scaler.transform(df_test[cont_cols])\n",
    "\n",
    "    return df_after_OD, df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd8e7e0-d47b-4bd3-bdec-3bce30db7c21",
   "metadata": {},
   "source": [
    "# Step 6: Folds generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8f98c91a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def FOLDS_GENERATOR(X, Y, normalisation_method=MinMaxScaler(), n_splits=5, random_state=None, oversampler=None, contamination=0.05, noise = None):\n",
    "    \"\"\"\n",
    "    Generates stratified folds with specified normalization.\n",
    "    normalisation_method should be an instance of a scaler, e.g.,\n",
    "    - MinMaxScaler()\n",
    "    Returns a list of tuples, each containing:\n",
    "    (X_train_scaled, X_test_scaled, Y_train, Y_test), representing data for each fold\n",
    "    \"\"\"\n",
    "    kF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    kFolds_list = []\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(kF.split(X, Y)):\n",
    "        # Split the data into training and testing sets for this fold\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        Y_train, Y_test = Y.iloc[train_idx], Y.iloc[test_idx]\n",
    "\n",
    "        # Merge X and Y into DataFrames for use with P()\n",
    "        df_train = X_train.copy()\n",
    "        df_train[\"DR\"] = Y_train.values\n",
    "        df_test = X_test.copy()\n",
    "        df_test[\"DR\"] = Y_test.values\n",
    "        # print(\"Original\\n\", X_train.shape, Y_train.shape, X_test.shape, Y_test.shape)\n",
    "        \n",
    "        # if contamination is not None and contamination > 0: #? using contamination = 0.0 works\n",
    "        #     X_train_cleaned, Y_train_cleaned = iso_forest(X_train, Y_train, contamination=contamination, random_state=random_state)\n",
    "\n",
    "        # Handle contamination (outlier detection) only if contamination > 0\n",
    "\n",
    "        # Define outlier detectors only if contamination > 0\n",
    "        # iso_majority = IsolationForest(contamination=contamination, random_state=random_state) if contamination else None\n",
    "        # iso_minority = IsolationForest(contamination=contamination, random_state=random_state) if contamination else None\n",
    "\n",
    "        if contamination:\n",
    "            if contamination >0:\n",
    "                OD_majority = IsolationForest(contamination=contamination, random_state=random_state)\n",
    "                OD_minority = IsolationForest(contamination=contamination, random_state=random_state)\n",
    "            else:\n",
    "                OD_majority = None\n",
    "                OD_minority = None\n",
    "            X_train_cleaned, Y_train_cleaned = X_train, Y_train\n",
    "        else:\n",
    "            OD_majority = None\n",
    "            OD_minority = None\n",
    "            X_train_cleaned, Y_train_cleaned = X_train, Y_train\n",
    "            \n",
    "        \n",
    "        #? data augmentation on leftover data\n",
    "        df_train_processed, df_test_processed = P(df_train, df_test, OD_majority, OD_minority, normalisation_method)\n",
    "\n",
    "        # Split back into X and Y\n",
    "        X_train_scaled = df_train_processed.drop(columns=[\"DR\"])\n",
    "        Y_train_cleaned = df_train_processed[[\"DR\"]]\n",
    "        X_test_scaled = df_test_processed.drop(columns=[\"DR\"])\n",
    "        # Note: do NOT clean Y_test — it is untouched ground truth\n",
    "        Y_test = df_test_processed[[\"DR\"]]\n",
    "\n",
    "         # Optional: Add Gaussian noise to class 0\n",
    "        if noise and noise > 0:\n",
    "            neg_idx = Y_train_cleaned[Y_train_cleaned[\"DR\"] == 0].index\n",
    "            noise_matrix = np.random.normal(0, noise, X_train_scaled.loc[neg_idx].shape)\n",
    "            X_train_scaled.loc[neg_idx] += noise_matrix\n",
    "        \n",
    "        # Handle oversampling if needed\n",
    "        #! use X_train_scaled and Y_train_cleaned for oversampling becasue y_train_cleaned no changes after augmentation\n",
    "        print(\"Before oversampling class distribution:\")\n",
    "        print(Y_train_cleaned.value_counts())\n",
    "        if oversampler:\n",
    "            X_train_scaled, Y_train_cleaned = oversampler.fit_resample(X_train_scaled, Y_train_cleaned)\n",
    "        print(\"\\nAfter oversampling class distribution:\")\n",
    "        print(Y_train_cleaned.value_counts())\n",
    "        \n",
    "        # Convert scaled data back to DataFrame with the correct column names\n",
    "        X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train_cleaned.columns)\n",
    "        X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "\n",
    "        # Handle community columns\n",
    "        community_cols = [col for col in X_train_scaled.columns if col.startswith('Community')]\n",
    "        if community_cols:\n",
    "            X_train_scaled[community_cols] = X_train_scaled[community_cols].apply(\n",
    "                lambda row: pd.Series(np.eye(len(row))[row.argmax()]), axis=1\n",
    "            ).set_axis(community_cols, axis=1)\n",
    "        # print(X_train_scaled[community_cols].describe())\n",
    "\n",
    "        # Ensure 'Gender' is still binary (0 or 1)\n",
    "        if 'Gender' in X_train_scaled.columns:\n",
    "            X_train_scaled['Gender'] = (X_train_scaled['Gender'] > 0.5).astype(int)\n",
    "            X_test_scaled['Gender'] = (X_test_scaled['Gender'] > 0.5).astype(int)\n",
    "\n",
    "        # Append the processed fold to the list\n",
    "        kFolds_list.append((X_train_scaled, X_test_scaled, Y_train_cleaned, Y_test))\n",
    "\n",
    "        print(f\"Fold: {fold+1}, Train: {X_train_scaled.shape}, Test: {X_test_scaled.shape}\")\n",
    "\n",
    "    return kFolds_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5e0402ac-bf9d-4dc9-a9e4-cf75ed97a172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before oversampling class distribution:\n",
      "DR \n",
      "0.0    3922\n",
      "1.0     440\n",
      "Name: count, dtype: int64\n",
      "\n",
      "After oversampling class distribution:\n",
      "DR \n",
      "0.0    3922\n",
      "1.0    3846\n",
      "Name: count, dtype: int64\n",
      "Fold: 1, Train: (7768, 28), Test: (1149, 28)\n",
      "Before oversampling class distribution:\n",
      "DR \n",
      "0.0    3922\n",
      "1.0     440\n",
      "Name: count, dtype: int64\n",
      "\n",
      "After oversampling class distribution:\n",
      "DR \n",
      "0.0    3922\n",
      "1.0    3841\n",
      "Name: count, dtype: int64\n",
      "Fold: 2, Train: (7763, 28), Test: (1149, 28)\n",
      "Before oversampling class distribution:\n",
      "DR \n",
      "0.0    3923\n",
      "1.0     440\n",
      "Name: count, dtype: int64\n",
      "\n",
      "After oversampling class distribution:\n",
      "DR \n",
      "0.0    3923\n",
      "1.0    3843\n",
      "Name: count, dtype: int64\n",
      "Fold: 3, Train: (7766, 28), Test: (1148, 28)\n",
      "Before oversampling class distribution:\n",
      "DR \n",
      "0.0    3923\n",
      "1.0     440\n",
      "Name: count, dtype: int64\n",
      "\n",
      "After oversampling class distribution:\n",
      "DR \n",
      "0.0    3923\n",
      "1.0    3866\n",
      "Name: count, dtype: int64\n",
      "Fold: 4, Train: (7789, 28), Test: (1148, 28)\n",
      "Before oversampling class distribution:\n",
      "DR \n",
      "0.0    3923\n",
      "1.0     440\n",
      "Name: count, dtype: int64\n",
      "\n",
      "After oversampling class distribution:\n",
      "DR \n",
      "0.0    3923\n",
      "1.0    3864\n",
      "Name: count, dtype: int64\n",
      "Fold: 5, Train: (7787, 28), Test: (1148, 28)\n"
     ]
    }
   ],
   "source": [
    "oversampler = ADASYN(sampling_strategy='minority', n_neighbors=5, random_state=42)\n",
    "contamination = 0.05\n",
    "normalisation_method = MinMaxScaler()\n",
    "kFolds = FOLDS_GENERATOR(X_FOR_FOLDS, Y_FOR_FOLDS, \n",
    "                         normalisation_method = normalisation_method, \n",
    "                         n_splits=5, \n",
    "                         oversampler = oversampler, random_state=42, contamination=contamination, noise = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4e05514d-0958-4c1b-9aee-463438b006bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7768, 28) (1149, 28) (7768, 1) (1149, 1)\n",
      "               Age       Gender         UAlb          Ucr         UACR  \\\n",
      "count  7768.000000  7768.000000  7768.000000  7768.000000  7768.000000   \n",
      "mean      0.490829     0.535917     0.037902     0.181086     0.025058   \n",
      "std       0.125350     0.498740     0.074946     0.248951     0.053499   \n",
      "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "25%       0.410714     0.000000     0.005620     0.000225     0.003719   \n",
      "50%       0.500000     1.000000     0.012574     0.000487     0.008170   \n",
      "75%       0.571429     1.000000     0.035340     0.337940     0.020753   \n",
      "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
      "\n",
      "                TC           TG         TCTG         LDLC         HDLC  ...  \\\n",
      "count  7768.000000  7768.000000  7768.000000  7768.000000  7768.000000  ...   \n",
      "mean      0.279667     0.096811     0.185084     0.378918     0.327945  ...   \n",
      "std       0.086005     0.074324     0.112545     0.113787     0.125012  ...   \n",
      "min       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
      "25%       0.222320     0.053333     0.106561     0.301393     0.238938  ...   \n",
      "50%       0.275923     0.078250     0.162609     0.378534     0.314159  ...   \n",
      "75%       0.331712     0.116324     0.241973     0.449886     0.402655  ...   \n",
      "max       1.000000     1.000000     1.000000     1.000000     1.000000  ...   \n",
      "\n",
      "       Community_baihe  Community_chonggu  Community_huaxin  Community_jinze  \\\n",
      "count      7768.000000        7768.000000       7768.000000      7768.000000   \n",
      "mean          0.147915           0.068872          0.123455         0.082261   \n",
      "std           0.355038           0.253253          0.328980         0.274779   \n",
      "min           0.000000           0.000000          0.000000         0.000000   \n",
      "25%           0.000000           0.000000          0.000000         0.000000   \n",
      "50%           0.000000           0.000000          0.000000         0.000000   \n",
      "75%           0.000000           0.000000          0.000000         0.000000   \n",
      "max           1.000000           1.000000          1.000000         1.000000   \n",
      "\n",
      "       Community_liantang  Community_xianghuaqiao  Community_xujin  \\\n",
      "count         7768.000000             7768.000000      7768.000000   \n",
      "mean             0.112513                0.085221         0.089341   \n",
      "std              0.316017                0.279229         0.285254   \n",
      "min              0.000000                0.000000         0.000000   \n",
      "25%              0.000000                0.000000         0.000000   \n",
      "50%              0.000000                0.000000         0.000000   \n",
      "75%              0.000000                0.000000         0.000000   \n",
      "max              1.000000                1.000000         1.000000   \n",
      "\n",
      "       Community_yingpu  Community_zhaoxian  Community_zhujiajiao  \n",
      "count       7768.000000         7768.000000           7768.000000  \n",
      "mean           0.080458            0.113671              0.096292  \n",
      "std            0.272019            0.317432              0.295011  \n",
      "min            0.000000            0.000000              0.000000  \n",
      "25%            0.000000            0.000000              0.000000  \n",
      "50%            0.000000            0.000000              0.000000  \n",
      "75%            0.000000            0.000000              0.000000  \n",
      "max            1.000000            1.000000              1.000000  \n",
      "\n",
      "[8 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "for list in kFolds:\n",
    "    print(list[0].shape, list[1].shape, list[2].shape, list[3].shape)\n",
    "    print(list[0].describe())\n",
    "    a = list[0]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7bb5811d-6394-4e9b-9b94-31e04215d4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before oversampling class distribution:\n",
      "DR \n",
      "0.0    3922\n",
      "1.0     440\n",
      "Name: count, dtype: int64\n",
      "\n",
      "After oversampling class distribution:\n",
      "DR \n",
      "0.0    3922\n",
      "1.0    3921\n",
      "Name: count, dtype: int64\n",
      "Fold: 1, Train: (7843, 28), Test: (1149, 28)\n",
      "Before oversampling class distribution:\n",
      "DR \n",
      "0.0    3922\n",
      "1.0     440\n",
      "Name: count, dtype: int64\n",
      "\n",
      "After oversampling class distribution:\n",
      "DR \n",
      "0.0    3922\n",
      "1.0    3922\n",
      "Name: count, dtype: int64\n",
      "Fold: 2, Train: (7844, 28), Test: (1149, 28)\n",
      "Before oversampling class distribution:\n",
      "DR \n",
      "0.0    3923\n",
      "1.0     440\n",
      "Name: count, dtype: int64\n",
      "\n",
      "After oversampling class distribution:\n",
      "DR \n",
      "1.0    3925\n",
      "0.0    3923\n",
      "Name: count, dtype: int64\n",
      "Fold: 3, Train: (7848, 28), Test: (1148, 28)\n",
      "Before oversampling class distribution:\n",
      "DR \n",
      "0.0    3923\n",
      "1.0     440\n",
      "Name: count, dtype: int64\n",
      "\n",
      "After oversampling class distribution:\n",
      "DR \n",
      "0.0    3923\n",
      "1.0    3920\n",
      "Name: count, dtype: int64\n",
      "Fold: 4, Train: (7843, 28), Test: (1148, 28)\n",
      "Before oversampling class distribution:\n",
      "DR \n",
      "0.0    3923\n",
      "1.0     440\n",
      "Name: count, dtype: int64\n",
      "\n",
      "After oversampling class distribution:\n",
      "DR \n",
      "1.0    3925\n",
      "0.0    3923\n",
      "Name: count, dtype: int64\n",
      "Fold: 5, Train: (7848, 28), Test: (1148, 28)\n"
     ]
    }
   ],
   "source": [
    "oversampler = ADASYN(sampling_strategy='minority', n_neighbors=5, random_state=42)\n",
    "contamination = 0.05\n",
    "normalisation_method = MinMaxScaler()\n",
    "kFolds2 = FOLDS_GENERATOR(X_FOR_FOLDS, Y_FOR_FOLDS, \n",
    "                         normalisation_method = normalisation_method, \n",
    "                         n_splits=5, \n",
    "                         oversampler = oversampler, random_state=42, contamination=contamination, noise = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "99063d9a-66d9-460b-ac2b-0c2b595f472b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7843, 28) (1149, 28) (7843, 1) (1149, 1)\n",
      "               Age       Gender         UAlb          Ucr         UACR  \\\n",
      "count  7843.000000  7843.000000  7843.000000  7843.000000  7843.000000   \n",
      "mean      0.483317     0.503124     0.087809     0.265081     0.061610   \n",
      "std       0.186747     0.500022     0.212319     0.300335     0.192249   \n",
      "min      -0.356629     0.000000    -0.629280    -0.591299    -0.640594   \n",
      "25%       0.374601     0.000000     0.001902     0.000435     0.001461   \n",
      "50%       0.481734     1.000000     0.030153     0.249329     0.021141   \n",
      "75%       0.573698     1.000000     0.194107     0.489366     0.153017   \n",
      "max       1.300583     1.000000     1.165687     1.508710     1.000000   \n",
      "\n",
      "                TC           TG         TCTG         LDLC         HDLC  ...  \\\n",
      "count  7843.000000  7843.000000  7843.000000  7843.000000  7843.000000  ...   \n",
      "mean      0.275731     0.097300     0.174943     0.375491     0.324194  ...   \n",
      "std       0.164283     0.156821     0.175114     0.177418     0.195543  ...   \n",
      "min      -0.644325    -0.638881    -0.561374    -0.553825    -0.460985  ...   \n",
      "25%       0.188757     0.048279     0.091913     0.269247     0.202444  ...   \n",
      "50%       0.275980     0.078794     0.161802     0.381510     0.298452  ...   \n",
      "75%       0.346533     0.160088     0.260600     0.467800     0.445320  ...   \n",
      "max       1.523531     1.191532     1.157940     1.248752     1.496119  ...   \n",
      "\n",
      "       Community_baihe  Community_chonggu  Community_huaxin  Community_jinze  \\\n",
      "count      7843.000000        7843.000000       7843.000000      7843.000000   \n",
      "mean          0.099452           0.039398          0.077649         0.079179   \n",
      "std           0.299287           0.194553          0.267635         0.270035   \n",
      "min           0.000000           0.000000          0.000000         0.000000   \n",
      "25%           0.000000           0.000000          0.000000         0.000000   \n",
      "50%           0.000000           0.000000          0.000000         0.000000   \n",
      "75%           0.000000           0.000000          0.000000         0.000000   \n",
      "max           1.000000           1.000000          1.000000         1.000000   \n",
      "\n",
      "       Community_liantang  Community_xianghuaqiao  Community_xujin  \\\n",
      "count         7843.000000             7843.000000      7843.000000   \n",
      "mean             0.087467                0.041311         0.064261   \n",
      "std              0.282536                0.199021         0.245233   \n",
      "min              0.000000                0.000000         0.000000   \n",
      "25%              0.000000                0.000000         0.000000   \n",
      "50%              0.000000                0.000000         0.000000   \n",
      "75%              0.000000                0.000000         0.000000   \n",
      "max              1.000000                1.000000         1.000000   \n",
      "\n",
      "       Community_yingpu  Community_zhaoxian  Community_zhujiajiao  \n",
      "count       7843.000000         7843.000000           7843.000000  \n",
      "mean           0.372179            0.076756              0.062349  \n",
      "std            0.483417            0.266221              0.241803  \n",
      "min            0.000000            0.000000              0.000000  \n",
      "25%            0.000000            0.000000              0.000000  \n",
      "50%            0.000000            0.000000              0.000000  \n",
      "75%            1.000000            0.000000              0.000000  \n",
      "max            1.000000            1.000000              1.000000  \n",
      "\n",
      "[8 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "for list in kFolds2:\n",
    "    print(list[0].shape, list[1].shape, list[2].shape, list[3].shape)\n",
    "    print(list[0].describe())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e66e43b4-2453-421d-8207-53a8b6b151fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before oversampling class distribution:\n",
      "DR \n",
      "0.0    3303\n",
      "1.0     371\n",
      "Name: count, dtype: int64\n",
      "\n",
      "After oversampling class distribution:\n",
      "DR \n",
      "0.0    3303\n",
      "1.0     371\n",
      "Name: count, dtype: int64\n",
      "Fold: 1, Train: (3674, 28), Test: (1149, 28)\n",
      "Before oversampling class distribution:\n",
      "DR \n",
      "0.0    3303\n",
      "1.0     371\n",
      "Name: count, dtype: int64\n",
      "\n",
      "After oversampling class distribution:\n",
      "DR \n",
      "0.0    3303\n",
      "1.0     371\n",
      "Name: count, dtype: int64\n",
      "Fold: 2, Train: (3674, 28), Test: (1149, 28)\n",
      "Before oversampling class distribution:\n",
      "DR \n",
      "0.0    3304\n",
      "1.0     371\n",
      "Name: count, dtype: int64\n",
      "\n",
      "After oversampling class distribution:\n",
      "DR \n",
      "0.0    3304\n",
      "1.0     371\n",
      "Name: count, dtype: int64\n",
      "Fold: 3, Train: (3675, 28), Test: (1148, 28)\n",
      "Before oversampling class distribution:\n",
      "DR \n",
      "0.0    3304\n",
      "1.0     371\n",
      "Name: count, dtype: int64\n",
      "\n",
      "After oversampling class distribution:\n",
      "DR \n",
      "0.0    3304\n",
      "1.0     371\n",
      "Name: count, dtype: int64\n",
      "Fold: 4, Train: (3675, 28), Test: (1148, 28)\n",
      "Before oversampling class distribution:\n",
      "DR \n",
      "0.0    3304\n",
      "1.0     371\n",
      "Name: count, dtype: int64\n",
      "\n",
      "After oversampling class distribution:\n",
      "DR \n",
      "0.0    3304\n",
      "1.0     371\n",
      "Name: count, dtype: int64\n",
      "Fold: 5, Train: (3675, 28), Test: (1148, 28)\n",
      "Before oversampling class distribution:\n",
      "DR \n",
      "0.0    3303\n",
      "1.0     371\n",
      "Name: count, dtype: int64\n",
      "\n",
      "After oversampling class distribution:\n",
      "DR \n",
      "0.0    3303\n",
      "1.0     371\n",
      "Name: count, dtype: int64\n",
      "Fold: 1, Train: (3674, 28), Test: (1149, 28)\n",
      "Before oversampling class distribution:\n",
      "DR \n",
      "0.0    3303\n",
      "1.0     371\n",
      "Name: count, dtype: int64\n",
      "\n",
      "After oversampling class distribution:\n",
      "DR \n",
      "0.0    3303\n",
      "1.0     371\n",
      "Name: count, dtype: int64\n",
      "Fold: 2, Train: (3674, 28), Test: (1149, 28)\n",
      "Before oversampling class distribution:\n",
      "DR \n",
      "0.0    3304\n",
      "1.0     371\n",
      "Name: count, dtype: int64\n",
      "\n",
      "After oversampling class distribution:\n",
      "DR \n",
      "0.0    3304\n",
      "1.0     371\n",
      "Name: count, dtype: int64\n",
      "Fold: 3, Train: (3675, 28), Test: (1148, 28)\n",
      "Before oversampling class distribution:\n",
      "DR \n",
      "0.0    3304\n",
      "1.0     371\n",
      "Name: count, dtype: int64\n",
      "\n",
      "After oversampling class distribution:\n",
      "DR \n",
      "0.0    3304\n",
      "1.0     371\n",
      "Name: count, dtype: int64\n",
      "Fold: 4, Train: (3675, 28), Test: (1148, 28)\n",
      "Before oversampling class distribution:\n",
      "DR \n",
      "0.0    3304\n",
      "1.0     371\n",
      "Name: count, dtype: int64\n",
      "\n",
      "After oversampling class distribution:\n",
      "DR \n",
      "0.0    3304\n",
      "1.0     371\n",
      "Name: count, dtype: int64\n",
      "Fold: 5, Train: (3675, 28), Test: (1148, 28)\n",
      "\n",
      "==================================================\n",
      "ANALYSIS OF FIRST FOLD (no adasyn)\n",
      "==================================================\n",
      "\n",
      "=== Class Distribution ===\n",
      "Original: 0=DR    5162\n",
      "dtype: int64 | 1=DR    580\n",
      "dtype: int64\n",
      "Processed: 0=DR    3303\n",
      "dtype: int64 | 1=DR    371\n",
      "dtype: int64\n",
      "\n",
      "=== Feature Statistics (Normalized Units) ===\n",
      "Age: 63.80±7.48 → -0.02±0.71 | KS=1.000, p=0.000\n",
      "Gender: 0.55±0.50 → 0.56±0.50 | KS=0.011, p=0.946\n",
      "UAlb: 49.82±124.38 → 0.77±2.33 | KS=0.787, p=0.000\n",
      "Ucr: 4873.05±6063.17 → 0.51±0.66 | KS=0.973, p=0.000\n",
      "UACR: 71.04±1140.34 → 0.85±2.64 | KS=0.819, p=0.000\n",
      "TC: 5.35±1.13 → 0.03±0.74 | KS=0.999, p=0.000\n",
      "TG: 1.84±2.19 → 0.26±0.99 | KS=0.676, p=0.000\n",
      "TCTG: 4.40±2.65 → 0.17±0.81 | KS=0.859, p=0.000\n",
      "LDLC: 3.22±0.95 → 0.01±0.72 | KS=0.953, p=0.000\n",
      "HDLC: 1.33±0.31 → 0.09±0.73 | KS=0.831, p=0.000\n",
      "Scr: 61.60±19.88 → 0.10±0.74 | KS=1.000, p=0.000\n",
      "BUN: 5.97±1.65 → 0.08±0.80 | KS=0.994, p=0.000\n",
      "FPG: 8.89±2.65 → 0.12±0.85 | KS=0.999, p=0.000\n",
      "HbA1c: 7.30±1.48 → 0.15±0.80 | KS=1.000, p=0.000\n",
      "Height: 161.59±7.60 → 0.05±0.66 | KS=1.000, p=0.000\n",
      "Weight: 64.29±12.03 → 0.04±0.70 | KS=1.000, p=0.000\n",
      "BMI: 24.61±4.54 → 0.04±0.79 | KS=1.000, p=0.000\n",
      "Duration: 7.82±5.84 → 0.17±0.65 | KS=0.849, p=0.000\n",
      "Community_baihe: 0.12±0.32 → 0.13±0.34 | KS=0.012, p=0.902\n",
      "Community_chonggu: 0.07±0.26 → 0.07±0.26 | KS=0.001, p=1.000\n",
      "Community_huaxin: 0.11±0.31 → 0.11±0.31 | KS=0.000, p=1.000\n",
      "Community_jinze: 0.08±0.28 → 0.08±0.27 | KS=0.005, p=1.000\n",
      "Community_liantang: 0.10±0.31 → 0.10±0.31 | KS=0.000, p=1.000\n",
      "Community_xianghuaqiao: 0.08±0.27 → 0.07±0.26 | KS=0.002, p=1.000\n",
      "Community_xujin: 0.09±0.28 → 0.08±0.27 | KS=0.009, p=0.987\n",
      "Community_yingpu: 0.13±0.33 → 0.12±0.33 | KS=0.003, p=1.000\n",
      "Community_zhaoxian: 0.11±0.31 → 0.11±0.32 | KS=0.008, p=0.999\n",
      "Community_zhujiajiao: 0.11±0.32 → 0.11±0.32 | KS=0.001, p=1.000\n",
      "\n",
      "==================================================\n",
      "ANALYSIS OF FIRST FOLD (with adasyn)\n",
      "==================================================\n",
      "\n",
      "=== Class Distribution ===\n",
      "Original: 0=DR    5162\n",
      "dtype: int64 | 1=DR    580\n",
      "dtype: int64\n",
      "Processed: 0=DR    3303\n",
      "dtype: int64 | 1=DR    371\n",
      "dtype: int64\n",
      "\n",
      "=== Feature Statistics (Normalized Units) ===\n",
      "Age: 63.80±7.48 → -0.03±0.70 | KS=1.000, p=0.000\n",
      "Gender: 0.55±0.50 → 0.56±0.50 | KS=0.014, p=0.767\n",
      "UAlb: 49.82±124.38 → 0.68±1.96 | KS=0.792, p=0.000\n",
      "Ucr: 4873.05±6063.17 → 0.51±0.66 | KS=0.973, p=0.000\n",
      "UACR: 71.04±1140.34 → 0.82±2.78 | KS=0.824, p=0.000\n",
      "TC: 5.35±1.13 → 0.03±0.72 | KS=0.999, p=0.000\n",
      "TG: 1.84±2.19 → 0.26±1.05 | KS=0.681, p=0.000\n",
      "TCTG: 4.40±2.65 → 0.17±0.79 | KS=0.856, p=0.000\n",
      "LDLC: 3.22±0.95 → 0.02±0.73 | KS=0.948, p=0.000\n",
      "HDLC: 1.33±0.31 → 0.09±0.73 | KS=0.830, p=0.000\n",
      "Scr: 61.60±19.88 → 0.08±0.73 | KS=1.000, p=0.000\n",
      "BUN: 5.97±1.65 → 0.11±0.74 | KS=0.995, p=0.000\n",
      "FPG: 8.89±2.65 → 0.14±0.83 | KS=0.998, p=0.000\n",
      "HbA1c: 7.30±1.48 → 0.12±0.75 | KS=1.000, p=0.000\n",
      "Height: 161.59±7.60 → 0.05±0.61 | KS=1.000, p=0.000\n",
      "Weight: 64.29±12.03 → 0.05±0.71 | KS=1.000, p=0.000\n",
      "BMI: 24.61±4.54 → 0.06±0.80 | KS=1.000, p=0.000\n",
      "Duration: 7.82±5.84 → 0.19±0.67 | KS=0.843, p=0.000\n",
      "Community_baihe: 0.12±0.32 → 0.13±0.34 | KS=0.012, p=0.871\n",
      "Community_chonggu: 0.07±0.26 → 0.07±0.25 | KS=0.006, p=1.000\n",
      "Community_huaxin: 0.11±0.31 → 0.11±0.32 | KS=0.004, p=1.000\n",
      "Community_jinze: 0.08±0.28 → 0.08±0.28 | KS=0.001, p=1.000\n",
      "Community_liantang: 0.10±0.31 → 0.10±0.30 | KS=0.001, p=1.000\n",
      "Community_xianghuaqiao: 0.08±0.27 → 0.07±0.26 | KS=0.002, p=1.000\n",
      "Community_xujin: 0.09±0.28 → 0.08±0.27 | KS=0.008, p=0.999\n",
      "Community_yingpu: 0.13±0.33 → 0.12±0.33 | KS=0.005, p=1.000\n",
      "Community_zhaoxian: 0.11±0.31 → 0.11±0.32 | KS=0.007, p=1.000\n",
      "Community_zhujiajiao: 0.11±0.32 → 0.11±0.32 | KS=0.002, p=1.000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'class_counts': {'original_0': DR    5162\n",
       "  dtype: int64,\n",
       "  'original_1': DR    580\n",
       "  dtype: int64,\n",
       "  'processed_0': DR    3303\n",
       "  dtype: int64,\n",
       "  'processed_1': DR    371\n",
       "  dtype: int64},\n",
       " 'feature_stats': {'Age': {'mean_orig': 63.8007662835249,\n",
       "   'std_orig': 7.483781181651379,\n",
       "   'mean_proc': -0.028007621121393583,\n",
       "   'std_proc': 0.7039014638695024,\n",
       "   'ks_stat': 1.0,\n",
       "   'ks_p': 0.0},\n",
       "  'Gender': {'mean_orig': 0.5499825844653431,\n",
       "   'std_orig': 0.49753879497959264,\n",
       "   'mean_proc': 0.5639629831246598,\n",
       "   'std_proc': 0.4959593603989724,\n",
       "   'ks_stat': 0.013980398659316686,\n",
       "   'ks_p': 0.7665395597412806},\n",
       "  'UAlb': {'mean_orig': 49.82375478927203,\n",
       "   'std_orig': 124.38322984308897,\n",
       "   'mean_proc': 0.6805222240344215,\n",
       "   'std_proc': 1.9620624568463496,\n",
       "   'ks_stat': 0.7919521458650098,\n",
       "   'ks_p': 0.0},\n",
       "  'Ucr': {'mean_orig': 4873.045280390108,\n",
       "   'std_orig': 6063.166596062857,\n",
       "   'mean_proc': 0.5145554128711642,\n",
       "   'std_proc': 0.6600181606642664,\n",
       "   'ks_stat': 0.9731800766283525,\n",
       "   'ks_p': 0.0},\n",
       "  'UACR': {'mean_orig': 71.03669453152213,\n",
       "   'std_orig': 1140.3448757527442,\n",
       "   'mean_proc': 0.8213270428839291,\n",
       "   'std_proc': 2.7809746202017274,\n",
       "   'ks_stat': 0.8236223477809271,\n",
       "   'ks_p': 0.0},\n",
       "  'TC': {'mean_orig': 5.348883664228492,\n",
       "   'std_orig': 1.131424206834216,\n",
       "   'mean_proc': 0.03199970852854028,\n",
       "   'std_proc': 0.7236842862564025,\n",
       "   'ks_stat': 0.9993795063999483,\n",
       "   'ks_p': 0.0},\n",
       "  'TG': {'mean_orig': 1.8427621037965864,\n",
       "   'std_orig': 2.1912216272330896,\n",
       "   'mean_proc': 0.26170386499727805,\n",
       "   'std_proc': 1.050842433674987,\n",
       "   'ks_stat': 0.6809056912298704,\n",
       "   'ks_p': 0.0},\n",
       "  'TCTG': {'mean_orig': 4.404496691048416,\n",
       "   'std_orig': 2.654190243437801,\n",
       "   'mean_proc': 0.17307395757662464,\n",
       "   'std_proc': 0.794875987835137,\n",
       "   'ks_stat': 0.8564563662643365,\n",
       "   'ks_p': 0.0},\n",
       "  'LDLC': {'mean_orig': 3.2173458725182864,\n",
       "   'std_orig': 0.9530179011207903,\n",
       "   'mean_proc': 0.024207711060093284,\n",
       "   'std_proc': 0.728354188464138,\n",
       "   'ks_stat': 0.948117349418196,\n",
       "   'ks_p': 0.0},\n",
       "  'HDLC': {'mean_orig': 1.3343138279345177,\n",
       "   'std_orig': 0.31253782115169026,\n",
       "   'mean_proc': 0.09024295905796058,\n",
       "   'std_proc': 0.7256456913065169,\n",
       "   'ks_stat': 0.8300097818991067,\n",
       "   'ks_p': 0.0},\n",
       "  'Scr': {'mean_orig': 61.60344827586207,\n",
       "   'std_orig': 19.88248900543915,\n",
       "   'mean_proc': 0.08402891187322326,\n",
       "   'std_proc': 0.7282270098682222,\n",
       "   'ks_stat': 1.0,\n",
       "   'ks_p': 0.0},\n",
       "  'BUN': {'mean_orig': 5.971891327063741,\n",
       "   'std_orig': 1.6514820641531318,\n",
       "   'mean_proc': 0.1056372104276298,\n",
       "   'std_proc': 0.7442053639825694,\n",
       "   'ks_stat': 0.9954823894530688,\n",
       "   'ks_p': 0.0},\n",
       "  'FPG': {'mean_orig': 8.891797283176594,\n",
       "   'std_orig': 2.64746656142721,\n",
       "   'mean_proc': 0.13867174741426266,\n",
       "   'std_proc': 0.8338058604613323,\n",
       "   'ks_stat': 0.9977464089584676,\n",
       "   'ks_p': 0.0},\n",
       "  'HbA1c': {'mean_orig': 7.296847788227098,\n",
       "   'std_orig': 1.475932031144591,\n",
       "   'mean_proc': 0.11668140990745762,\n",
       "   'std_proc': 0.7462883439800928,\n",
       "   'ks_stat': 0.9998258446534308,\n",
       "   'ks_p': 0.0},\n",
       "  'Height': {'mean_orig': 161.5921281783351,\n",
       "   'std_orig': 7.599456844350907,\n",
       "   'mean_proc': 0.04756396298312466,\n",
       "   'std_proc': 0.6076988711995797,\n",
       "   'ks_stat': 1.0,\n",
       "   'ks_p': 0.0},\n",
       "  'Weight': {'mean_orig': 64.28894113549286,\n",
       "   'std_orig': 12.031707281237942,\n",
       "   'mean_proc': 0.05182135728542915,\n",
       "   'std_proc': 0.7138931623078688,\n",
       "   'ks_stat': 1.0,\n",
       "   'ks_p': 0.0},\n",
       "  'BMI': {'mean_orig': 24.609103099965168,\n",
       "   'std_orig': 4.537095508416528,\n",
       "   'mean_proc': 0.05971504180288963,\n",
       "   'std_proc': 0.799803379540628,\n",
       "   'ks_stat': 1.0,\n",
       "   'ks_p': 0.0},\n",
       "  'Duration': {'mean_orig': 7.8191048415186355,\n",
       "   'std_orig': 5.841676066995645,\n",
       "   'mean_proc': 0.18582222193072778,\n",
       "   'std_proc': 0.6673974083643033,\n",
       "   'ks_stat': 0.8428117641415184,\n",
       "   'ks_p': 0.0},\n",
       "  'Community_baihe': {'mean_orig': 0.11790316962730756,\n",
       "   'std_orig': 0.32252151537620416,\n",
       "   'mean_proc': 0.13037561241154055,\n",
       "   'std_proc': 0.3367620525000539,\n",
       "   'ks_stat': 0.012472442784232997,\n",
       "   'ks_p': 0.8710926668088909},\n",
       "  'Community_chonggu': {'mean_orig': 0.0724486241727621,\n",
       "   'std_orig': 0.2592518587684336,\n",
       "   'mean_proc': 0.06668481219379423,\n",
       "   'std_proc': 0.24950930390297396,\n",
       "   'ks_stat': 0.005763811978967874,\n",
       "   'ks_p': 0.9999990358237717},\n",
       "  'Community_huaxin': {'mean_orig': 0.11058864507140369,\n",
       "   'std_orig': 0.3136493732605634,\n",
       "   'mean_proc': 0.1145890038105607,\n",
       "   'std_proc': 0.3185686531458381,\n",
       "   'ks_stat': 0.0040003587391570046,\n",
       "   'ks_p': 0.9999999999999316},\n",
       "  'Community_jinze': {'mean_orig': 0.08359456635318704,\n",
       "   'std_orig': 0.2768029237335401,\n",
       "   'mean_proc': 0.08301578660860098,\n",
       "   'std_proc': 0.2759436375787054,\n",
       "   'ks_stat': 0.000578779744586063,\n",
       "   'ks_p': 1.0},\n",
       "  'Community_liantang': {'mean_orig': 0.10414489724834552,\n",
       "   'std_orig': 0.305475021777091,\n",
       "   'mean_proc': 0.10315732172019597,\n",
       "   'std_proc': 0.3042056488545256,\n",
       "   'ks_stat': 0.0009875755281495526,\n",
       "   'ks_p': 1.0},\n",
       "  'Community_xianghuaqiao': {'mean_orig': 0.07610588645071403,\n",
       "   'std_orig': 0.2651905506462678,\n",
       "   'mean_proc': 0.07430593358737071,\n",
       "   'std_proc': 0.26230381030449795,\n",
       "   'ks_stat': 0.0017999528633433238,\n",
       "   'ks_p': 1.0},\n",
       "  'Community_xujin': {'mean_orig': 0.0886450714036921,\n",
       "   'std_orig': 0.28425550949733414,\n",
       "   'mean_proc': 0.08083832335329341,\n",
       "   'std_proc': 0.2726237672626312,\n",
       "   'ks_stat': 0.0078067480503986805,\n",
       "   'ks_p': 0.9990126132226658},\n",
       "  'Community_yingpu': {'mean_orig': 0.12591431556948798,\n",
       "   'std_orig': 0.3317816624875664,\n",
       "   'mean_proc': 0.12139357648339684,\n",
       "   'std_proc': 0.32662855698160387,\n",
       "   'ks_stat': 0.0045207390860911405,\n",
       "   'ks_p': 0.9999999999335996},\n",
       "  'Community_zhaoxian': {'mean_orig': 0.10658307210031348,\n",
       "   'std_orig': 0.30860931182680296,\n",
       "   'mean_proc': 0.11377245508982035,\n",
       "   'std_proc': 0.3175779191942049,\n",
       "   'ks_stat': 0.0071893829895068794,\n",
       "   'ks_p': 0.9997748396147788},\n",
       "  'Community_zhujiajiao': {'mean_orig': 0.11407175200278649,\n",
       "   'std_orig': 0.31792607709357734,\n",
       "   'mean_proc': 0.11186717474142624,\n",
       "   'std_proc': 0.31524587147662775,\n",
       "   'ks_stat': 0.002204577261360247,\n",
       "   'ks_p': 1.0}}}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================\n",
    "# PARAMETER EXPERIMENTATION\n",
    "# =============================================\n",
    "# 1. First capture the original data distribution\n",
    "original_X = X_FOR_FOLDS.copy()\n",
    "original_y = Y_FOR_FOLDS.copy()\n",
    "\n",
    "# 2. Generate folds as you normally would\n",
    "kFolds_without_adasyn = FOLDS_GENERATOR(X_FOR_FOLDS, Y_FOR_FOLDS,\n",
    "                        normalisation_method=RobustScaler(),\n",
    "                        n_splits=5,\n",
    "                        oversampler=None,\n",
    "                        contamination=0.2,\n",
    "                        noise=None)\n",
    "\n",
    "oversampler = ADASYN(sampling_strategy='minority', n_neighbors=10, random_state=42)\n",
    "kFolds_with_adasyn = FOLDS_GENERATOR(X_FOR_FOLDS, Y_FOR_FOLDS,\n",
    "                        normalisation_method=RobustScaler(),\n",
    "                        n_splits=5,\n",
    "                        oversampler=None,\n",
    "                        contamination=0.2,\n",
    "                        noise=None)\n",
    "\n",
    "# 3. Analyze the first fold\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ANALYSIS OF FIRST FOLD (no adasyn)\")\n",
    "print(\"=\"*50)\n",
    "analyze_augmentation_impact(original_X, original_y, kFolds_without_adasyn[0])\n",
    "# quality_metrics = evaluate_augmentation_quality(original_X, original_y, kFolds_without_adasyn[0])\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ANALYSIS OF FIRST FOLD (with adasyn)\")\n",
    "print(\"=\"*50)\n",
    "# quality_metrics = evaluate_augmentation_quality(original_X, original_y, kFolds_with_adasyn[0])\n",
    "analyze_augmentation_impact(original_X, original_y, kFolds_with_adasyn[0])\n",
    "\n",
    "# print(\"\\nQuality Metrics:\")\n",
    "# # for k, v in quality_metrics.items():\n",
    "# #     print(f\"{k}: {v}\")\n",
    "# quality_metrics = evaluate_augmentation_quality(original_X, original_y, (X_FOR_FOLDS, X_FINAL_TEST, Y_FOR_FOLDS, Y_FINAL_TEST))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60fc1a3-152b-420f-8f05-3ed3f64c83a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
