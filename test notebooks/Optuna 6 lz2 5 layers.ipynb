{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d772ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in d:\\github repos\\adl\\.venv\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.23.2 in d:\\github repos\\adl\\.venv\\lib\\site-packages (from pandas) (2.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\github repos\\adl\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\github repos\\adl\\.venv\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\github repos\\adl\\.venv\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in d:\\github repos\\adl\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: optuna in d:\\github repos\\adl\\.venv\\lib\\site-packages (4.2.1)\n",
      "Requirement already satisfied: alembic>=1.5.0 in d:\\github repos\\adl\\.venv\\lib\\site-packages (from optuna) (1.15.2)\n",
      "Requirement already satisfied: colorlog in d:\\github repos\\adl\\.venv\\lib\\site-packages (from optuna) (6.9.0)\n",
      "Requirement already satisfied: numpy in d:\\github repos\\adl\\.venv\\lib\\site-packages (from optuna) (2.2.4)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\github repos\\adl\\.venv\\lib\\site-packages (from optuna) (24.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in d:\\github repos\\adl\\.venv\\lib\\site-packages (from optuna) (2.0.40)\n",
      "Requirement already satisfied: tqdm in d:\\github repos\\adl\\.venv\\lib\\site-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in d:\\github repos\\adl\\.venv\\lib\\site-packages (from optuna) (6.0.2)\n",
      "Requirement already satisfied: Mako in d:\\github repos\\adl\\.venv\\lib\\site-packages (from alembic>=1.5.0->optuna) (1.3.9)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in d:\\github repos\\adl\\.venv\\lib\\site-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
      "Requirement already satisfied: greenlet>=1 in d:\\github repos\\adl\\.venv\\lib\\site-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
      "Requirement already satisfied: colorama in d:\\github repos\\adl\\.venv\\lib\\site-packages (from colorlog->optuna) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in d:\\github repos\\adl\\.venv\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
      "Requirement already satisfied: optuna-dashboard in d:\\github repos\\adl\\.venv\\lib\\site-packages (0.18.0)\n",
      "Requirement already satisfied: bottle>=0.13.0 in d:\\github repos\\adl\\.venv\\lib\\site-packages (from optuna-dashboard) (0.13.2)\n",
      "Requirement already satisfied: optuna>=3.1.0 in d:\\github repos\\adl\\.venv\\lib\\site-packages (from optuna-dashboard) (4.2.1)\n",
      "Requirement already satisfied: packaging in d:\\github repos\\adl\\.venv\\lib\\site-packages (from optuna-dashboard) (24.2)\n",
      "Requirement already satisfied: scikit-learn in d:\\github repos\\adl\\.venv\\lib\\site-packages (from optuna-dashboard) (1.6.1)\n",
      "Requirement already satisfied: alembic>=1.5.0 in d:\\github repos\\adl\\.venv\\lib\\site-packages (from optuna>=3.1.0->optuna-dashboard) (1.15.2)\n",
      "Requirement already satisfied: colorlog in d:\\github repos\\adl\\.venv\\lib\\site-packages (from optuna>=3.1.0->optuna-dashboard) (6.9.0)\n",
      "Requirement already satisfied: numpy in d:\\github repos\\adl\\.venv\\lib\\site-packages (from optuna>=3.1.0->optuna-dashboard) (2.2.4)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in d:\\github repos\\adl\\.venv\\lib\\site-packages (from optuna>=3.1.0->optuna-dashboard) (2.0.40)\n",
      "Requirement already satisfied: tqdm in d:\\github repos\\adl\\.venv\\lib\\site-packages (from optuna>=3.1.0->optuna-dashboard) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in d:\\github repos\\adl\\.venv\\lib\\site-packages (from optuna>=3.1.0->optuna-dashboard) (6.0.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in d:\\github repos\\adl\\.venv\\lib\\site-packages (from scikit-learn->optuna-dashboard) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\github repos\\adl\\.venv\\lib\\site-packages (from scikit-learn->optuna-dashboard) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\github repos\\adl\\.venv\\lib\\site-packages (from scikit-learn->optuna-dashboard) (3.6.0)\n",
      "Requirement already satisfied: Mako in d:\\github repos\\adl\\.venv\\lib\\site-packages (from alembic>=1.5.0->optuna>=3.1.0->optuna-dashboard) (1.3.9)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in d:\\github repos\\adl\\.venv\\lib\\site-packages (from alembic>=1.5.0->optuna>=3.1.0->optuna-dashboard) (4.12.2)\n",
      "Requirement already satisfied: greenlet>=1 in d:\\github repos\\adl\\.venv\\lib\\site-packages (from sqlalchemy>=1.4.2->optuna>=3.1.0->optuna-dashboard) (3.1.1)\n",
      "Requirement already satisfied: colorama in d:\\github repos\\adl\\.venv\\lib\\site-packages (from colorlog->optuna>=3.1.0->optuna-dashboard) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in d:\\github repos\\adl\\.venv\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna>=3.1.0->optuna-dashboard) (3.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement scikit (from versions: none)\n",
      "ERROR: No matching distribution found for scikit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imbalanced-learn in d:\\github repos\\adl\\.venv\\lib\\site-packages (0.13.0)\n",
      "Requirement already satisfied: numpy<3,>=1.24.3 in d:\\github repos\\adl\\.venv\\lib\\site-packages (from imbalanced-learn) (2.2.4)\n",
      "Requirement already satisfied: scipy<2,>=1.10.1 in d:\\github repos\\adl\\.venv\\lib\\site-packages (from imbalanced-learn) (1.15.2)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.3.2 in d:\\github repos\\adl\\.venv\\lib\\site-packages (from imbalanced-learn) (1.6.1)\n",
      "Requirement already satisfied: sklearn-compat<1,>=0.1 in d:\\github repos\\adl\\.venv\\lib\\site-packages (from imbalanced-learn) (0.1.3)\n",
      "Requirement already satisfied: joblib<2,>=1.1.1 in d:\\github repos\\adl\\.venv\\lib\\site-packages (from imbalanced-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in d:\\github repos\\adl\\.venv\\lib\\site-packages (from imbalanced-learn) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install optuna\n",
    "!pip install optuna-dashboard\n",
    "!pip install scikit\n",
    "!pip install imbalanced-learn\n",
    "from sklearn.preprocessing import (\n",
    "    MaxAbsScaler,\n",
    "    MinMaxScaler,\n",
    "    Normalizer,\n",
    "    PowerTransformer,\n",
    "    QuantileTransformer,\n",
    "    RobustScaler,\n",
    "    StandardScaler,\n",
    "    minmax_scale,\n",
    ")\n",
    "from sklearn.metrics import recall_score, accuracy_score,f1_score, precision_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import warnings\n",
    "import optuna\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9bdf1a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using\", device)\n",
    "def init_weights(model):\n",
    "    if isinstance(model, nn.Linear):  # Apply only to linear layers\n",
    "        # He initialization (recommended for ReLU activations)\n",
    "        # print(\"Initializing weights using kaiming\")\n",
    "        nn.init.kaiming_normal_(model.weight, mode='fan_in', nonlinearity='relu')\n",
    "        \n",
    "        # Bias initialization (zero initialization is fine)\n",
    "        if model.bias is not None:\n",
    "            nn.init.zeros_(model.bias)\n",
    "def fold_to_dataloader_tensor(train_x, test_x, train_y, test_y, batch_size=64, device=device):\n",
    "    train_dataset = TensorDataset(\n",
    "        torch.tensor(train_x.values,dtype=torch.float32).to(device), \n",
    "        torch.tensor(train_y.values,dtype=torch.float32).to(device))\n",
    "    val_dataset = TensorDataset(\n",
    "        torch.tensor(test_x.values,dtype=torch.float32).to(device), \n",
    "        torch.tensor(test_y.values,dtype=torch.float32).to(device))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=True, drop_last=True)\n",
    "    return train_loader, val_loader \n",
    "\n",
    "def get_feature_count(loader):\n",
    "    \"\"\"returns the number of features in the dataset\"\"\"\n",
    "    return next(iter(loader))[0].shape[1]\n",
    "\n",
    "from Criterion_Models import *\n",
    "def criterion_mapping(criterion_choice:str, pos_weight:float=None, alpha:float=None, gamma:float=None):\n",
    "    \"\"\"\n",
    "    Feel free to add any custom loss functions here.\n",
    "    returns function for criterion\n",
    "    \"\"\"\n",
    "    if criterion_choice == \"FocalLoss\":\n",
    "        return FocalLoss(alpha =alpha, gamma=gamma)\n",
    "    elif criterion_choice == \"DiceLoss\":\n",
    "        return DiceLoss()\n",
    "    elif criterion_choice == \"BCEWithLogitsLoss\":\n",
    "        return nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight])) if pos_weight else nn.BCEWithLogitsLoss()\n",
    "    return nn.BCEWithLogitsLoss() \n",
    "\n",
    "def augment_data_in_place(X, X_test, Y=None, normalisation_method=MinMaxScaler(), noise=None):\n",
    "    all_numerical_columns = ['Age', 'UAlb', 'Ucr', 'UACR', 'TC', 'TG', 'TCTG', 'LDLC', 'HDLC', 'Scr', 'BUN', 'FPG', 'HbA1c', 'Height', 'Weight', 'BMI', 'Duration']\n",
    "    binary_columns = ['Gender', 'DR', 'Community_baihe', 'Community_chonggu', 'Community_huaxin', 'Community_jinze', 'Community_liantang', 'Community_xianghuaqiao', 'Community_xujin', 'Community_yingpu', 'Community_zhaoxian', 'Community_zhujiajiao']\n",
    "    \n",
    "    existing_columns = [col for col in all_numerical_columns if col in X.columns and col in X_test.columns]\n",
    "\n",
    "    if not existing_columns:\n",
    "        print(\"No matching columns found for augmentation. Normalised data only.\")\n",
    "        X = normalisation_method.fit_transform(X)\n",
    "        X_test = normalisation_method.transform(X_test)\n",
    "        return X, X_test\n",
    "\n",
    "    X_copy = X.copy()\n",
    "    X_test_copy = X_test.copy()\n",
    "    \n",
    "    # Log-transform\n",
    "    X_copy.loc[:, existing_columns] = X_copy.loc[:, existing_columns].apply(np.log1p)\n",
    "    X_test_copy.loc[:, existing_columns] = X_test_copy.loc[:, existing_columns].apply(np.log1p)\n",
    "\n",
    "    # Add noise ONLY to negatives (class 0) if Y is provided and noise is set\n",
    "    if noise and noise > 0:\n",
    "        if Y is None:\n",
    "            raise ValueError(\"Y must be provided if noise is being added selectively.\")\n",
    "        # Identify negative class indices (class 0)\n",
    "        negative_indices = Y[Y.iloc[:, 0] == 0].index\n",
    "        noise_matrix = np.random.normal(0, noise, X_copy.loc[negative_indices, existing_columns].shape)\n",
    "        X_copy.loc[negative_indices, existing_columns] += noise_matrix\n",
    "\n",
    "    # Scale\n",
    "    scaler = normalisation_method\n",
    "    X_copy.loc[:, existing_columns] = scaler.fit_transform(X_copy.loc[:, existing_columns])\n",
    "    X_test_copy.loc[:, existing_columns] = scaler.transform(X_test_copy.loc[:, existing_columns])\n",
    "\n",
    "    return X_copy, X_test_copy\n",
    "\n",
    "\n",
    "def iso_forest(X_train, Y_train, contamination=None, random_state=42):\n",
    "    # print(\"Original\\n\", X_train.shape, Y_train.shape, X_test.shape, Y_test.shape)\n",
    "    X_train_cleaned, Y_train_cleaned = X_train.copy(), Y_train.copy()\n",
    "    \n",
    "    X_train_zeros = X_train[Y_train.iloc[:, 0] == 0]\n",
    "    X_train_ones = X_train[Y_train.iloc[:, 0] == 1]\n",
    "    Y_train_zeros = Y_train[Y_train.iloc[:, 0] == 0]\n",
    "    Y_train_ones = Y_train[Y_train.iloc[:, 0] == 1] \n",
    "    # print(\"Ones and zeros\\n\", X_train_zeros.shape, Y_train_zeros.shape, X_train_ones.shape, Y_train_ones.shape)\n",
    "    #only class 0s\n",
    "    if X_train_zeros.isna().any().any():\n",
    "        print(\"got NaN values in the training set\")\n",
    "    \n",
    "    # Apply Isolation Forest to majority class only\n",
    "    iso_forest = IsolationForest(contamination=contamination, random_state=random_state)\n",
    "    try:\n",
    "        outliers = iso_forest.fit_predict(X_train_zeros)\n",
    "    except UserWarning as e:\n",
    "        print(\"Caught warning during IsolationForest fitting:\", e)\n",
    "        outliers = np.ones(len(X_train_zeros))  # If warning occurs, keep all data\n",
    "    # Keep only non-outlier majority samples\n",
    "    X_train_zeros = X_train_zeros[outliers == 1]\n",
    "    Y_train_zeros = Y_train_zeros[outliers == 1]\n",
    "    # print(\"After iso:\\n\", X_train_zeros.shape, Y_train_zeros.shape, X_train_ones.shape, Y_train_ones.shape)\n",
    "    \n",
    "    # Combine the cleaned majority class with the untouched minority class\n",
    "    X_train_cleaned = pd.concat([X_train_zeros, X_train_ones])\n",
    "    Y_train_cleaned = pd.concat([Y_train_zeros, Y_train_ones])\n",
    "    return X_train_cleaned, Y_train_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74d7ac6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Age  Gender   UAlb     Ucr    UACR    TC     TG  TCTG  LDLC  HDLC  ...  \\\n",
      "0     63.0     1.0   18.3     5.0    29.1  4.97   5.98  0.83  2.31  0.97  ...   \n",
      "1     64.0     0.0   14.8     4.0    29.3  3.92   1.77  2.21  1.44  1.71  ...   \n",
      "2     84.0     1.0   60.5    18.0    29.5  3.97   2.90  1.37  1.96  0.94  ...   \n",
      "3     53.0     0.0    0.2     3.0     0.3  8.57   4.69  1.83  4.54  1.74  ...   \n",
      "4     88.0     1.0    0.1     4.0     0.5  4.70   1.30  3.62  3.29  0.88  ...   \n",
      "...    ...     ...    ...     ...     ...   ...    ...   ...   ...   ...  ...   \n",
      "6375  70.0     0.0  291.7  3352.0   770.1  4.63   4.35  1.06  2.23  1.05  ...   \n",
      "6376  70.0     0.0  467.0  4805.0   860.1  4.66   1.23  3.79  2.81  1.12  ...   \n",
      "6377  66.0     0.0  931.6  8681.0   949.7  5.45   1.00  5.45  3.41  1.66  ...   \n",
      "6378  53.0     0.0  787.5  7141.0   975.9  9.62  18.66  3.88  0.98  0.65  ...   \n",
      "6379  57.0     1.0  508.4  3570.0  1260.3  4.38   2.83  1.55  2.37  1.07  ...   \n",
      "\n",
      "      Community_baihe  Community_chonggu  Community_huaxin  Community_jinze  \\\n",
      "0                 0.0                1.0               0.0              0.0   \n",
      "1                 0.0                1.0               0.0              0.0   \n",
      "2                 0.0                1.0               0.0              0.0   \n",
      "3                 0.0                1.0               0.0              0.0   \n",
      "4                 0.0                1.0               0.0              0.0   \n",
      "...               ...                ...               ...              ...   \n",
      "6375              0.0                0.0               0.0              0.0   \n",
      "6376              0.0                0.0               0.0              0.0   \n",
      "6377              0.0                0.0               0.0              0.0   \n",
      "6378              0.0                0.0               0.0              0.0   \n",
      "6379              0.0                0.0               0.0              0.0   \n",
      "\n",
      "      Community_liantang  Community_xianghuaqiao  Community_xujin  \\\n",
      "0                    0.0                     0.0              0.0   \n",
      "1                    0.0                     0.0              0.0   \n",
      "2                    0.0                     0.0              0.0   \n",
      "3                    0.0                     0.0              0.0   \n",
      "4                    0.0                     0.0              0.0   \n",
      "...                  ...                     ...              ...   \n",
      "6375                 0.0                     0.0              0.0   \n",
      "6376                 0.0                     0.0              0.0   \n",
      "6377                 0.0                     0.0              0.0   \n",
      "6378                 0.0                     0.0              0.0   \n",
      "6379                 0.0                     0.0              0.0   \n",
      "\n",
      "      Community_yingpu  Community_zhaoxian  Community_zhujiajiao  \n",
      "0                  0.0                 0.0                   0.0  \n",
      "1                  0.0                 0.0                   0.0  \n",
      "2                  0.0                 0.0                   0.0  \n",
      "3                  0.0                 0.0                   0.0  \n",
      "4                  0.0                 0.0                   0.0  \n",
      "...                ...                 ...                   ...  \n",
      "6375               0.0                 0.0                   1.0  \n",
      "6376               0.0                 0.0                   1.0  \n",
      "6377               0.0                 0.0                   1.0  \n",
      "6378               0.0                 0.0                   1.0  \n",
      "6379               0.0                 0.0                   1.0  \n",
      "\n",
      "[6380 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "random_state = 69\n",
    "raw_dataset = pd.read_csv(\"./data/processed_data.csv\") #data has X and Y\n",
    "X = raw_dataset.drop(columns=[\"DR\"])\n",
    "Y = pd.DataFrame(raw_dataset[\"DR\"])\n",
    "# Slice your data\n",
    "# X = X.drop(columns = ['Gender', 'Community_baihe', 'Community_chonggu', 'Community_huaxin', 'Community_jinze', 'Community_liantang', 'Community_xianghuaqiao', 'Community_xujin', 'Community_yingpu', 'Community_zhaoxian', 'Community_zhujiajiao'])\n",
    "# all_numerical_columns = ['Age', 'UAlb', 'Ucr', 'UACR', 'TC', 'TG', 'TCTG', 'LDLC', 'HDLC', 'Scr', 'BUN', 'FPG', 'HbA1c', 'Height', 'Weight', 'BMI', 'Duration']\n",
    "#     binary_columns = ['Gender', 'DR', 'Community_baihe', 'Community_chonggu', 'Community_huaxin', 'Community_jinze', 'Community_liantang', 'Community_xianghuaqiao', 'Community_xujin', 'Community_yingpu', 'Community_zhaoxian', 'Community_zhujiajiao']\n",
    "print(X)\n",
    "X_FOR_FOLDS, X_FINAL_TEST, Y_FOR_FOLDS, Y_FINAL_TEST = train_test_split(X, Y, test_size=0.1, random_state=random_state, stratify=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f38ac840",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FOLDS_GENERATOR(X, Y, normalisation_method=MinMaxScaler(), n_splits=5, random_state=None, oversampler=None, contamination=0.05, noise = None):\n",
    "    \"\"\"\n",
    "    Generates stratified folds with specified normalization.\n",
    "    normalisation_method should be an instance of a scaler, e.g.,\n",
    "    - MinMaxScaler()\n",
    "    Returns a list of tuples, each containing:\n",
    "    (X_train_scaled, X_test_scaled, Y_train, Y_test), representing data for each fold\n",
    "    \"\"\"\n",
    "    kF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    kFolds_list = []\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(kF.split(X, Y)):\n",
    "        # Split the data into training and testing sets for this fold\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        Y_train, Y_test = Y.iloc[train_idx], Y.iloc[test_idx]\n",
    "\n",
    "        # print(\"Original\\n\", X_train.shape, Y_train.shape, X_test.shape, Y_test.shape)\n",
    "        X_train_cleaned, Y_train_cleaned = X_train.copy(), Y_train.copy()\n",
    "        if contamination is not None and contamination > 0: #? using contamination = 0.0 works\n",
    "            X_train_cleaned, Y_train_cleaned = iso_forest(X_train, Y_train, contamination=contamination, random_state=random_state)\n",
    "        \n",
    "        #? data augmentation on leftover data\n",
    "        X_train_scaled, X_test_scaled = augment_data_in_place(X_train_cleaned, X_test, Y_train_cleaned,normalisation_method=normalisation_method, noise = noise)\n",
    "        \n",
    "        # Handle oversampling if needed\n",
    "        #! use X_train_scaled and Y_train_cleaned for oversampling becasue y_train_cleaned no changes after augmentation\n",
    "        # print(\"Before oversampling class distribution:\")\n",
    "        # print(Y_train_cleaned.value_counts())\n",
    "        if oversampler:\n",
    "            X_train_scaled, Y_train_cleaned = oversampler.fit_resample(X_train_scaled, Y_train_cleaned)\n",
    "        # print(\"\\nAfter oversampling class distribution:\")\n",
    "        # print(Y_train_cleaned.value_counts())\n",
    "        # Convert scaled data back to DataFrame with the correct column names\n",
    "        X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train_cleaned.columns)\n",
    "        X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "\n",
    "        # Handle community columns\n",
    "        community_cols = [col for col in X_train_scaled.columns if col.startswith('Community')]\n",
    "        if community_cols:\n",
    "            X_train_scaled[community_cols] = X_train_scaled[community_cols].apply(\n",
    "                lambda row: pd.Series(np.eye(len(row))[row.argmax()]), axis=1\n",
    "            ).set_axis(community_cols, axis=1)\n",
    "        # print(X_train_scaled[community_cols].describe())\n",
    "\n",
    "        # Ensure 'Gender' is still binary (0 or 1)\n",
    "        if 'Gender' in X_train_scaled.columns:\n",
    "            X_train_scaled['Gender'] = (X_train_scaled['Gender'] > 0.5).astype(int)\n",
    "            X_test_scaled['Gender'] = (X_test_scaled['Gender'] > 0.5).astype(int)\n",
    "\n",
    "        # Append the processed fold to the list\n",
    "        kFolds_list.append((X_train_scaled, X_test_scaled, Y_train_cleaned, Y_test))\n",
    "\n",
    "        print(f\"Fold: {fold+1}, Train: {X_train_scaled.shape}, Test: {X_test_scaled.shape}\")\n",
    "\n",
    "    return kFolds_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "80faf996",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, criterion, optimiser, scheduler, train_loader, val_loader, epochs=20, patience=5, device=device, threshold = 0.5):\n",
    "    # if isinstance(model.last_layer(), nn.Sigmoid) and isinstance(criterion, nn.BCEWithLogitsLoss):\n",
    "    #     raise ValueError(\"Model output is Sigmoid but criterion is BCEWithLogitsLoss. Please check your model and criterion compatibility.\")\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    wait = 0\n",
    "    criterion.to(device) #? Move criterion to device\n",
    "    #* Epoch Training loop for this fold\n",
    "    for epoch in range(1,epochs+1):\n",
    "        #* Set model to training mode: essential for dropout and batch norm layers\n",
    "        model.train()\n",
    "        running_loss = 0.0 #? loss for this epoch\n",
    "        #* Mini-batch training loop\n",
    "        for batch, (inputs, labels) in enumerate(train_loader,start=1):\n",
    "            optimiser.zero_grad() #? Zero the gradients\n",
    "            assert not torch.isnan(inputs).any(), \"Input has NaNs\"\n",
    "            assert not torch.isinf(inputs).any(), \"Input has Infs\"\n",
    "            outputs = model(inputs) #? Forward pass through the model\n",
    "            assert not torch.isnan(outputs).any(), \"Model output has NaNs\"\n",
    "            assert not torch.isinf(outputs).any(), \"Model output has Infs\"\n",
    "            loss = criterion(outputs, labels) #? Calculate loss\n",
    "            assert not torch.isnan(loss).any(), \"Model loss has NaNs\"\n",
    "            loss.backward() #? Backpropagation\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            running_loss += loss.item()\n",
    "            optimiser.step() #? Update weights\n",
    "            scheduler.step()\n",
    "                \n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        # print(f\"Epoch: {epoch}, training loss: {train_loss:.4f}\")\n",
    "    \n",
    "        #* Now we evaluate the model on the validation set, to track training vs validation loss\n",
    "        model.eval() #? Set model to evaluation mode\n",
    "        with torch.no_grad(): #? No need to track gradients during evaluation\n",
    "            val_loss = 0.0    \n",
    "            for batch, (inputs, labels) in enumerate(val_loader,start=1):#! one pass because val_loader batch size is all, if you want to do it in mini-batches, you MUST change the metric calculations to accept mini-batches\n",
    "                outputs = model(inputs)\n",
    "                # labels = labels.cpu() \n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() #? Calculate loss\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "        # loss_ratio = val_loss / train_loss    \n",
    "        # pos_weight = loss_ratio  # or any other function of loss_ratio you choose\n",
    "    \n",
    "        # Update criterion with new pos_weight\n",
    "        # criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight]).to(device))\n",
    "        # Early stopping\n",
    "        if epoch > 30:\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                best_model_state = model.state_dict()\n",
    "                wait = 0\n",
    "            elif avg_val_loss*0.95 <= best_val_loss:\n",
    "                    wait = 0\n",
    "            else:\n",
    "                wait += 1\n",
    "        if wait >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch}, best val loss: {best_val_loss:.4f}\")\n",
    "            break\n",
    "        print(f\"Epoch: {epoch}\".ljust(12), f\"training loss:{train_loss:.3f}\".ljust(16), f\"best_val_loss:{best_val_loss:.3f}\".ljust(12), f\"Val Loss: {avg_val_loss:.3f}\", f\"Scheduler lr: {scheduler.get_last_lr()}\",end=\"\\r\")\n",
    "    #* Use best model to calculate metrics on the validation set\n",
    "    #! must be outside epoch loop, it comes after the training and cv loop\n",
    "    model.load_state_dict(best_model_state) #? Load the best model state\n",
    "    with torch.no_grad():\n",
    "        for batch, (inputs, labels) in enumerate(val_loader,start=1):#! one pass because val_loader batch size is all, if you want to do it in mini-batches, you MUST change the metric calculations to accept mini-batches\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                labels = labels.cpu() \n",
    "                # predictions = (torch.sigmoid(outputs) < 0.5).float().cpu().numpy()\n",
    "                predictions = (torch.sigmoid(outputs) >= threshold).float().cpu().numpy()\n",
    "                val_loss += loss.item() #? Calculate loss\n",
    "                \n",
    "    #! The following should have length equal to fold number           \n",
    "    accuracy=accuracy_score(labels, predictions) \n",
    "    precision=precision_score(labels, predictions, pos_label=1, zero_division=0)\n",
    "    recall=recall_score(labels, predictions, pos_label=1)\n",
    "    f1=f1_score(labels, predictions, pos_label=1)\n",
    "    auc=roc_auc_score(labels, predictions)\n",
    "    \n",
    "    return model, accuracy, precision, recall, f1, auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cfc26627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyModelWithSkip(\n",
      "  (block1): FeedForwardBlock_ELU(\n",
      "    (block): Sequential(\n",
      "      (0): Linear(in_features=20, out_features=64, bias=True)\n",
      "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "  )\n",
      "  (block2): FeedForwardBlock_TanH(\n",
      "    (block): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=32, bias=True)\n",
      "      (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (block3): FeedForwardBlock_ELU(\n",
      "    (block): Sequential(\n",
      "      (0): Linear(in_features=32, out_features=16, bias=True)\n",
      "      (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "  )\n",
      "  (output_layer): Linear(in_features=16, out_features=1, bias=True)\n",
      "  (skip13): Linear(in_features=64, out_features=16, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class FeedForwardBlock_ELU(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout=None, activation=nn.LeakyReLU):\n",
    "        super().__init__()\n",
    "        layers = [\n",
    "            nn.Linear(in_features, out_features),\n",
    "            nn.BatchNorm1d(out_features),\n",
    "            activation()\n",
    "        ]\n",
    "        if dropout and dropout > 0:\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "        self.block = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "    \n",
    "class FeedForwardBlock_TanH(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout=None, activation=nn.Tanh):\n",
    "        super().__init__()\n",
    "        layers = [\n",
    "            nn.Linear(in_features, out_features),\n",
    "            nn.BatchNorm1d(out_features),\n",
    "            activation()\n",
    "        ]\n",
    "        if dropout and dropout > 0:\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "        self.block = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "    \n",
    "class MyModelWithSkip(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, hidden_dim2, output_dim, hidden_dim3 = None, dropout=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.block1 = FeedForwardBlock_ELU(input_dim, hidden_dim, dropout=dropout)\n",
    "        self.block2 = FeedForwardBlock_TanH(hidden_dim, hidden_dim2, dropout=dropout / 2)\n",
    "        self.block3 = FeedForwardBlock_ELU(hidden_dim2, output_dim, dropout=dropout / 2)\n",
    "        self.output_layer = nn.Linear(output_dim, 1)\n",
    "\n",
    "        self.skip13 = nn.Identity() if hidden_dim == output_dim else nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.block1(x)\n",
    "        x2 = self.block2(x1) \n",
    "        x3 = self.block3(x2) + self.skip13(x1)\n",
    "        out = self.output_layer(x3)\n",
    "        return out\n",
    "test_model = MyModelWithSkip(\n",
    "    input_dim=20,\n",
    "    hidden_dim=64,\n",
    "    hidden_dim2=32,\n",
    "    output_dim=16,\n",
    "    dropout=0.0\n",
    ")\n",
    "print(test_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3ce2c9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximise_combined_score(trial):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "    epochs = 10000\n",
    "    random_state = 69\n",
    "\n",
    "    n_neighbours = trial.suggest_int(\"n_neighbours\", 7, 7)\n",
    "    oversampler = ADASYN(sampling_strategy='minority', n_neighbors=n_neighbours, random_state=random_state)\n",
    "    \n",
    "    normalisation_method = trial.suggest_categorical(\"normalisation_method\", [\"MinMaxScaler\",])\n",
    "    if normalisation_method:\n",
    "        if normalisation_method == \"MinMaxScaler\":\n",
    "            normalisation_method = MinMaxScaler()\n",
    "        else:\n",
    "            normalisation_method = MinMaxScaler()\n",
    "    contamination = trial.suggest_float(\"contamination\", 0.0, 0.0)\n",
    "    noise = trial.suggest_float(\"noise\", 0.0, 0.00)\n",
    "    kFolds = FOLDS_GENERATOR(X_FOR_FOLDS, Y_FOR_FOLDS, \n",
    "                         normalisation_method = normalisation_method, \n",
    "                         n_splits=5, \n",
    "                         oversampler = oversampler, random_state=random_state, contamination=contamination, noise = noise)\n",
    "                        \n",
    "    # Model hyperparameters (first-level optimization)\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 64, 64, step = 2)\n",
    "    hidden_dim2 = trial.suggest_int(\"hidden_dim2\", 128, 128, step = 2)\n",
    "    # hidden_dim3 = trial.suggest_int(\"hidden_dim2\", 64, 64, step = 2)\n",
    "    output_dim = trial.suggest_int(\"output_dim\", 32, 32, step = 2)\n",
    "    \n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0, 0.0)\n",
    "    threshold = trial.suggest_float(\"threshold\", 0.5, 0.5)\n",
    "    # dropout = None\n",
    "    initial_lr = trial.suggest_float(\"initial_lr\", 1e-4, 1e-4, log=True)\n",
    "    max_lr = trial.suggest_float(\"max_lr\", 1e-3, 1e-3, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3, log=True)\n",
    "    \n",
    "    # Loss function hyperparameters\n",
    "    # criterion_choice = trial.suggest_categorical(\"criterion\", [\"FocalLoss\"]) \n",
    "    criterion_choice = trial.suggest_categorical(\"criterion\", [\"BCEWithLogitsLoss\"]) \n",
    "    \n",
    "    # Hyperparameter exploration optimization\n",
    "    if criterion_choice == \"BCEWithLogitsLoss\":\n",
    "        pos_weight = trial.suggest_int(\"pos_weight\", 1, 1)\n",
    "        alpha = None\n",
    "        gamma = None\n",
    "    elif criterion_choice == \"FocalLoss\":\n",
    "        pos_weight= None\n",
    "        alpha = trial.suggest_float(\"alpha\", .75, .75)\n",
    "        gamma = trial.suggest_float(\"gamma\", 2.5, 2.5)\n",
    "    else:\n",
    "        pos_weight = None\n",
    "    \n",
    "    # Initialize lists for metrics across folds\n",
    "    accuracy_list = []\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    f1_list = []\n",
    "    auc_list = []\n",
    "\n",
    "    # Cross-validation loop\n",
    "    for fold, (train_x, test_x, train_y, test_y) in enumerate(kFolds, start=1):\n",
    "        print(f\"Fold {fold}:\")\n",
    "        # Create DataLoader for current fold\n",
    "        train_loader, val_loader = fold_to_dataloader_tensor(train_x, test_x, train_y, test_y, batch_size=128, device=device)\n",
    "        # Instantiate and initialize the model\n",
    "        model = MyModelWithSkip(input_dim=get_feature_count(train_loader), hidden_dim=hidden_dim, hidden_dim2 = hidden_dim2, output_dim=output_dim, dropout=dropout)\n",
    "        model.to(device)\n",
    "        model.apply(init_weights)\n",
    "        \n",
    "        # Map the choice to the actual loss function\n",
    "        criterion = criterion_mapping(criterion_choice, pos_weight, alpha, gamma)\n",
    "        optimiser = optim.Adam(model.parameters(), lr=initial_lr, weight_decay=weight_decay)\n",
    "        scheduler = torch.optim.lr_scheduler.CyclicLR(\n",
    "            optimiser, \n",
    "            base_lr=1e-5, \n",
    "            max_lr=max_lr,\n",
    "            cycle_momentum=False)\n",
    "        \n",
    "        # Train and evaluate the model on the current fold\n",
    "        model, accuracy, precision, recall, f1, auc = train_and_evaluate(\n",
    "            model, criterion, optimiser, scheduler, train_loader, val_loader, epochs=epochs, patience=1000, device=device, threshold = threshold\n",
    "        )\n",
    "        print(f\"Accuracy: {accuracy:.4f}, precision: {precision:.4f}, recall: {recall:.4f}, f1: {f1:.4f}, auc: {auc:.4f}\")\n",
    "        del model\n",
    "        del train_loader\n",
    "        del val_loader\n",
    "    \n",
    "        # Append the metrics from the current fold\n",
    "        accuracy_list.append(accuracy)\n",
    "        precision_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "        f1_list.append(f1)\n",
    "        auc_list.append(auc)\n",
    "        break\n",
    "    # Calculate the average metrics across all folds\n",
    "    avg_accuracy = np.sum(accuracy_list) / len(accuracy_list)\n",
    "    avg_precision = np.sum(precision_list) / len(precision_list)\n",
    "    avg_recall = np.sum(recall_list) / len(recall_list)\n",
    "    avg_f1 = np.sum(f1_list) / len(f1_list)\n",
    "    avg_auc = np.sum(auc_list) / len(auc_list)\n",
    "\n",
    "    # Combine metrics into a single \"score\"\n",
    "    # combined_score = (avg_f1 + avg_precision + avg_recall + avg_accuracy + avg_auc) / 5\n",
    "    combined_score = avg_f1\n",
    "\n",
    "    return combined_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5187dc94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bottle v0.13.2 server starting up (using WSGIRefServer())...\n",
      "Listening on http://localhost:8080/\n",
      "Hit Ctrl-C to quit.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1, Train: (8217, 28), Test: (1149, 28)\n",
      "Fold: 2, Train: (8223, 28), Test: (1149, 28)\n",
      "Fold: 3, Train: (8274, 28), Test: (1148, 28)\n",
      "Fold: 4, Train: (8270, 28), Test: (1148, 28)\n",
      "Fold: 5, Train: (8273, 28), Test: (1148, 28)\n",
      "Fold 1:\n",
      "Epoch: 1032  training loss:0.010 best_val_loss:0.502 Val Loss: 1.942 Scheduler lr: [0.0009762399999999991]]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-16 14:00:33,101] Trial 7 finished with value: 0.2018348623853211 and parameters: {'n_neighbours': 7, 'normalisation_method': 'MinMaxScaler', 'contamination': 0.0, 'noise': 0.0, 'hidden_dim': 64, 'hidden_dim2': 128, 'output_dim': 32, 'dropout': 0.0, 'threshold': 0.5, 'initial_lr': 0.0001, 'max_lr': 0.001, 'weight_decay': 1.0654960859816215e-05, 'criterion': 'BCEWithLogitsLoss', 'pos_weight': 1}. Best is trial 1 with value: 0.250996015936255.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 1033, best val loss: 0.5024\n",
      "Accuracy: 0.8486, precision: 0.2157, recall: 0.1897, f1: 0.2018, auc: 0.5561\n",
      "Using device: cuda\n",
      "Fold: 1, Train: (8217, 28), Test: (1149, 28)\n",
      "Fold: 2, Train: (8223, 28), Test: (1149, 28)\n",
      "Fold: 3, Train: (8274, 28), Test: (1148, 28)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-04-16 14:00:35,581] Trial 8 failed with parameters: {'n_neighbours': 7, 'normalisation_method': 'MinMaxScaler', 'contamination': 0.0, 'noise': 0.0} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tanle\\AppData\\Local\\Temp\\ipykernel_25080\\45341341.py\", line 18, in maximise_combined_score\n",
      "    kFolds = FOLDS_GENERATOR(X_FOR_FOLDS, Y_FOR_FOLDS,\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tanle\\AppData\\Local\\Temp\\ipykernel_25080\\1973545265.py\", line 40, in FOLDS_GENERATOR\n",
      "    X_train_scaled[community_cols] = X_train_scaled[community_cols].apply(\n",
      "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py\", line 10374, in apply\n",
      "    return op.apply().__finalize__(self, method=\"apply\")\n",
      "           ^^^^^^^^^^\n",
      "  File \"d:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py\", line 916, in apply\n",
      "    return self.apply_standard()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py\", line 1068, in apply_standard\n",
      "    return self.wrap_results(results, res_index)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py\", line 1107, in wrap_results\n",
      "    return self.wrap_results_for_axis(results, res_index)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py\", line 1354, in wrap_results_for_axis\n",
      "    result = self.infer_to_same_shape(results, res_index)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py\", line 1360, in infer_to_same_shape\n",
      "    result = self.obj._constructor(data=results)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py\", line 778, in __init__\n",
      "    mgr = dict_to_mgr(data, index, columns, dtype=dtype, copy=copy, typ=manager)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\construction.py\", line 503, in dict_to_mgr\n",
      "    return arrays_to_mgr(arrays, columns, index, dtype=dtype, typ=typ, consolidate=copy)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\construction.py\", line 119, in arrays_to_mgr\n",
      "    arrays, refs = _homogenize(arrays, index, dtype)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\construction.py\", line 611, in _homogenize\n",
      "    val = val.reindex(index, copy=False)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\pandas\\core\\series.py\", line 5153, in reindex\n",
      "    return super().reindex(\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py\", line 5603, in reindex\n",
      "    return self.copy(deep=copy)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py\", line 6813, in copy\n",
      "    return self._constructor_from_mgr(data, axes=data.axes).__finalize__(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\pandas\\core\\series.py\", line 665, in _constructor_from_mgr\n",
      "    ser = Series._from_mgr(mgr, axes=axes)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py\", line 357, in _from_mgr\n",
      "    NDFrame.__init__(obj, mgr)\n",
      "  File \"d:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py\", line 283, in __init__\n",
      "    object.__setattr__(self, \"_flags\", Flags(self, allows_duplicate_labels=True))\n",
      "                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2025-04-16 14:00:35,650] Trial 8 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_25080\\1066112411.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     17\u001b[39m dashboard_thread = threading.Thread(target=\u001b[38;5;28;01mlambda\u001b[39;00m: run_server(storage), daemon=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     18\u001b[39m dashboard_thread.start()\n\u001b[32m     19\u001b[39m \n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Run optimization\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m study.optimize(maximise_combined_score, n_trials=\u001b[32m100\u001b[39m)\n\u001b[32m     22\u001b[39m \n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Print results\u001b[39;00m\n\u001b[32m     24\u001b[39m print(\u001b[33m\"Best trial:\"\u001b[39m)\n",
      "\u001b[32md:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\optuna\\study\\study.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    471\u001b[39m         Raises:\n\u001b[32m    472\u001b[39m             RuntimeError:\n\u001b[32m    473\u001b[39m                 If nested invocation of this method occurs.\n\u001b[32m    474\u001b[39m         \"\"\"\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m         _optimize(\n\u001b[32m    476\u001b[39m             study=self,\n\u001b[32m    477\u001b[39m             func=func,\n\u001b[32m    478\u001b[39m             n_trials=n_trials,\n",
      "\u001b[32md:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    115\u001b[39m                         )\n\u001b[32m    116\u001b[39m                     )\n\u001b[32m    117\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    118\u001b[39m         study._thread_local.in_optimize_loop = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m         progress_bar.close()\n",
      "\u001b[32md:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    162\u001b[39m             \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    163\u001b[39m             \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    164\u001b[39m             \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    165\u001b[39m             \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n\u001b[32m    167\u001b[39m                 gc.collect()\n\u001b[32m    168\u001b[39m \n\u001b[32m    169\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m callbacks \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32md:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    244\u001b[39m         frozen_trial.state == TrialState.FAIL\n\u001b[32m    245\u001b[39m         \u001b[38;5;28;01mand\u001b[39;00m func_err \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    246\u001b[39m         \u001b[38;5;28;01mand\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m isinstance(func_err, catch)\n\u001b[32m    247\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m248\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "\u001b[32md:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    244\u001b[39m         frozen_trial.state == TrialState.FAIL\n\u001b[32m    245\u001b[39m         \u001b[38;5;28;01mand\u001b[39;00m func_err \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    246\u001b[39m         \u001b[38;5;28;01mand\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m isinstance(func_err, catch)\n\u001b[32m    247\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m248\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_25080\\45341341.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m     14\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     15\u001b[39m             normalisation_method = MinMaxScaler()\n\u001b[32m     16\u001b[39m     contamination = trial.suggest_float(\u001b[33m\"contamination\"\u001b[39m, \u001b[32m0.0\u001b[39m, \u001b[32m0.0\u001b[39m)\n\u001b[32m     17\u001b[39m     noise = trial.suggest_float(\u001b[33m\"noise\"\u001b[39m, \u001b[32m0.0\u001b[39m, \u001b[32m0.00\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     kFolds = FOLDS_GENERATOR(X_FOR_FOLDS, Y_FOR_FOLDS, \n\u001b[32m     19\u001b[39m                          normalisation_method = normalisation_method,\n\u001b[32m     20\u001b[39m                          n_splits=\u001b[32m5\u001b[39m,\n\u001b[32m     21\u001b[39m                          oversampler = oversampler, random_state=random_state, contamination=contamination, noise = noise)\n",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_25080\\1973545265.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(X, Y, normalisation_method, n_splits, random_state, oversampler, contamination, noise)\u001b[39m\n\u001b[32m     36\u001b[39m \n\u001b[32m     37\u001b[39m         \u001b[38;5;66;03m# Handle community columns\u001b[39;00m\n\u001b[32m     38\u001b[39m         community_cols = [col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;28;01min\u001b[39;00m X_train_scaled.columns \u001b[38;5;28;01mif\u001b[39;00m col.startswith(\u001b[33m'Community'\u001b[39m)]\n\u001b[32m     39\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m community_cols:\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m             X_train_scaled[community_cols] = X_train_scaled[community_cols].apply(\n\u001b[32m     41\u001b[39m                 \u001b[38;5;28;01mlambda\u001b[39;00m row: pd.Series(np.eye(len(row))[row.argmax()]), axis=\u001b[32m1\u001b[39m\n\u001b[32m     42\u001b[39m             ).set_axis(community_cols, axis=1)\n\u001b[32m     43\u001b[39m         \u001b[38;5;66;03m# print(X_train_scaled[community_cols].describe())\u001b[39;00m\n",
      "\u001b[32md:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[39m\n\u001b[32m  10370\u001b[39m             engine_kwargs=engine_kwargs,\n\u001b[32m  10371\u001b[39m             args=args,\n\u001b[32m  10372\u001b[39m             kwargs=kwargs,\n\u001b[32m  10373\u001b[39m         )\n\u001b[32m> \u001b[39m\u001b[32m10374\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m op.apply().__finalize__(self, method=\u001b[33m\"apply\"\u001b[39m)\n",
      "\u001b[32md:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    912\u001b[39m         \u001b[38;5;66;03m# raw\u001b[39;00m\n\u001b[32m    913\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m self.raw:\n\u001b[32m    914\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self.apply_raw(engine=self.engine, engine_kwargs=self.engine_kwargs)\n\u001b[32m    915\u001b[39m \n\u001b[32m--> \u001b[39m\u001b[32m916\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m self.apply_standard()\n",
      "\u001b[32md:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1064\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1065\u001b[39m             results, res_index = self.apply_series_numba()\n\u001b[32m   1066\u001b[39m \n\u001b[32m   1067\u001b[39m         \u001b[38;5;66;03m# wrap results\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1068\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m self.wrap_results(results, res_index)\n",
      "\u001b[32md:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, results, res_index)\u001b[39m\n\u001b[32m   1103\u001b[39m         \u001b[38;5;28;01mfrom\u001b[39;00m pandas \u001b[38;5;28;01mimport\u001b[39;00m Series\n\u001b[32m   1104\u001b[39m \n\u001b[32m   1105\u001b[39m         \u001b[38;5;66;03m# see if we can infer the results\u001b[39;00m\n\u001b[32m   1106\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m len(results) > \u001b[32m0\u001b[39m \u001b[38;5;28;01mand\u001b[39;00m \u001b[32m0\u001b[39m \u001b[38;5;28;01min\u001b[39;00m results \u001b[38;5;28;01mand\u001b[39;00m is_sequence(results[\u001b[32m0\u001b[39m]):\n\u001b[32m-> \u001b[39m\u001b[32m1107\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self.wrap_results_for_axis(results, res_index)\n\u001b[32m   1108\u001b[39m \n\u001b[32m   1109\u001b[39m         \u001b[38;5;66;03m# dict of scalars\u001b[39;00m\n\u001b[32m   1110\u001b[39m \n",
      "\u001b[32md:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, results, res_index)\u001b[39m\n\u001b[32m   1350\u001b[39m             result.index = res_index\n\u001b[32m   1351\u001b[39m \n\u001b[32m   1352\u001b[39m         \u001b[38;5;66;03m# we may want to infer results\u001b[39;00m\n\u001b[32m   1353\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1354\u001b[39m             result = self.infer_to_same_shape(results, res_index)\n\u001b[32m   1355\u001b[39m \n\u001b[32m   1356\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[32md:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, results, res_index)\u001b[39m\n\u001b[32m   1358\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m infer_to_same_shape(self, results: ResType, res_index: Index) -> DataFrame:\n\u001b[32m   1359\u001b[39m         \u001b[33m\"\"\"infer the results to the same shape as the input object\"\"\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1360\u001b[39m         result = self.obj._constructor(data=results)\n\u001b[32m   1361\u001b[39m         result = result.T\n\u001b[32m   1362\u001b[39m \n\u001b[32m   1363\u001b[39m         \u001b[38;5;66;03m# set the index\u001b[39;00m\n",
      "\u001b[32md:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, data, index, columns, dtype, copy)\u001b[39m\n\u001b[32m    774\u001b[39m             )\n\u001b[32m    775\u001b[39m \n\u001b[32m    776\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m isinstance(data, dict):\n\u001b[32m    777\u001b[39m             \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m778\u001b[39m             mgr = dict_to_mgr(data, index, columns, dtype=dtype, copy=copy, typ=manager)\n\u001b[32m    779\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m isinstance(data, ma.MaskedArray):\n\u001b[32m    780\u001b[39m             \u001b[38;5;28;01mfrom\u001b[39;00m numpy.ma \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n\u001b[32m    781\u001b[39m \n",
      "\u001b[32md:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(data, index, columns, dtype, typ, copy)\u001b[39m\n\u001b[32m    499\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    500\u001b[39m             \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[32m    501\u001b[39m             arrays = [x.copy() \u001b[38;5;28;01mif\u001b[39;00m hasattr(x, \u001b[33m\"dtype\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;28;01min\u001b[39;00m arrays]\n\u001b[32m    502\u001b[39m \n\u001b[32m--> \u001b[39m\u001b[32m503\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arrays_to_mgr(arrays, columns, index, dtype=dtype, typ=typ, consolidate=copy)\n",
      "\u001b[32md:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[39m\n\u001b[32m    115\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    116\u001b[39m             index = ensure_index(index)\n\u001b[32m    117\u001b[39m \n\u001b[32m    118\u001b[39m         \u001b[38;5;66;03m# don't force copy because getting jammed in an ndarray anyway\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m         arrays, refs = _homogenize(arrays, index, dtype)\n\u001b[32m    120\u001b[39m         \u001b[38;5;66;03m# _homogenize ensures\u001b[39;00m\n\u001b[32m    121\u001b[39m         \u001b[38;5;66;03m#  - all(len(x) == len(index) for x in arrays)\u001b[39;00m\n\u001b[32m    122\u001b[39m         \u001b[38;5;66;03m#  - all(x.ndim == 1 for x in arrays)\u001b[39;00m\n",
      "\u001b[32md:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(data, index, dtype)\u001b[39m\n\u001b[32m    607\u001b[39m                 val = val.astype(dtype, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    608\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m isinstance(val, ABCSeries) \u001b[38;5;28;01mand\u001b[39;00m val.index \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m index:\n\u001b[32m    609\u001b[39m                 \u001b[38;5;66;03m# Forces alignment. No need to copy data since we\u001b[39;00m\n\u001b[32m    610\u001b[39m                 \u001b[38;5;66;03m# are putting it into an ndarray later\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m611\u001b[39m                 val = val.reindex(index, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    612\u001b[39m             refs.append(val._references)\n\u001b[32m    613\u001b[39m             val = val._values\n\u001b[32m    614\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32md:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\pandas\\core\\series.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, index, axis, method, copy, level, fill_value, limit, tolerance)\u001b[39m\n\u001b[32m   5149\u001b[39m         fill_value: Scalar | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   5150\u001b[39m         limit: int | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   5151\u001b[39m         tolerance=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   5152\u001b[39m     ) -> Series:\n\u001b[32m-> \u001b[39m\u001b[32m5153\u001b[39m         return super().reindex(\n\u001b[32m   5154\u001b[39m             index=index,\n\u001b[32m   5155\u001b[39m             method=method,\n\u001b[32m   5156\u001b[39m             copy=copy,\n",
      "\u001b[32md:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, labels, index, columns, axis, method, copy, level, fill_value, limit, tolerance)\u001b[39m\n\u001b[32m   5599\u001b[39m             self._get_axis(axis_name).identical(ax)\n\u001b[32m   5600\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m axis_name, ax \u001b[38;5;28;01min\u001b[39;00m axes.items()\n\u001b[32m   5601\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m ax \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   5602\u001b[39m         ):\n\u001b[32m-> \u001b[39m\u001b[32m5603\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self.copy(deep=copy)\n\u001b[32m   5604\u001b[39m \n\u001b[32m   5605\u001b[39m         \u001b[38;5;66;03m# check if we are a multi reindex\u001b[39;00m\n\u001b[32m   5606\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m self._needs_reindex_multi(axes, method, level):\n",
      "\u001b[32md:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, deep)\u001b[39m\n\u001b[32m   6809\u001b[39m         dtype: int64\n\u001b[32m   6810\u001b[39m         \"\"\"\n\u001b[32m   6811\u001b[39m         data = self._mgr.copy(deep=deep)\n\u001b[32m   6812\u001b[39m         self._clear_item_cache()\n\u001b[32m-> \u001b[39m\u001b[32m6813\u001b[39m         return self._constructor_from_mgr(data, axes=data.axes).__finalize__(\n\u001b[32m   6814\u001b[39m             self, method=\u001b[33m\"copy\"\u001b[39m\n\u001b[32m   6815\u001b[39m         )\n",
      "\u001b[32md:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\pandas\\core\\series.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, mgr, axes)\u001b[39m\n\u001b[32m    664\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m _constructor_from_mgr(self, mgr, axes):\n\u001b[32m--> \u001b[39m\u001b[32m665\u001b[39m         ser = Series._from_mgr(mgr, axes=axes)\n\u001b[32m    666\u001b[39m         ser._name = \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# caller is responsible for setting real name\u001b[39;00m\n\u001b[32m    667\u001b[39m \n\u001b[32m    668\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m type(self) \u001b[38;5;28;01mis\u001b[39;00m Series:\n",
      "\u001b[32md:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(cls, mgr, axes)\u001b[39m\n\u001b[32m    353\u001b[39m         The axes must match mgr.axes, but are required \u001b[38;5;28;01mfor\u001b[39;00m future-proofing\n\u001b[32m    354\u001b[39m         \u001b[38;5;28;01min\u001b[39;00m the event that axes are refactored out of the Manager objects.\n\u001b[32m    355\u001b[39m         \"\"\"\n\u001b[32m    356\u001b[39m         obj = cls.__new__(cls)\n\u001b[32m--> \u001b[39m\u001b[32m357\u001b[39m         NDFrame.__init__(obj, mgr)\n\u001b[32m    358\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "\u001b[32md:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m    279\u001b[39m         object.__setattr__(self, \u001b[33m\"_is_copy\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    280\u001b[39m         object.__setattr__(self, \u001b[33m\"_mgr\"\u001b[39m, data)\n\u001b[32m    281\u001b[39m         object.__setattr__(self, \u001b[33m\"_item_cache\"\u001b[39m, {})\n\u001b[32m    282\u001b[39m         object.__setattr__(self, \u001b[33m\"_attrs\"\u001b[39m, {})\n\u001b[32m--> \u001b[39m\u001b[32m283\u001b[39m         object.__setattr__(self, \u001b[33m\"_flags\"\u001b[39m, Flags(self, allows_duplicate_labels=\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import optuna\n",
    "from optuna_dashboard import run_server\n",
    "# !fuser -k 8080/tcp\n",
    "\n",
    "# Define your persistent storage\n",
    "storage = \"sqlite:///opt6 - 4.db\"\n",
    "\n",
    "# Create or load your study\n",
    "study_name = \"optuna6 - 4\"\n",
    "try:\n",
    "    study = optuna.load_study(study_name=study_name, storage=storage)\n",
    "except KeyError:\n",
    "    study = optuna.create_study(study_name=study_name, direction=\"maximize\", storage=storage)\n",
    "\n",
    "# Start Optuna Dashboard in a separate thread\n",
    "dashboard_thread = threading.Thread(target=lambda: run_server(storage), daemon=True)\n",
    "dashboard_thread.start()\n",
    "\n",
    "# Run optimization\n",
    "study.optimize(maximise_combined_score, n_trials=100)\n",
    "\n",
    "# Print results\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(f\"  Combined score: {trial.value}\")\n",
    "print(\"  Best hyperparameters:\")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d65396",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
