{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import (\n",
    "    MaxAbsScaler,\n",
    "    MinMaxScaler,\n",
    "    Normalizer,\n",
    "    PowerTransformer,\n",
    "    QuantileTransformer,\n",
    "    RobustScaler,\n",
    "    StandardScaler,\n",
    "    minmax_scale,\n",
    ")\n",
    "from sklearn.metrics import recall_score, accuracy_score,f1_score, precision_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import optuna\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install optuna-dashboard\n",
    "# optuna-dashboard sqlite:///db.sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "randomState = 42\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "raw_dataset = pd.read_csv(\"./data/processed_data.csv\") #data has X and Y\n",
    "X = raw_dataset.drop(columns=[\"DR\"])\n",
    "Y = pd.DataFrame(raw_dataset[\"DR\"])\n",
    "\n",
    "#* 90/10 split for training and final test\n",
    "X_FOR_FOLDS, X_FINAL_TEST, Y_FOR_FOLDS, Y_FINAL_TEST = train_test_split(X, Y, test_size=0.1, random_state=randomState, stratify=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FOLDS_GENERATOR(X, Y, normalisation_method=MinMaxScaler(), n_splits=5, randomState=None, oversample=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    Generates stratified folds with specified normalization.\n",
    "    \n",
    "    For list of scalers, see:\n",
    "    https://scikit-learn.org/stable/api/sklearn.preprocessing.html\n",
    "    \n",
    "    For more details on scaling and normalization effects, see:\n",
    "    https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#\n",
    "    \n",
    "    normalisation_method should be an instance of a scaler, e.g.,\n",
    "    - MinMaxScaler()\n",
    "    - MaxAbsScaler()\n",
    "    - Quantile_Transform(output_distribution='uniform')\n",
    "    \n",
    "    Returns a list of tuples, each containing:\n",
    "    (X_train_scaled, X_test_scaled, Y_train, Y_test), representing data for each fold\n",
    "    \"\"\"\n",
    "    kF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=randomState)\n",
    "    kFolds_list = []\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(kF.split(X, Y)):\n",
    "        # Split the data into training and testing sets for this fold\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        Y_train, Y_test = Y.iloc[train_idx], Y.iloc[test_idx]\n",
    "        \n",
    "        # Fit the scaler on the training data and transform both train and test sets\n",
    "        X_train_scaled = normalisation_method.fit_transform(X_train)\n",
    "        X_test_scaled = normalisation_method.transform(X_test)\n",
    "        \n",
    "        if oversample:\n",
    "            # Oversample the training data if needed (e.g., using SMOTE or similar techniques)\n",
    "            # This is a placeholder; actual oversampling code should be implemented here\n",
    "            # X_train_scaled....\n",
    "            pass\n",
    "        \n",
    "        # Convert back to DataFrame to maintain column names\n",
    "        X_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns, index=X_train.index)\n",
    "        X_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns, index=X_test.index)\n",
    "        \n",
    "        # Ensure 'gender' is still binary (0 or 1)\n",
    "        if X_train_scaled['Gender'].isin([0, 1]).all():\n",
    "            kFolds_list.append((X_train_scaled, X_test_scaled, Y_train, Y_test))\n",
    "        else:\n",
    "            print(\"Warning: 'gender' column contains unexpected values after scaling.\") \n",
    "               \n",
    "        print(f\"Fold: {fold+1}, Train: {kFolds_list[fold][0].shape}, Test: {kFolds_list[fold][1].shape}\")   \n",
    "    return kFolds_list\n",
    "\n",
    "def init_weights(model): #tested already\n",
    "    if isinstance(model, nn.Linear):  # Apply only to linear layers\n",
    "        nn.init.xavier_uniform_(model.weight)\n",
    "        if model.bias is not None:\n",
    "            nn.init.zeros_(model.bias)\n",
    "            \n",
    "def fold_to_dataloader_tensor(train_x, test_x, train_y, test_y, batchSize=64, device=device):\n",
    "    train_dataset = TensorDataset(\n",
    "        torch.tensor(train_x.values,dtype=torch.float32).to(device), \n",
    "        torch.tensor(train_y.values,dtype=torch.float32).to(device))\n",
    "    val_dataset = TensorDataset(\n",
    "        torch.tensor(test_x.values,dtype=torch.float32).to(device), \n",
    "        torch.tensor(test_y.values,dtype=torch.float32).to(device))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batchSize, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=False)\n",
    "    return train_loader, val_loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1, Train: (4593, 28), Test: (1149, 28)\n",
      "Fold: 2, Train: (4593, 28), Test: (1149, 28)\n",
      "Fold: 3, Train: (4594, 28), Test: (1148, 28)\n",
      "Fold: 4, Train: (4594, 28), Test: (1148, 28)\n",
      "Fold: 5, Train: (4594, 28), Test: (1148, 28)\n"
     ]
    }
   ],
   "source": [
    "kFolds = FOLDS_GENERATOR(X_FOR_FOLDS, Y_FOR_FOLDS, normalisation_method=MinMaxScaler(), n_splits=5, randomState=randomState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Criterion_Models import *\n",
    "def criterion_mapping(criterion_choice:str, pos_weight:float=None):\n",
    "    \"\"\"\n",
    "    Feel free to add any custom loss functions here.\n",
    "    returns function for criterion\n",
    "    \"\"\"\n",
    "    if criterion_choice == \"FocalLoss\":\n",
    "        return FocalLoss()\n",
    "    elif criterion_choice == \"DiceLoss\":\n",
    "        return DiceLoss()\n",
    "    elif criterion_choice == \"BCEWithLogitsLoss\":\n",
    "        return nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight])) if pos_weight else nn.BCEWithLogitsLoss()\n",
    "    return nn.BCEWithLogitsLoss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, criterion, optimiser, scheduler, kFolds, epochs, batchsize, patience=5, device=device):\n",
    "    if isinstance(model[-1], nn.Sigmoid) and isinstance(criterion, nn.BCEWithLogitsLoss):\n",
    "        raise ValueError(\"Model output is Sigmoid but criterion is BCEWithLogitsLoss. Please check your model and criterion compatibility.\")\n",
    "\n",
    "    \n",
    "    accuracy_list = []\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    f1_list = []\n",
    "    auc_list = []\n",
    "    \n",
    "    for fold, (train_x, test_x, train_y, test_y) in enumerate(kFolds, start=1):\n",
    "        best_val_loss = float('inf')\n",
    "        best_model_state = None\n",
    "        wait = 0\n",
    "        # print(f\"Fold: {fold}\")\n",
    "        #* Convert the fold to PyTorch tensors and create DataLoader objects\n",
    "        train_loader, val_loader = fold_to_dataloader_tensor(train_x, test_x, train_y, test_y, batchsize, device)\n",
    "\n",
    "        #* Set model to training mode: essential for dropout and batch norm layers\n",
    "        model.train()\n",
    "        #* Epoch Training loop for this fold\n",
    "        for epoch in range(1,epochs+1):\n",
    "            running_loss = 0.0 #? loss for this epoch\n",
    "            #* Mini-batch training loop\n",
    "            for batch, (inputs, labels) in enumerate(train_loader,start=1):\n",
    "                optimiser.zero_grad() #? Zero the gradients\n",
    "                outputs = model(inputs) #? Forward pass through the model\n",
    "                loss = criterion(outputs, labels) #? Calculate loss\n",
    "                loss.backward() #? Backpropagation\n",
    "                running_loss += loss.item()\n",
    "                optimiser.step() #? Update weights\n",
    "                if scheduler:\n",
    "                    scheduler.step()\n",
    "                   \n",
    "            train_loss = running_loss / len(train_loader)\n",
    "            print(f\"Epoch: {epoch}, training loss: {train_loss:.4f}\")\n",
    "        \n",
    "            #* Now we evaluate the model on the validation set, to track training vs validation loss\n",
    "            model.eval() #? Set model to evaluation mode\n",
    "            with torch.no_grad(): #? No need to track gradients during evaluation\n",
    "                val_loss = 0.0    \n",
    "                for batch, (inputs, labels) in enumerate(val_loader,start=1):#! one pass because val_loader batch size is all, if you want to do it in mini-batches, you MUST change the metric calculations to accept mini-batches\n",
    "                    outputs = model(inputs)\n",
    "                    if isinstance(model[-1], nn.Sigmoid):\n",
    "                        predictions = (outputs > 0.5).float().cpu() #? assume model output is 1s and 0s\n",
    "                    else: #? if model output is logits, convert to binary predictions\n",
    "                        predictions = (torch.sigmoid(outputs) > 0.5).float().cpu()\n",
    "                    labels = labels.cpu() \n",
    "                    loss = criterion(predictions, labels)\n",
    "                    val_loss += loss.item() #? Calculate loss\n",
    "                    avg_val_loss = val_loss / len(val_loader)\n",
    "                    print(f\"Epoch {epoch}, Val Loss: {avg_val_loss:.4f}\")\n",
    "            \n",
    "                    # Early stopping\n",
    "                    if avg_val_loss < best_val_loss:\n",
    "                        best_val_loss = avg_val_loss\n",
    "                        best_model_state = model.state_dict()\n",
    "                        wait = 0\n",
    "                    else:\n",
    "                        wait += 1\n",
    "                        if wait >= patience:\n",
    "                            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "                            break\n",
    "        \n",
    "        #* Use best model to calculate metrics on the validation set\n",
    "        #! must be outside epoch loop, it comes after the training and cv loop\n",
    "        model.load_state_dict(best_model_state) #? Load the best model state\n",
    "        with torch.nograd():\n",
    "            for batch, (inputs, labels) in enumerate(val_loader,start=1):#! one pass because val_loader batch size is all, if you want to do it in mini-batches, you MUST change the metric calculations to accept mini-batches\n",
    "                    outputs = model(inputs)\n",
    "                    if isinstance(model[-1], nn.Sigmoid):\n",
    "                        predictions = (outputs > 0.5).float().cpu() #? assume model output is 1s and 0s\n",
    "                    else: #? if model output is logits, convert to binary predictions\n",
    "                        predictions = (torch.sigmoid(outputs) > 0.5).float().cpu()\n",
    "                    labels = labels.cpu() \n",
    "                    loss = criterion(predictions, labels)\n",
    "                    val_loss += loss.item() #? Calculate loss\n",
    "                    \n",
    "        #! The following should have length equal to fold number           \n",
    "        accuracy_list.append(accuracy_score(labels, predictions)) \n",
    "        precision_list.append(precision_score(labels, predictions, pos_label=1)) \n",
    "        recall_list.append(recall_score(labels, predictions, pos_label=1))\n",
    "        f1_list.append(f1_score(labels, predictions, pos_label=1))\n",
    "        auc_list.append(roc_auc_score(labels, predictions)) \n",
    "\n",
    "    return model, accuracy_list, precision_list, recall_list, f1_list, auc_list \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    #* Model hyperparameters (first-level optimization)\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 16, 128)  #? Number of units in the hidden layer\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)     #? Dropout rate for regularization\n",
    "    initial_lr = trial.suggest_float(\"initial_lr\", 1e-5, 1e-3, log=True) #? Initial learning rate for the optimizer\n",
    "    max_lr = trial.suggest_loguniform(\"max_lr\", 1e-3, 1e-1)\n",
    "    #? Learning rate on a log scale\n",
    "    \n",
    "    #* Loss function hyperparameters (first-level optimization)\n",
    "    criterion_choice = trial.suggest_categorical(\"criterion\", [\"BCEWithLogitsLoss\", \"FocalLoss\", \"DiceLoss\"]) \n",
    "    #? See which loss functions yield the best results\n",
    "    \n",
    "    #* Hyperparameter exploration optimization(second-level optimization)\n",
    "    if isinstance(criterion_choice, nn.BCEWithLogitsLoss): \n",
    "        #? For BCEWithLogitsLoss: see which weight gives the best results\n",
    "        pos_weight = trial.suggest_int(\"pos_weight\", 1.0, 10.0) \n",
    "    else:\n",
    "        pos_weight = None\n",
    "    # if criterion_choice == ..\n",
    "        \n",
    "    #* Loading in the hyperparameters for training\n",
    "    model = BinaryClassifier(input_dim=20, hidden_dim=hidden_dim, dropout=dropout)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.apply(init_weights)\n",
    "    \n",
    "    #* Map the choice to the actual loss function\n",
    "    criterion = criterion_mapping(criterion_choice, pos_weight).to(device) #! always .to(device) this\n",
    "    optimiser = optim.Adam(model.parameters(), lr=max_lr)\n",
    "    \n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimiser,\n",
    "        max_lr=max_lr,\n",
    "        steps_per_epoch=len(train_loader),\n",
    "        epochs=epoch,\n",
    "        anneal_strategy='linear'\n",
    "    )\n",
    "    \n",
    "    model, accuracy_list, precision_list, recall_list, f1_list, auc_list = train_and_evaluate(model, criterion, optimiser, scheduler, kFolds, epochs=100, batchsize=64, patience=5, device=device)\n",
    "\n",
    "    # Calculate the average of each metric across all folds\n",
    "    avg_accuracy = sum(accuracy_list) / len(accuracy_list)\n",
    "    avg_precision = sum(precision_list) / len(precision_list)\n",
    "    avg_recall = sum(recall_list) / len(recall_list)\n",
    "    avg_f1 = sum(f1_list) / len(f1_list)\n",
    "    avg_auc = sum(auc_list) / len(auc_list)\n",
    "\n",
    "    # Combine metrics into a single \"score\" (simple average here, but feel free to adjust the weights)\n",
    "    combined_score = (avg_f1 + avg_precision + avg_recall + avg_accuracy + avg_auc) / 5  # or use weighted sum\n",
    "\n",
    "    return combined_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-05 23:54:21,625] A new study created in memory with name: no-name-1d89d88d-54b0-4843-bafb-c11002bb15c8\n",
      "C:\\Users\\tanle\\AppData\\Local\\Temp\\ipykernel_10640\\1161310067.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  max_lr = trial.suggest_loguniform(\"max_lr\", 1e-3, 1e-1)\n",
      "[W 2025-04-05 23:54:28,264] Trial 0 failed with parameters: {'hidden_dim': 65, 'dropout': 0.3917573210540152, 'initial_lr': 0.00020480352771089, 'max_lr': 0.09374634214340735, 'criterion': 'FocalLoss'} because of the following error: NameError(\"name 'train_loader' is not defined\").\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tanle\\AppData\\Local\\Temp\\ipykernel_10640\\1161310067.py\", line 34, in objective\n",
      "    steps_per_epoch=len(train_loader),\n",
      "                        ^^^^^^^^^^^^\n",
      "NameError: name 'train_loader' is not defined\n",
      "[W 2025-04-05 23:54:28,265] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m study = optuna.create_study(direction=\u001b[33m\"\u001b[39m\u001b[33mmaximize\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mstudy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# You can adjust the number of trials\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBest trial:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m trial = study.best_trial\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\optuna\\study\\study.py:475\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    373\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    374\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    375\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    382\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    383\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    384\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    385\u001b[39m \n\u001b[32m    386\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    473\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    474\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:63\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     76\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     frozen_trial = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    163\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:248\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    241\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    244\u001b[39m     frozen_trial.state == TrialState.FAIL\n\u001b[32m    245\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    246\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    247\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m248\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\GitHub repos\\ADL\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:197\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    196\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    198\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    199\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    200\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mobjective\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m     28\u001b[39m criterion = criterion_mapping(criterion_choice, pos_weight).to(device) \u001b[38;5;66;03m#! always .to(device) this\u001b[39;00m\n\u001b[32m     29\u001b[39m optimiser = optim.Adam(model.parameters(), lr=max_lr)\n\u001b[32m     31\u001b[39m scheduler = torch.optim.lr_scheduler.OneCycleLR(\n\u001b[32m     32\u001b[39m     optimiser,\n\u001b[32m     33\u001b[39m     max_lr=max_lr,\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m     steps_per_epoch=\u001b[38;5;28mlen\u001b[39m(\u001b[43mtrain_loader\u001b[49m),\n\u001b[32m     35\u001b[39m     epochs=epoch,\n\u001b[32m     36\u001b[39m     anneal_strategy=\u001b[33m'\u001b[39m\u001b[33mlinear\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     37\u001b[39m )\n\u001b[32m     39\u001b[39m model, accuracy_list, precision_list, recall_list, f1_list, auc_list = train_and_evaluate(model, criterion, optimiser, scheduler, kFolds, epochs=\u001b[32m100\u001b[39m, batchsize=\u001b[32m64\u001b[39m, patience=\u001b[32m5\u001b[39m, device=device)\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# Calculate the average of each metric across all folds\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=30)  # You can adjust the number of trials\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(f\"  F1 Score: {trial.value}\")\n",
    "print(\"  Best hyperparameters:\")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
