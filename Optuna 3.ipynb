{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9230c445-891b-41ec-83f2-7f3d6c14d0af",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (1.26.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: optuna in /home/jovyan/.local/lib/python3.11/site-packages (4.2.1)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /opt/conda/lib/python3.11/site-packages (from optuna) (1.12.1)\n",
      "Requirement already satisfied: colorlog in /home/jovyan/.local/lib/python3.11/site-packages (from optuna) (6.9.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from optuna) (1.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from optuna) (23.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /opt/conda/lib/python3.11/site-packages (from optuna) (2.0.23)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from optuna) (4.66.1)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.11/site-packages (from optuna) (6.0.1)\n",
      "Requirement already satisfied: Mako in /opt/conda/lib/python3.11/site-packages (from alembic>=1.5.0->optuna) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4 in /opt/conda/lib/python3.11/site-packages (from alembic>=1.5.0->optuna) (4.8.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.11/site-packages (from sqlalchemy>=1.4.2->optuna) (3.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /opt/conda/lib/python3.11/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n",
      "Requirement already satisfied: optuna-dashboard in /home/jovyan/.local/lib/python3.11/site-packages (0.18.0)\n",
      "Requirement already satisfied: bottle>=0.13.0 in /home/jovyan/.local/lib/python3.11/site-packages (from optuna-dashboard) (0.13.2)\n",
      "Requirement already satisfied: optuna>=3.1.0 in /home/jovyan/.local/lib/python3.11/site-packages (from optuna-dashboard) (4.2.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from optuna-dashboard) (23.2)\n",
      "Requirement already satisfied: scikit-learn in /home/jovyan/.local/lib/python3.11/site-packages (from optuna-dashboard) (1.6.1)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /opt/conda/lib/python3.11/site-packages (from optuna>=3.1.0->optuna-dashboard) (1.12.1)\n",
      "Requirement already satisfied: colorlog in /home/jovyan/.local/lib/python3.11/site-packages (from optuna>=3.1.0->optuna-dashboard) (6.9.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from optuna>=3.1.0->optuna-dashboard) (1.26.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /opt/conda/lib/python3.11/site-packages (from optuna>=3.1.0->optuna-dashboard) (2.0.23)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from optuna>=3.1.0->optuna-dashboard) (4.66.1)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.11/site-packages (from optuna>=3.1.0->optuna-dashboard) (6.0.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/jovyan/.local/lib/python3.11/site-packages (from scikit-learn->optuna-dashboard) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/jovyan/.local/lib/python3.11/site-packages (from scikit-learn->optuna-dashboard) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/jovyan/.local/lib/python3.11/site-packages (from scikit-learn->optuna-dashboard) (3.6.0)\n",
      "Requirement already satisfied: Mako in /opt/conda/lib/python3.11/site-packages (from alembic>=1.5.0->optuna>=3.1.0->optuna-dashboard) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4 in /opt/conda/lib/python3.11/site-packages (from alembic>=1.5.0->optuna>=3.1.0->optuna-dashboard) (4.8.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.11/site-packages (from sqlalchemy>=1.4.2->optuna>=3.1.0->optuna-dashboard) (3.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /opt/conda/lib/python3.11/site-packages (from Mako->alembic>=1.5.0->optuna>=3.1.0->optuna-dashboard) (2.1.3)\n",
      "ERROR: unknown command \"installl\" - maybe you meant \"install\"\n",
      "Requirement already satisfied: imbalanced-learn in /opt/conda/lib/python3.11/site-packages (0.13.0)\n",
      "Requirement already satisfied: numpy<3,>=1.24.3 in /opt/conda/lib/python3.11/site-packages (from imbalanced-learn) (1.26.0)\n",
      "Requirement already satisfied: scipy<2,>=1.10.1 in /home/jovyan/.local/lib/python3.11/site-packages (from imbalanced-learn) (1.15.2)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.3.2 in /home/jovyan/.local/lib/python3.11/site-packages (from imbalanced-learn) (1.6.1)\n",
      "Requirement already satisfied: sklearn-compat<1,>=0.1 in /opt/conda/lib/python3.11/site-packages (from imbalanced-learn) (0.1.3)\n",
      "Requirement already satisfied: joblib<2,>=1.1.1 in /home/jovyan/.local/lib/python3.11/site-packages (from imbalanced-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /home/jovyan/.local/lib/python3.11/site-packages (from imbalanced-learn) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install optuna\n",
    "!pip install optuna-dashboard\n",
    "!pip installl scikit\n",
    "!pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "255c0953",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import (\n",
    "    MaxAbsScaler,\n",
    "    MinMaxScaler,\n",
    "    Normalizer,\n",
    "    PowerTransformer,\n",
    "    QuantileTransformer,\n",
    "    RobustScaler,\n",
    "    StandardScaler,\n",
    "    minmax_scale,\n",
    ")\n",
    "from sklearn.metrics import recall_score, accuracy_score,f1_score, precision_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from imblearn.pipeline import Pipeline\n",
    "import warnings\n",
    "import optuna\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6cbd367d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "random_state = 42\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "raw_dataset = pd.read_csv(\"./data/processed_data.csv\") #data has X and Y\n",
    "X = raw_dataset.drop(columns=[\"DR\"])\n",
    "Y = pd.DataFrame(raw_dataset[\"DR\"])\n",
    "X.drop(columns = ['Age', 'Gender', 'UAlb', 'Ucr', 'UACR', 'LDLC', 'HDLC'], inplace=True)\n",
    "# [Age,Gender,UAlb,Ucr,UACR,TC,TG,TCTG,LDLC,HDLC,Scr,BUN,FPG,HbA1c,Height,Weight,BMI,Duration,DR,Community_baihe,Community_chonggu,Community_huaxin,Community_jinze,Community_liantang,Community_xianghuaqiao,Community_xujin,Community_yingpu,Community_zhaoxian,Community_zhujiajiao]\n",
    "#* 90/10 split for training and final test\n",
    "X_FOR_FOLDS, X_FINAL_TEST, Y_FOR_FOLDS, Y_FINAL_TEST = train_test_split(X, Y, test_size=0.1, random_state=random_state, stratify=Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81efd19",
   "metadata": {},
   "source": [
    "Helper functions that don't need tweakin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7e54029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from modularModels1 import BlockMaker, modularNN, BasicModel\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using\", device)\n",
    "\n",
    "def init_weights(model): #tested already\n",
    "    if isinstance(model, nn.Linear):  # Apply only to linear layers\n",
    "        nn.init.xavier_uniform_(model.weight)\n",
    "        if model.bias is not None:\n",
    "            nn.init.zeros_(model.bias)\n",
    "            \n",
    "def fold_to_dataloader_tensor(train_x, test_x, train_y, test_y, batch_size=64, device=device):\n",
    "    train_dataset = TensorDataset(\n",
    "        torch.tensor(train_x.values,dtype=torch.float32).to(device), \n",
    "        torch.tensor(train_y.values,dtype=torch.float32).to(device))\n",
    "    val_dataset = TensorDataset(\n",
    "        torch.tensor(test_x.values,dtype=torch.float32).to(device), \n",
    "        torch.tensor(test_y.values,dtype=torch.float32).to(device))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=False)\n",
    "    return train_loader, val_loader \n",
    "\n",
    "def get_feature_count(loader):\n",
    "    \"\"\"returns the number of features in the dataset\"\"\"\n",
    "    return next(iter(loader))[0].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a19d07a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Criterion_Models import *\n",
    "def criterion_mapping(criterion_choice:str, pos_weight:float=None, alpha:float=None, gamma:float=None):\n",
    "    \"\"\"\n",
    "    Feel free to add any custom loss functions here.\n",
    "    returns function for criterion\n",
    "    \"\"\"\n",
    "    if criterion_choice == \"FocalLoss\":\n",
    "        return FocalLoss(alpha =alpha, gamma=gamma)\n",
    "    elif criterion_choice == \"DiceLoss\":\n",
    "        return DiceLoss()\n",
    "    elif criterion_choice == \"BCEWithLogitsLoss\":\n",
    "        return nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight])) if pos_weight else nn.BCEWithLogitsLoss()\n",
    "    return nn.BCEWithLogitsLoss() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86721b4d",
   "metadata": {},
   "source": [
    "Helper functions that could use tweakin to improve model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8673bf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FOLDS_GENERATOR(X, Y, normalisation_method=MinMaxScaler(), n_splits=5, random_state=None, oversampler=None):\n",
    "    \"\"\"\n",
    "    Generates stratified folds with specified normalization.\n",
    "    \n",
    "    For list of scalers, see:\n",
    "    https://scikit-learn.org/stable/api/sklearn.preprocessing.html\n",
    "    \n",
    "    For more details on scaling and normalization effects, see:\n",
    "    https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#\n",
    "    \n",
    "    normalisation_method should be an instance of a scaler, e.g.,\n",
    "    - MinMaxScaler()\n",
    "    - MaxAbsScaler()\n",
    "    - Quantile_Transform(output_distribution='uniform')\n",
    "    \n",
    "    Returns a list of tuples, each containing:\n",
    "    (X_train_scaled, X_test_scaled, Y_train, Y_test), representing data for each fold\n",
    "    \"\"\"\n",
    "    kF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    kFolds_list = []\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(kF.split(X, Y)):\n",
    "        # Split the data into training and testing sets for this fold\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        Y_train, Y_test = Y.iloc[train_idx], Y.iloc[test_idx]\n",
    "        \n",
    "        # IsolationForest for outlier removal (optional)\n",
    "        iso_forest = IsolationForest(contamination=0.05, random_state=random_state, warm_start = True)\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\", UserWarning)\n",
    "            outliers = iso_forest.fit_predict(X_train)    \n",
    "        X_train = X_train[outliers == 1]\n",
    "        Y_train = Y_train[outliers == 1]\n",
    "        \n",
    "        # Scale the entire data (binary and continuous together)\n",
    "        X_train_scaled = normalisation_method.fit_transform(X_train)\n",
    "        X_test_scaled = normalisation_method.transform(X_test)\n",
    "\n",
    "        # Handle oversampling if needed\n",
    "        if oversampler:\n",
    "            # Apply oversampling to both features and target\n",
    "            X_train_scaled, Y_train = oversampler.fit_resample(X_train_scaled, Y_train)\n",
    "\n",
    "        # Convert scaled data back to DataFrame with the correct column names\n",
    "        X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "        X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "        \n",
    "        # Handle community columns\n",
    "        community_cols = [col for col in X_train_scaled.columns if col.startswith('Community')] \n",
    "        if community_cols:\n",
    "        # Check for rows where multiple communities are flagged\n",
    "            for idx, row in X_train_scaled[community_cols].iterrows():\n",
    "                if set(np.unique(row)) != {0, 1}:  # If the unique values aren't just 0 or 1\n",
    "                    # Fix row by ensuring only one community is marked\n",
    "                    X_train_scaled.loc[idx, community_cols] = 0  # Set all community columns to 0\n",
    "                    max_col = row.idxmax()  # Find the column with the maximum value\n",
    "                    X_train_scaled.at[idx, max_col] = 1  # Set the column with the max value to 1\n",
    "\n",
    "        # Ensure 'gender' is still binary (0 or 1)\n",
    "        if 'Gender' in X_train_scaled.columns:\n",
    "            X_train_scaled['Gender'] = (X_train_scaled['Gender'] > 0.5).astype(int)\n",
    "            X_test_scaled['Gender'] = (X_test_scaled['Gender'] > 0.5).astype(int)\n",
    "        \n",
    "        # Append the processed fold to the list\n",
    "        kFolds_list.append((X_train_scaled, X_test_scaled, Y_train, Y_test))\n",
    "        \n",
    "        print(f\"Fold: {fold+1}, Train: {kFolds_list[fold][0].shape}, Test: {kFolds_list[fold][1].shape}\")\n",
    "    \n",
    "    return kFolds_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e4aa9daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, criterion, optimiser, scheduler, train_loader, val_loader, epochs=20, patience=5, device=device):\n",
    "    if isinstance(model.last_layer(), nn.Sigmoid) and isinstance(criterion, nn.BCEWithLogitsLoss):\n",
    "        raise ValueError(\"Model output is Sigmoid but criterion is BCEWithLogitsLoss. Please check your model and criterion compatibility.\")\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    wait = 0\n",
    "\n",
    "    \n",
    "    #* Epoch Training loop for this fold\n",
    "    for epoch in range(1,epochs+1):\n",
    "        #* Set model to training mode: essential for dropout and batch norm layers\n",
    "        model.train()\n",
    "        running_loss = 0.0 #? loss for this epoch\n",
    "        #* Mini-batch training loop\n",
    "        for batch, (inputs, labels) in enumerate(train_loader,start=1):\n",
    "            optimiser.zero_grad() #? Zero the gradients\n",
    "            outputs = model(inputs) #? Forward pass through the model\n",
    "            loss = criterion(outputs, labels) #? Calculate loss\n",
    "            loss.backward() #? Backpropagation\n",
    "            running_loss += loss.item()\n",
    "            optimiser.step() #? Update weights\n",
    "            if isinstance(scheduler,torch.optim.lr_scheduler.OneCycleLR):\n",
    "                scheduler.step()\n",
    "        if not isinstance(scheduler,torch.optim.lr_scheduler.OneCycleLR):\n",
    "            scheduler.step()\n",
    "                \n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        # print(f\"Epoch: {epoch}, training loss: {train_loss:.4f}\")\n",
    "    \n",
    "        #* Now we evaluate the model on the validation set, to track training vs validation loss\n",
    "        model.eval() #? Set model to evaluation mode\n",
    "        with torch.no_grad(): #? No need to track gradients during evaluation\n",
    "            val_loss = 0.0    \n",
    "            for batch, (inputs, labels) in enumerate(val_loader,start=1):#! one pass because val_loader batch size is all, if you want to do it in mini-batches, you MUST change the metric calculations to accept mini-batches\n",
    "                outputs = model(inputs)\n",
    "                # labels = labels.cpu() \n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() #? Calculate loss\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            # print(f\"Epoch: {epoch}, training loss: {train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "            # print(f\"Epoch: {epoch}\".ljust(12), f\"training loss:{train_loss:.4f}\".ljust(12), f\"Val Loss: {avg_val_loss:.4f}\",end=\"\\r\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model_state = model.state_dict()\n",
    "            wait = 0\n",
    "        elif avg_val_loss*0.85 <= best_val_loss:\n",
    "                wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "        if wait >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch}, best val loss: {best_val_loss:.4f}\")\n",
    "            break\n",
    "        print(f\"Epoch: {epoch}\".ljust(12), f\"training loss:{train_loss:.4f}\".ljust(12), f\"best_val_loss:{best_val_loss:.4f}\".ljust(12), f\"Val Loss: {avg_val_loss:.4f}\",end=\"\\r\")\n",
    "    #* Use best model to calculate metrics on the validation set\n",
    "    #! must be outside epoch loop, it comes after the training and cv loop\n",
    "    model.load_state_dict(best_model_state) #? Load the best model state\n",
    "    with torch.no_grad():\n",
    "        for batch, (inputs, labels) in enumerate(val_loader,start=1):#! one pass because val_loader batch size is all, if you want to do it in mini-batches, you MUST change the metric calculations to accept mini-batches\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                labels = labels.cpu() \n",
    "                predictions = (torch.sigmoid(outputs) < 0.5).float().cpu().numpy()\n",
    "                \n",
    "                val_loss += loss.item() #? Calculate loss\n",
    "                \n",
    "    #! The following should have length equal to fold number           \n",
    "    accuracy=accuracy_score(labels, predictions) \n",
    "    precision=precision_score(labels, predictions, pos_label=1, zero_division=0)\n",
    "    recall=recall_score(labels, predictions, pos_label=1)\n",
    "    f1=f1_score(labels, predictions, pos_label=1)\n",
    "    auc=roc_auc_score(labels, predictions)\n",
    "    \n",
    "    return model, accuracy, precision, recall, f1, auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7000f2cd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            # nn.Sigmoid()\n",
    "   \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "    def last_layer(self):\n",
    "        return self.net[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d49d191",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FeedForwardBlock(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout=0.1, activation=nn.ReLU):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(in_features, out_features),\n",
    "            nn.BatchNorm1d(out_features),\n",
    "            activation(),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, hidden_dim2, output_dim, dropout=0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # A couple of FeedForward blocks\n",
    "        self.block1 = FeedForwardBlock(input_dim, hidden_dim, dropout)\n",
    "        self.block2 = FeedForwardBlock(hidden_dim, hidden_dim2, dropout)\n",
    "        self.block3 = FeedForwardBlock(hidden_dim2, output_dim, dropout)\n",
    "\n",
    "        # Final output layer (could be softmax, sigmoid, or whatever your target is)\n",
    "        self.output_layer = nn.Linear(output_dim, 1)  # Just in case you're doing regression or binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.output_layer(x)  # Final linear layer\n",
    "        return x\n",
    "    \n",
    "    def last_layer(self):\n",
    "        return self.output_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1eabae8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximise_combined_score(trial):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "    epochs = 10000\n",
    "    random_state = 42\n",
    "    oversampler = ADASYN(sampling_strategy='minority', n_neighbors=5, random_state=random_state)\n",
    "    if isinstance(oversampler, ADASYN):\n",
    "        n_neighbours = trial.suggest_int(\"n_neighbours\", 1, 10)\n",
    "        oversampler = ADASYN(sampling_strategy='minority', n_neighbors=n_neighbours, random_state=random_state)\n",
    "    \n",
    "    normalisation_method = trial.suggest_categorical(\"normalisation_method\", [\"StandardScaler\", \"MinMaxScaler\"])\n",
    "    #     # \"MinMaxScaler\",\n",
    "    #     # \"MaxAbsScaler\",\n",
    "    #     \"StandardScaler\",\n",
    "    #     # \"RobustScaler\",\n",
    "    #     # \"PowerTransformer\",\n",
    "    #     # \"QuantileTransformer\",\n",
    "    # ])\n",
    "    if normalisation_method:\n",
    "        if normalisation_method == \"MinMaxScaler\":\n",
    "            normalisation_method = MinMaxScaler()\n",
    "        elif normalisation_method == \"MaxAbsScaler\":\n",
    "            normalisation_method = MaxAbsScaler()\n",
    "        elif normalisation_method == \"StandardScaler\":\n",
    "            normalisation_method = StandardScaler()\n",
    "        elif normalisation_method == \"RobustScaler\":\n",
    "            normalisation_method = RobustScaler()\n",
    "        elif normalisation_method == \"PowerTransformer\":\n",
    "            normalisation_method = PowerTransformer()\n",
    "        elif normalisation_method == \"QuantileTransformer\":\n",
    "            normalisation_method = QuantileTransformer(output_distribution='uniform')\n",
    "        else:\n",
    "            normalisation_method = MinMaxScaler()\n",
    "    \n",
    "    kFolds = FOLDS_GENERATOR(X_FOR_FOLDS, Y_FOR_FOLDS, \n",
    "                         normalisation_method = normalisation_method, \n",
    "                         n_splits=5, \n",
    "                         oversampler = oversampler, random_state=42)\n",
    "                        \n",
    "    # Model hyperparameters (first-level optimization)\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 28, 256)\n",
    "    hidden_dim2 = trial.suggest_int(\"hidden_dim2\", 28, 128)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.4)\n",
    "    initial_lr = trial.suggest_float(\"initial_lr\", 1e-5, 1e-4, log=True)\n",
    "    max_lr = trial.suggest_float(\"max_lr\", 1e-4, 1e-3, log=True)\n",
    "    \n",
    "    # Loss function hyperparameters\n",
    "    criterion_choice = trial.suggest_categorical(\"criterion\", [\"BCEWithLogitsLoss\", \"FocalLoss\"]) \n",
    "    \n",
    "    # Hyperparameter exploration optimization\n",
    "    if criterion_choice == \"BCEWithLogitsLoss\":\n",
    "        pos_weight = trial.suggest_int(\"pos_weight\", 1, 10)\n",
    "        alpha = None\n",
    "        gamma = None\n",
    "    elif criterion_choice == \"FocalLoss\":\n",
    "        pos_weight= None\n",
    "        alpha = trial.suggest_float(\"alpha\", .9, 3)\n",
    "        gamma = trial.suggest_float(\"gamma\", 0.3, 3)\n",
    "    else:\n",
    "        pos_weight = None\n",
    "    \n",
    "    # Initialize lists for metrics across folds\n",
    "    accuracy_list = []\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    f1_list = []\n",
    "    auc_list = []\n",
    "\n",
    "    # Cross-validation loop\n",
    "    for fold, (train_x, test_x, train_y, test_y) in enumerate(kFolds, start=1):\n",
    "        # Create DataLoader for current fold\n",
    "        train_loader, val_loader = fold_to_dataloader_tensor(train_x, test_x, train_y, test_y, batch_size=256, device=device)\n",
    "        # Calculate steps_per_epoch from the current fold's train_loader\n",
    "        train_loader_len = len(train_loader)\n",
    "        \n",
    "        # Instantiate and initialize the model\n",
    "        # model = BinaryClassifier(input_dim=get_feature_count(train_loader), hidden_dim=hidden_dim, dropout=dropout)\n",
    "        model = MyModel(input_dim=get_feature_count(train_loader), hidden_dim=hidden_dim, output_dim=hidden_dim, dropout=dropout)\n",
    "        model.to(device)\n",
    "        model.apply(init_weights)\n",
    "        \n",
    "        # Map the choice to the actual loss function\n",
    "        criterion = criterion_mapping(criterion_choice, pos_weight, alpha, gamma).to(device)\n",
    "        optimiser = optim.Adam(model.parameters(), lr=initial_lr, weight_decay=1e-5)\n",
    "\n",
    "        \n",
    "        # Initialize scheduler\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimiser,\n",
    "            max_lr=max_lr,\n",
    "            steps_per_epoch=train_loader_len,\n",
    "            epochs=epochs,\n",
    "            anneal_strategy='linear'\n",
    "        )\n",
    "        print(f\"Fold {fold}:\")\n",
    "        # Train and evaluate the model on the current fold\n",
    "        model, accuracy, precision, recall, f1, auc = train_and_evaluate(\n",
    "            model, criterion, optimiser, scheduler, train_loader, val_loader, epochs=epochs, patience=40, device=device\n",
    "        )\n",
    "        print(f\"Accuracy: {accuracy:.4f}, precision: {precision:.4f}, recall: {recall:.4f}, f1: {f1:.4f}, auc: {auc:.4f}\")\n",
    "\n",
    "        # Append the metrics from the current fold\n",
    "        accuracy_list.append(accuracy)\n",
    "        precision_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "        f1_list.append(f1)\n",
    "        auc_list.append(auc)\n",
    "\n",
    "    # Calculate the average metrics across all folds\n",
    "    avg_accuracy = np.sum(accuracy_list) / len(accuracy_list)\n",
    "    avg_precision = np.sum(precision_list) / len(precision_list)\n",
    "    avg_recall = np.sum(recall_list) / len(recall_list)\n",
    "    avg_f1 = np.sum(f1_list) / len(f1_list)\n",
    "    avg_auc = np.sum(auc_list) / len(auc_list)\n",
    "\n",
    "    # Combine metrics into a single \"score\"\n",
    "    # combined_score = (avg_f1 + avg_precision + avg_recall + avg_accuracy + avg_auc) / 5\n",
    "    combined_score = avg_f1\n",
    "\n",
    "    return combined_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11392d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-14 10:19:15,614] A new study created in RDB with name: my_cursed_study\n",
      "Bottle v0.13.2 server starting up (using WSGIRefServer())...\n",
      "Listening on http://localhost:8080/\n",
      "Hit Ctrl-C to quit.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-15 (<lambda>):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.11/threading.py\", line 982, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/tmp/ipykernel_7963/971150222.py\", line 17, in <lambda>\n",
      "  File \"/home/jovyan/.local/lib/python3.11/site-packages/optuna_dashboard/_app.py\", line 661, in run_server\n",
      "    run(app, host=host, port=port)\n",
      "  File \"/home/jovyan/.local/lib/python3.11/site-packages/bottle.py\", line 3984, in run\n",
      "    server.run(app)\n",
      "  File \"/home/jovyan/.local/lib/python3.11/site-packages/bottle.py\", line 3527, in run\n",
      "    self.srv = make_server(self.host, self.port, app, server_cls,\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/wsgiref/simple_server.py\", line 154, in make_server\n",
      "    server = server_class((host, port), handler_class)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 456, in __init__\n",
      "    self.server_bind()\n",
      "  File \"/opt/conda/lib/python3.11/wsgiref/simple_server.py\", line 50, in server_bind\n",
      "    HTTPServer.server_bind(self)\n",
      "  File \"/opt/conda/lib/python3.11/http/server.py\", line 136, in server_bind\n",
      "    socketserver.TCPServer.server_bind(self)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 472, in server_bind\n",
      "    self.socket.bind(self.server_address)\n",
      "OSError: [Errno 98] Address already in use\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1, Train: (8017, 21), Test: (1149, 21)\n",
      "Fold: 2, Train: (7921, 21), Test: (1149, 21)\n",
      "Fold: 3, Train: (7850, 21), Test: (1148, 21)\n",
      "Fold: 4, Train: (7986, 21), Test: (1148, 21)\n",
      "Fold: 5, Train: (7906, 21), Test: (1148, 21)\n",
      "Fold 1:\n",
      "Early stopping triggered at epoch 79, best val loss: 0.3233Loss: 0.4545\n",
      "Accuracy: 0.2872, precision: 0.0634, recall: 0.4397, f1: 0.1107, auc: 0.3549\n",
      "Fold 2:\n",
      "Early stopping triggered at epoch 983, best val loss: 0.4264oss: 0.5115\n",
      "Accuracy: 0.1993, precision: 0.0847, recall: 0.7069, f1: 0.1513, auc: 0.4246\n",
      "Fold 3:\n",
      "Early stopping triggered at epoch 497, best val loss: 0.4142oss: 0.4969\n",
      "Accuracy: 0.3084, precision: 0.0730, recall: 0.5000, f1: 0.1275, auc: 0.3934\n",
      "Fold 4:\n",
      "Early stopping triggered at epoch 997, best val loss: 0.4486oss: 0.5388\n",
      "Accuracy: 0.1890, precision: 0.0921, recall: 0.7931, f1: 0.1650, auc: 0.4571\n",
      "Fold 5:\n",
      "Epoch: 1080  training loss:0.2625 best_val_loss:0.4039 Val Loss: 0.4914\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-14 10:28:54,878] Trial 0 finished with value: 0.14442064302952806 and parameters: {'n_neighbours': 5, 'normalisation_method': 'StandardScaler', 'hidden_dim': 100, 'hidden_dim2': 77, 'dropout': 0.31407031378122163, 'initial_lr': 1.0157645723862672e-05, 'max_lr': 0.00032990724040519716, 'criterion': 'FocalLoss', 'alpha': 1.8972798473838854, 'gamma': 1.2357828746624455}. Best is trial 0 with value: 0.14442064302952806.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 1081, best val loss: 0.4039\n",
      "Accuracy: 0.1951, precision: 0.0936, recall: 0.8017, f1: 0.1676, auc: 0.4643\n",
      "Using device: cuda\n",
      "Fold: 1, Train: (7902, 21), Test: (1149, 21)\n",
      "Fold: 2, Train: (7863, 21), Test: (1149, 21)\n",
      "Fold: 3, Train: (7933, 21), Test: (1148, 21)\n",
      "Fold: 4, Train: (7942, 21), Test: (1148, 21)\n",
      "Fold: 5, Train: (7860, 21), Test: (1148, 21)\n",
      "Fold 1:\n",
      "Early stopping triggered at epoch 62, best val loss: 0.0904Loss: 0.1431\n",
      "Accuracy: 0.4926, precision: 0.0448, recall: 0.1983, f1: 0.0731, auc: 0.3620\n",
      "Fold 2:\n",
      "Early stopping triggered at epoch 72, best val loss: 0.1050Loss: 0.1409\n",
      "Accuracy: 0.4943, precision: 0.0485, recall: 0.2155, f1: 0.0792, auc: 0.3706\n",
      "Fold 3:\n",
      "Early stopping triggered at epoch 759, best val loss: 0.1299oss: 0.1552\n",
      "Accuracy: 0.3868, precision: 0.0676, recall: 0.3966, f1: 0.1156, auc: 0.3911\n",
      "Fold 4:\n",
      "Early stopping triggered at epoch 2088, best val loss: 0.1272ss: 0.1737\n",
      "Accuracy: 0.3676, precision: 0.0740, recall: 0.4569, f1: 0.1274, auc: 0.4072\n",
      "Fold 5:\n",
      "Epoch: 1031  training loss:0.0841 best_val_loss:0.1308 Val Loss: 0.1628\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-14 10:38:43,806] Trial 1 finished with value: 0.10490482911124373 and parameters: {'n_neighbours': 6, 'normalisation_method': 'MinMaxScaler', 'hidden_dim': 57, 'hidden_dim2': 45, 'dropout': 0.3301006029939527, 'initial_lr': 1.5494639219330314e-05, 'max_lr': 0.0007090065768282207, 'criterion': 'FocalLoss', 'alpha': 1.089677588729725, 'gamma': 2.405216852547888}. Best is trial 0 with value: 0.14442064302952806.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 1032, best val loss: 0.1308\n",
      "Accuracy: 0.2953, precision: 0.0738, recall: 0.5172, f1: 0.1292, auc: 0.3938\n",
      "Using device: cuda\n",
      "Fold: 1, Train: (8003, 21), Test: (1149, 21)\n",
      "Fold: 2, Train: (7918, 21), Test: (1149, 21)\n",
      "Fold: 3, Train: (7876, 21), Test: (1148, 21)\n",
      "Fold: 4, Train: (7991, 21), Test: (1148, 21)\n",
      "Fold: 5, Train: (7940, 21), Test: (1148, 21)\n",
      "Fold 1:\n",
      "Early stopping triggered at epoch 72, best val loss: 0.7806Loss: 1.0640\n",
      "Accuracy: 0.4212, precision: 0.0402, recall: 0.2069, f1: 0.0673, auc: 0.3261\n",
      "Fold 2:\n",
      "Early stopping triggered at epoch 241, best val loss: 1.1091oss: 1.3846\n",
      "Accuracy: 0.2889, precision: 0.0699, recall: 0.4914, f1: 0.1224, auc: 0.3788\n",
      "Fold 3:\n",
      "Early stopping triggered at epoch 82, best val loss: 0.8647Loss: 1.1737\n",
      "Accuracy: 0.4164, precision: 0.0503, recall: 0.2672, f1: 0.0847, auc: 0.3502\n",
      "Fold 4:\n",
      "Early stopping triggered at epoch 80, best val loss: 0.8344Loss: 1.2187\n",
      "Accuracy: 0.4547, precision: 0.0648, recall: 0.3276, f1: 0.1083, auc: 0.3983\n",
      "Fold 5:\n",
      "Epoch: 417   training loss:0.6802 best_val_loss:1.2334 Val Loss: 1.6116\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-14 10:41:19,578] Trial 2 finished with value: 0.1013739301121915 and parameters: {'n_neighbours': 3, 'normalisation_method': 'StandardScaler', 'hidden_dim': 250, 'hidden_dim2': 85, 'dropout': 0.1482730947353682, 'initial_lr': 1.3938623560797687e-05, 'max_lr': 0.0002938735405052945, 'criterion': 'BCEWithLogitsLoss', 'pos_weight': 4}. Best is trial 0 with value: 0.14442064302952806.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 418, best val loss: 1.2334\n",
      "Accuracy: 0.3362, precision: 0.0716, recall: 0.4655, f1: 0.1241, auc: 0.3936\n",
      "Using device: cuda\n",
      "Fold: 1, Train: (8010, 21), Test: (1149, 21)\n",
      "Fold: 2, Train: (7939, 21), Test: (1149, 21)\n",
      "Fold: 3, Train: (7861, 21), Test: (1148, 21)\n",
      "Fold: 4, Train: (7987, 21), Test: (1148, 21)\n",
      "Fold: 5, Train: (7980, 21), Test: (1148, 21)\n",
      "Fold 1:\n",
      "Early stopping triggered at epoch 41, best val loss: 0.4023Loss: 0.8684\n",
      "Accuracy: 0.5892, precision: 0.0459, recall: 0.1552, f1: 0.0709, auc: 0.3966\n",
      "Fold 2:\n",
      "Early stopping triggered at epoch 48, best val loss: 0.5433Loss: 0.7836\n",
      "Accuracy: 0.5257, precision: 0.0367, recall: 0.1466, f1: 0.0587, auc: 0.3574\n",
      "Fold 3:\n",
      "Early stopping triggered at epoch 74, best val loss: 0.6207Loss: 0.7987\n",
      "Accuracy: 0.5479, precision: 0.0389, recall: 0.1466, f1: 0.0615, auc: 0.3698\n",
      "Fold 4:\n",
      "Early stopping triggered at epoch 590, best val loss: 0.6601oss: 0.8317\n",
      "Accuracy: 0.2666, precision: 0.0856, recall: 0.6466, f1: 0.1512, auc: 0.4352\n",
      "Fold 5:\n",
      "Early stopping triggered at epoch 83, best val loss: 0.6371Loss: 0.8048\n",
      "Accuracy: 0.5383, precision: 0.0359, recall: 0.1379, f1: 0.0569, auc: 0.3606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-14 10:43:32,612] Trial 3 finished with value: 0.07984401525541847 and parameters: {'n_neighbours': 9, 'normalisation_method': 'MinMaxScaler', 'hidden_dim': 213, 'hidden_dim2': 42, 'dropout': 0.30212441485116137, 'initial_lr': 8.839493523264046e-05, 'max_lr': 0.0008192305598539905, 'criterion': 'FocalLoss', 'alpha': 2.7081542295134033, 'gamma': 1.5926106625190333}. Best is trial 0 with value: 0.14442064302952806.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Fold: 1, Train: (8010, 21), Test: (1149, 21)\n",
      "Fold: 2, Train: (7939, 21), Test: (1149, 21)\n",
      "Fold: 3, Train: (7861, 21), Test: (1148, 21)\n",
      "Fold: 4, Train: (7987, 21), Test: (1148, 21)\n",
      "Fold: 5, Train: (7980, 21), Test: (1148, 21)\n",
      "Fold 1:\n",
      "Early stopping triggered at epoch 41, best val loss: 0.4575Loss: 0.5705\n",
      "Accuracy: 0.3768, precision: 0.0627, recall: 0.3707, f1: 0.1072, auc: 0.3741\n",
      "Fold 2:\n",
      "Epoch: 723   training loss:0.4825 best_val_loss:0.5307 Val Loss: 0.6088\r"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import optuna\n",
    "from optuna_dashboard import run_server\n",
    "# !fuser -k 8080/tcp\n",
    "\n",
    "# Define your persistent storage\n",
    "storage = \"sqlite:///my_cursed_study.db\"\n",
    "\n",
    "# Create or load your study\n",
    "study_name = \"my_cursed_study\"\n",
    "try:\n",
    "    study = optuna.load_study(study_name=study_name, storage=storage)\n",
    "except KeyError:\n",
    "    study = optuna.create_study(study_name=study_name, direction=\"maximize\", storage=storage)\n",
    "\n",
    "# Start Optuna Dashboard in a separate thread\n",
    "dashboard_thread = threading.Thread(target=lambda: run_server(storage), daemon=True)\n",
    "dashboard_thread.start()\n",
    "\n",
    "# Run optimization\n",
    "study.optimize(maximise_combined_score, n_trials=30)\n",
    "\n",
    "# Print results\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(f\"  Combined score: {trial.value}\")\n",
    "print(\"  Best hyperparameters:\")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cb2751",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4650cd-edba-4d83-8dc7-ed8bb6442832",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
